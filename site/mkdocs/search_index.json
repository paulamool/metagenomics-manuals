{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nSome welcome text should appear here.\n\n\nPerhaps the logos should be added here as well?", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "Some welcome text should appear here.  Perhaps the logos should be added here as well?", 
            "title": "Welcome"
        }, 
        {
            "location": "/trainers/trainers/", 
            "text": "Dr. Dan Andrews\n Bioinformatics Fellow, John Curtin School of Medical Research, Australian National University, Canberra   \n\n\n \nMs. Katherine Champ\n Workshop Coordinator, Project Officer, Bioplatform Autralia Ltd.  \n\n\n \nDr. Zhiliang Chen\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Susan Corley\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Nandan Deshpande\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Konsta Duesing\n Research Team Leader - Statistics \n Bioinformatics, CSIRO Animal, Food and Health Science, Sydney  \n\n\n \nDr. Matthew Field\n Senior Research Fellow, Australian National University/James Cook University, Cairns\n\n\n \nDr. Velimir Gayevskiy\n Translational Bioinformatics Officer, KCCG, Garvan Institute of Medical Research NSW  \n\n\n \nPaul Greenfield\n Principal Experimental Scientist, CSIRO, Sydney  \n\n\n \nDr. Xi (Sean) Li\n The Australian National University, Canberra  \n\n\n \nDr. Annette McGrath\n Principal Research Scientist, Team Leader, Life Science Informatics DATA61, CSIRO, Canberra  \n\n\n \nMr. Sean McWilliam\n Bioinformatics Analyst, CSIRO Agriculture, Brisbane  \n\n\n \nDr. Philippe Moncuquet\n Research Project Officer, Cotton Disease Markers, CSIRO, Canberra\n\n\n \nDr. Paula Moolhuijzen\n Bioinformatics Analyst, Centre for Crop Disease Management, Curtin University, Perth  \n\n\n \nDr. Ann-Marie Patch\n Senior Research Officer, Medical Genomics QIMR Berghofer Medical Research Institute, Brisbane  \n\n\n \nDr. Gayle Philip\n Research Fellow (Bioinformatics), Melbourne Bioinformatics, Carlton, Melbourne  \n\n\n \nMr. Jerico Revote\n Software Developer Monash eResearch Centre, Monash University, Clayton Melbourne  \n\n\n \nA/Prof. Torsten Seemann\n Lead Bioinformatician, Melbourne Bioinformatics and MDU-PHL, The University of Melbourne, VIC  \n\n\n \nDr Anna Syme\n Bioinformatician, Melbourne Bioinformatics, Melbourne  \n\n\n \nDr. Erdahl Teber\n Senior Research Officer (Bioinformatics), Children Medical Research Institute, Kids Cancer Alliance, University of Sydney, Sydney  \n\n\n \nDr. Sonika Tyagi\n Bioinformatics Supervisor, Australian Genome Research Facility Ltd, The Walter and Eliza Hall Institute, Melbourne   \n\n\n \nDr. Nathan S. Watson-Haigh\n Research Fellow in Bioinformatics, The Australian Centre for Plant Functional Genomics (ACPFG), Adelaide", 
            "title": "The Trainers"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/", 
            "text": "Metagenomics Workshop\n\n\n\n\nMacquarie University, NSW - 11\nth\n - 12\nth\n July 2017\n\n\nInstructors\n\n\n\n\nKatherine Champ (KC) - Bioplatforms Australia, Sydney\n\n\nSonika Tyagi (ST) - AGRF, Melbourne\n\n\nMatthew Field (MF) - Australian National University/James Cook University, Cairns\n\n\nXi (Sean) Li (SL) - Australian National University, Canberra\n\n\nSusan Corley (SC) - UNSW Systems Biology Initiative, Sydney\n\n\n\n\nTimetable\n\n\nDay 1 - Introduction to the command line, data quality \n alignment \n ChIP-Seq\n\n\n27\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nIntroductions and course orientation\n\n\n\n\nK\n\n\n\n\n\n\n09:45\n\n\nPractical: Introduction to the command line\n\n\n\n\n\n\n\n\n\n\n10:15\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:40\n\n\nPractical: Introduction to the command line course and R course\n\n\n\n\n\n\n\n\n\n\n11:20\n\n\nIntroduction to NGS- technology, data formats and introduction to quality control\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:15\n\n\nQuality control: Intro to practical\n\n\n\n\n\n\n\n\n\n\n13:25\n\n\nPractical: Quality control\n\n\n\n\n\n\n\n\n\n\n14:05\n\n\nIntroduction to sequence alignment\n\n\n\n\n\n\n\n\n\n\n14:15\n\n\nPractical: Sequence alignment\n\n\n\n\n\n\n\n\n\n\n15:00\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:25\n\n\nIntroduction to ChIP-Seq\n\n\n\n\n\n\n\n\n\n\n15:55\n\n\nPractical: ChIP-Seq analysis - Peak calling and annotation\n\n\n\n\n\n\n\n\n\n\n16:30\n\n\nQ\nA and day 1 wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 2 - ChIP-Seq motif analysis and RNA-Seq analysis\n\n\n28\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nPractical: Motif analysis\n\n\n\n\n\n\n\n\n\n\n09:40\n\n\nIntroduction to RNA-Seq\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:50\n\n\nPractical: Alignment and splice junction identification\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nPractical: Differential gene expression with Bio-conductor package: EdgeR and Voom\n\n\n\n\n\n\n\n\n\n\n15:00\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:30\n\n\nPractical: Biological interpretation\n\n\n\n\n\n\n\n\n\n\n16:30\n\n\nQ\nA and day 2 wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 3 - \nde novo\n Assembly\n\n\n29\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nIntroduction to de novo assembly\n\n\n\n\n\n\n\n\n\n\n09:40\n\n\nPractical: de novo assembly using Illumina reads\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:50\n\n\nPractical: de novo assembly using Illumina reads (cont.)\n\n\n\n\n\n\n\n\n\n\n11:30\n\n\nPractical: de novo assembly using PacBio \u2013 Canu workflow\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nPractical: de novo assembly using PacBio \u2013 Canu workflow\n\n\n\n\n\n\n\n\n\n\n15:30\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:50\n\n\nPractical: Polishing PacBio de novo assembly with Illumina reads\n\n\n\n\n\n\n\n\n\n\n16:30\n\n\nQ\nA and workshop wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n17:00\n\n\nWorkshop Survey", 
            "title": "Metagenomics"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#metagenomics-workshop", 
            "text": "Macquarie University, NSW - 11 th  - 12 th  July 2017", 
            "title": "Metagenomics Workshop"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#instructors", 
            "text": "Katherine Champ (KC) - Bioplatforms Australia, Sydney  Sonika Tyagi (ST) - AGRF, Melbourne  Matthew Field (MF) - Australian National University/James Cook University, Cairns  Xi (Sean) Li (SL) - Australian National University, Canberra  Susan Corley (SC) - UNSW Systems Biology Initiative, Sydney", 
            "title": "Instructors"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#timetable", 
            "text": "", 
            "title": "Timetable"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#day-1-introduction-to-the-command-line-data-quality-alignment-chip-seq", 
            "text": "27 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Introductions and course orientation   K    09:45  Practical: Introduction to the command line      10:15  Morning Tea      10:40  Practical: Introduction to the command line course and R course      11:20  Introduction to NGS- technology, data formats and introduction to quality control      12:30  Lunch      13:15  Quality control: Intro to practical      13:25  Practical: Quality control      14:05  Introduction to sequence alignment      14:15  Practical: Sequence alignment      15:00  Afternoon Tea      15:25  Introduction to ChIP-Seq      15:55  Practical: ChIP-Seq analysis - Peak calling and annotation      16:30  Q A and day 1 wrap-up   All", 
            "title": "Day 1 - Introduction to the command line, data quality &amp; alignment &amp; ChIP-Seq"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#day-2-chip-seq-motif-analysis-and-rna-seq-analysis", 
            "text": "28 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Practical: Motif analysis      09:40  Introduction to RNA-Seq      10:30  Morning Tea      10:50  Practical: Alignment and splice junction identification      12:30  Lunch      13:30  Practical: Differential gene expression with Bio-conductor package: EdgeR and Voom      15:00  Afternoon Tea      15:30  Practical: Biological interpretation      16:30  Q A and day 2 wrap-up   All", 
            "title": "Day 2 - ChIP-Seq motif analysis and RNA-Seq analysis"
        }, 
        {
            "location": "/timetables/timetable_metagenomics/#day-3-de-novo-assembly", 
            "text": "29 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Introduction to de novo assembly      09:40  Practical: de novo assembly using Illumina reads      10:30  Morning Tea      10:50  Practical: de novo assembly using Illumina reads (cont.)      11:30  Practical: de novo assembly using PacBio \u2013 Canu workflow      12:30  Lunch      13:30  Practical: de novo assembly using PacBio \u2013 Canu workflow      15:30  Afternoon Tea      15:50  Practical: Polishing PacBio de novo assembly with Illumina reads      16:30  Q A and workshop wrap-up   All    17:00  Workshop Survey", 
            "title": "Day 3 - de novo Assembly"
        }, 
        {
            "location": "/preamble/", 
            "text": "Providing Feedback\n\n\nWhile we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.\n\n\nWhilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!\n\n\nClearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!\n\n\nWith that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:\n\n\n\n\n\n\nA sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!\n\n\n\n\n\n\nSome empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.\n\n\n\n\n\n\nAn online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!\n\n\n\n\n\n\nDocument Structure\n\n\nWe have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.\n\n\n\n\nWhile you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!\n\n\n\n\nThe commands to enter at a terminal look something like this:\n\n\n1\ntophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nThe following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc\n\n\n1\ntophat [options]* \nindex_base\n \nreads_1\n \nreads_2\n\n\n\n\n\n\n\nThe following is an example how of R commands are styled:\n\n\n1\n2\n3\n4\n5\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n\ndata \n-\n read.table\n(\nrun_25/stats.txt\n,\n header\n=\nTRUE\n)\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n+\ndata\n$\nshort2_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n70\n)\n\n\nq\n()\n\n\n\n\n\n\n\nThe following icons are used throughout the documentation\nto help you navigate around the document more easily:\n\n\n\n\nQuestion\n\n\nQuestions to answer.\n\n\n\n\n\n\nAnswer\n\n\nAnswers will be provided at the end of the workskop.\n\n\n\n\n\n\nImportant\n\n\nThis is important. \n\n\n\n\n\n\nSTOP\n\n\nWarning - STOP and read.\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nAdvanced exercise\n\n\nAdvanced exercise for super-fast learners\n\n\n\n\nResources Used\n\n\nWe have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Workshop Information"
        }, 
        {
            "location": "/preamble/#providing-feedback", 
            "text": "While we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.  Whilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!  Clearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!  With that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:    A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!    Some empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.    An online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!", 
            "title": "Providing Feedback"
        }, 
        {
            "location": "/preamble/#document-structure", 
            "text": "We have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.   While you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!   The commands to enter at a terminal look something like this:  1 tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   The following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc  1 tophat [options]*  index_base   reads_1   reads_2    The following is an example how of R commands are styled:  1\n2\n3\n4\n5 R  -- no - save  library ( plotrix ) \ndata  -  read.table ( run_25/stats.txt ,  header = TRUE ) \nweighted.hist ( data $ short1_cov + data $ short2_cov ,  data $ lgth ,  breaks = 0 : 70 )  q ()    The following icons are used throughout the documentation\nto help you navigate around the document more easily:   Question  Questions to answer.    Answer  Answers will be provided at the end of the workskop.    Important  This is important.     STOP  Warning - STOP and read.    Bonus exercise  Bonus exercise for fast learners.    Advanced exercise  Advanced exercise for super-fast learners", 
            "title": "Document Structure"
        }, 
        {
            "location": "/preamble/#resources-used", 
            "text": "We have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Resources Used"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nFamiliarise yourself with the command line environment on a Linux\n    operating system.\n\n\n\n\n\n\nRun some basic linux system and file operation commands\n\n\n\n\n\n\nNavigration of biological data files structure and manipulation\n\n\n\n\n\n\nResources\n\n\nTools\n\n\n\n\n\n\nBasic Linux system commands on an Ubuntu OS.\n\n\n\n\n\n\nBasic file operation commands\n\n\n\n\n\n\nLinks\n\n\n\n\n\n\nSoftware Carpentry\n\n\n\n\n\n\nExample 1000Genome Project data\n\n\n\n\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Matt Field \n     \n\n\nShell Exercise\n\n\nLet\u2019s try out your new shell skills on some real data.\n\n\nThe file \n1000gp.vcf\n is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project (\nhttp://www.1000genomes.org\n). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n1\n2\n3\n4\n5\ncd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n\n\n\n\n\n\n\nQuestion\n\n\nWhat is the file size (in kilo-bytes), and how many lines are in the file?.\n\n\n\n\nHint\nHint: \nman ls\n, \nman wc\n\n\nAnswer\n3.6M\n45034 lines\n\n\nBecause this file is so large, you\u2019re going to almost always want to\npipe (\n|\n) the result of any command to less (a simple text viewer, type\n\u2018\nq\n\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.\n\n\nLet\u2019s start by printing the first 5 lines to see what it looks like.\n\n1\nhead -5 1000gp.vcf\n\n\n\n\n\nThat isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!\n\n\nPrint the first 20 lines to see more of the file.\n\n1\nhead -20 1000gp.vcf\n\n\n\n\n\nOkay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:\n\n\n\n\n\n\nthe chromosome (which volume the difference is in)\n\n\n\n\n\n\nthe position (which character in the volume the difference starts\n    at)\n\n\n\n\n\n\nthe ID of the difference\n\n\n\n\n\n\nthe sequence in the reference human(s)\n\n\n\n\n\n\nThe rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.\n\n\nTo start analyzing the actual data, we have to remove the header.\n\n\n\n\nQuestion\n\n\nHow can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?\n\n\n\n\nHint\nHint: \nman grep\n (remember to use pipes \n|\n)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | head  \n\n\n\n\nQuestion\n\n\nHow many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\n\n\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | wc -l (should print 45024)\n\n\nWhere these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).\n\n\n\n\nQuestion\n\n\nPrint the first 10 chromosomes, one per line.\n\n\n\n\nHint\nHint: \nman cut\n (remember to remove header lines first)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | head\n\n\nAs you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).\n\n\nLet\u2019s look at which chromosomes these variations are on.\n\n\n\n\nQuestion\n\n\nPrint a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).\n\n\n\n\nHint\nHint: remove all duplicates from your previous answer (\nman sort\n)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort -u\n\n\nRather than using \nsort\n to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command, \nuniq\n. The \nuniq\n\ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted, \nuniq\n won\u2019t work properly.\n\n\n\n\nQuestion\n\n\nUsing \nsort\n and \nuniq\n, print the number of times each chromosome\noccurs in the file.\n\n\n\n\nHint\nHint: \nman uniq\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c\n\n\n\n\nQuestion\n\n\nAdd to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.\n\n\n\n\nHint\nHint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n\n\nThis is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.\n\n\n\n\nQuestion\n\n\nSort the previous output by chromosome number\n\n\n\n\nHint\nHint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Introduction to Command Line"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Familiarise yourself with the command line environment on a Linux\n    operating system.    Run some basic linux system and file operation commands    Navigration of biological data files structure and manipulation", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#tools", 
            "text": "Basic Linux system commands on an Ubuntu OS.    Basic file operation commands", 
            "title": "Tools"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#links", 
            "text": "Software Carpentry    Example 1000Genome Project data", 
            "title": "Links"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#author-information", 
            "text": "Primary Author(s): \n    Matt Field", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/metagenomics-module-cli/commandline/#shell-exercise", 
            "text": "Let\u2019s try out your new shell skills on some real data.  The file  1000gp.vcf  is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project ( http://www.1000genomes.org ). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.  Open the Terminal and go to the directory where the data are stored: 1\n2\n3\n4\n5 cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf    Question  What is the file size (in kilo-bytes), and how many lines are in the file?.   Hint Hint:  man ls ,  man wc  Answer 3.6M 45034 lines  Because this file is so large, you\u2019re going to almost always want to\npipe ( | ) the result of any command to less (a simple text viewer, type\n\u2018 q \u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.  Let\u2019s start by printing the first 5 lines to see what it looks like. 1 head -5 1000gp.vcf   That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!  Print the first 20 lines to see more of the file. 1 head -20 1000gp.vcf   Okay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:    the chromosome (which volume the difference is in)    the position (which character in the volume the difference starts\n    at)    the ID of the difference    the sequence in the reference human(s)    The rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.  To start analyzing the actual data, we have to remove the header.   Question  How can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?   Hint Hint:  man grep  (remember to use pipes  | )  Answer grep -v  ^#  1000gp.vcf | head     Question  How many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?   Answer grep -v  ^#  1000gp.vcf | wc -l (should print 45024)  Where these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).   Question  Print the first 10 chromosomes, one per line.   Hint Hint:  man cut  (remember to remove header lines first)  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | head  As you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).  Let\u2019s look at which chromosomes these variations are on.   Question  Print a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).   Hint Hint: remove all duplicates from your previous answer ( man sort )  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort -u  Rather than using  sort  to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command,  uniq . The  uniq \ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted,  uniq  won\u2019t work properly.   Question  Using  sort  and  uniq , print the number of times each chromosome\noccurs in the file.   Hint Hint:  man uniq  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c   Question  Add to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.   Hint Hint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r  This is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.   Question  Sort the previous output by chromosome number   Hint Hint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Shell Exercise"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/", 
            "text": "Data Quality Control\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n\n\nPicard:\n\n\nhttp://picard.sourceforge.net/\n\n\nUseful Links\n\n\n\n\nFASTQ Encoding:\n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nIntroduction\n\n\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage (\n15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\n\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n(\nhttp://shop.alterlinks.com/ascii-table/ascii-table-us.php\n). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $\n$ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nPrepare the Environment\n\n\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing fastqc -h\n\n\nQuality Visualisation\n\n\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019\n\u2019 sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html \n\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n1000000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nFastQC Basic Statistics table\n\n\n\n\nPer base sequence quality plot for\n\nqcdemo_R2.fastq.gz\n.\n\n\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base \nA\n is wrong \nP(sim A)\n is expressed by a quality score, \nQ(A)\n, according to the\nrelationship:\n\n\nQ(A) =-10 log10(P(sim A))\n\n\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n\n\n\n\n\n\nQuality score\n\n\nError probability\n\n\nAccuracy of the base call\n\n\n\n\n\n\n\n\n\n\n10\n\n\n0.1\n\n\n90%\n\n\n\n\n\n\n20\n\n\n0.01\n\n\n99%\n\n\n\n\n\n\n30\n\n\n0.001\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n0.0001\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n0.00001\n\n\n99.999%\n\n\n\n\n\n\n\n\nError probabilities associated with various quality (Q) values\n\n\n[tab:quality_error_probs]\n\n\n\n\n\n\nHow many sequences were there in your file? What is the read length?\n\n\n\n\nThis is a spoiler: {%s%}Hello World.{%ends%}\n\n\n1\n 1,000,000. read length=150bp\n\n\n\n\n\n\n\n\n\n\nDoes the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)\n\n\n\n\nYes. Quality scores are dropping towards the end of the reads.\n\n\n\n\n\n\n\n\nWhat is the quality score range you see?\n\n\n\n\n2-40\n\n\n\n\n\n\n\n\nAt around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?\n\n\n\n\nAround 30 bp position\n\n\n\n\n\n\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\n\n\nBy trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.\n\n\n\n\n\n\n\n\nGood Quality Data\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html \n\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\nRead Trimming\n\n\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd\n ~/qc\nskewer -t \n20\n -l \n50\n  -q \n30\n -Q \n25\n -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode\n\n\n\n\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\n1\n2\n3\n4\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html \n\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html\n\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\nFastQC Basic Statistics table\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R1.fastq-trimmed-pair2.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n742262\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n50\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nPer base sequence quality plot for the quality-trimmed\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\n\n\n\n\nQuality trimming discarded \n1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.\n\n\n\n\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\n\n\n\n\n50-150 Reads \n50 bp, following quality trimming, were discarded.\n\n\n\n\n\n\nDid you observe adapter sequences in the data?\n\n\n\n\n\n\nNo. (Hint: look at the overrepresented sequences.\n\n\n\n\n\n\nHow can you use -a option with fastqc ? (Hint: try fastqc -h).\n\n\n\n\n\n\nAdaptors can be supplied in a file for screening.\n\n\n\n\nAdapter Clipping\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.\n\n\nCutadapt: (\nhttp://code.google.com/p/cutadapt/\n)\nTrimmomatic:\n(\nhttp://www.usadellab.org/cms/?page=trimmomatic\n)\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\ncd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n-x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n\n\n\n\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html \n\n\n\n\n\n\n\nAn alternative tool, not installed on this system, for adapter clipping\nis \nfastq-mcf\n. A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at\n\nhttp://code.google.com/p/ea-utils/wiki/FastqMcf\n.\n\n\nFixed Length Trimming\n\n\nWe will not cover Fixed Length Trimming but provide the following for your information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name\n\n\n\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html \n\n\n\n\n\n\n\nThe output should look like:\n\n\n\n\n\n\n\n\nFilename\n\n\nbad_example_trimmed01.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base call\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n40000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n80\n\n\n\n\n\n\n%GC\n\n\n48\n\n\n\n\n\n\n\n\n: FastQC Basic Statistics table\n\n\n\n\ntab:badexampletrimmed\n\n\n\n\n![Per base sequence quality plot for the fixed-length trimmed\n\nbad_example.fastq\n\n\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?\n\n\n-f 11\n\n\nRemoving Duplicates\n\n\nDuplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.\n\n\nThe duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.\n\n\nWe will not cover Picard but provide the following for your\ninformation.\n\n\nPicard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:\n\n\nhttp://picard.sourceforge.net/command-line-overview.shtml\n\n\nA good list of tools for filtering PCR duplication can also be found at\n\nhttp://omictools.com/duplicate-reads-removal-c495-p1.html\n\n\nPicard is installed on this system in \n/usr/share/java\n\n\nOne of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.\n\n\nInterested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):\n\n\n1\n2\ncd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT=\nalignment_file.bam\n VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true", 
            "title": "Metagenomics QC"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#data-quality-control", 
            "text": "", 
            "title": "Data Quality Control"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis    Clean up adaptors and pre-process the sequence data for further analysis", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#tools-used", 
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/  Picard:  http://picard.sourceforge.net/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#useful-links", 
            "text": "FASTQ Encoding:  http://en.wikipedia.org/wiki/FASTQ_format#Encoding", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#introduction", 
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage ( 15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-value-encoding-schema", 
            "text": "In order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n( http://shop.alterlinks.com/ascii-table/ascii-table-us.php ). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $ $ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding", 
            "title": "Quality Value Encoding Schema"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#prepare-the-environment", 
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3 ls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing fastqc -h", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-visualisation", 
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019 \u2019 sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html     The report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:     Filename  qcdemo_R2.fastq.gz      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  1000000    Filtered Sequences  0    Sequence length  150    %GC  37     FastQC Basic Statistics table   Per base sequence quality plot for qcdemo_R2.fastq.gz .   A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base  A  is wrong  P(sim A)  is expressed by a quality score,  Q(A) , according to the\nrelationship:  Q(A) =-10 log10(P(sim A))  The relationship between the quality score and error probability is\ndemonstrated with the following table:     Quality score  Error probability  Accuracy of the base call      10  0.1  90%    20  0.01  99%    30  0.001  99.9%    40  0.0001  99.99%    50  0.00001  99.999%     Error probabilities associated with various quality (Q) values  [tab:quality_error_probs]    How many sequences were there in your file? What is the read length?   This is a spoiler: {%s%}Hello World.{%ends%}  1  1,000,000. read length=150bp     Does the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)   Yes. Quality scores are dropping towards the end of the reads.     What is the quality score range you see?   2-40     At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?   Around 30 bp position     How can we trim the reads to filter out the low quality data?   By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.", 
            "title": "Quality Visualisation"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#good-quality-data", 
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html     Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.", 
            "title": "Good Quality Data"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#read-trimming", 
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.", 
            "title": "Read Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-based-trimming", 
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd  ~/qc\nskewer -t  20  -l  50   -q  30  -Q  25  -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz   1\n2\n3\n4\n5\n6\n7\n8\n9 -t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode   Run FastQC on the quality trimmed file and visualise the quality scores.  1\n2\n3\n4 fastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html  \nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html    Let\u2019s look at the quality from the second reads. The output should look\nlike:  FastQC Basic Statistics table     Filename  qcdemo_R1.fastq-trimmed-pair2.fastq      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  742262    Filtered Sequences  0    Sequence length  50    %GC  37     Per base sequence quality plot for the quality-trimmed  qcdemo_R2.fastq.gz     Did the number of total reads in R1 and R2 change after trimming?    Quality trimming discarded  1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.    What reads lengths were obtained after quality based trimming?    50-150 Reads  50 bp, following quality trimming, were discarded.    Did you observe adapter sequences in the data?    No. (Hint: look at the overrepresented sequences.    How can you use -a option with fastqc ? (Hint: try fastqc -h).    Adaptors can be supplied in a file for screening.", 
            "title": "Quality Based Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#adapter-clipping", 
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.  Cutadapt: ( http://code.google.com/p/cutadapt/ )\nTrimmomatic:\n( http://www.usadellab.org/cms/?page=trimmomatic )  Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3 cd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 -x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim   Run FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html     An alternative tool, not installed on this system, for adapter clipping\nis  fastq-mcf . A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at http://code.google.com/p/ea-utils/wiki/FastqMcf .", 
            "title": "Adapter Clipping"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#fixed-length-trimming", 
            "text": "We will not cover Fixed Length Trimming but provide the following for your information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above: 1\n2\n3\n4\n5\n6\n7\n8\n9 -Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name   Run FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html     The output should look like:     Filename  bad_example_trimmed01.fastq      File type  Conventional base call    Encoding  Sanger / Illumina 1.9    Total Sequences  40000    Filtered Sequences  0    Sequence length  80    %GC  48     : FastQC Basic Statistics table   tab:badexampletrimmed   ![Per base sequence quality plot for the fixed-length trimmed bad_example.fastq   What values would you use for  -f  if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?  -f 11", 
            "title": "Fixed Length Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#removing-duplicates", 
            "text": "Duplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.  The duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.  We will not cover Picard but provide the following for your\ninformation.  Picard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:  http://picard.sourceforge.net/command-line-overview.shtml  A good list of tools for filtering PCR duplication can also be found at http://omictools.com/duplicate-reads-removal-c495-p1.html  Picard is installed on this system in  /usr/share/java  One of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.  Interested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):  1\n2 cd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT= alignment_file.bam  VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true", 
            "title": "Removing Duplicates"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/", 
            "text": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand the open source software package QIIME for analysis\n\n\n\n\n\n\nPerform a taxonomic analysis on a 16S rRNA amplicon dataset\n\n\n\n\n\n\nResources you will be using\n\n\n\n\nTools Used 16S Analysis\n\n\nQIIME :  \nhttp://qiime.org/\n\n\nPEAR: \nhttp://sco.h-its.org/exelixis/web/software/pear/doc.html\n\n\nFASTX toolkit (v0.0.14): \nhttp://hannonlab.cshl.edu/fastx_toolkit/download.html\n\n\nBBMap: \nhttp://sourceforge.net/projects/bbmap\n\n\nVSEARCH: \nhttps://github.com/torognes/vsearch\n\n\nSortMeRNA: \nhttp://bioinfo.lifl.fr/RNA/sortmerna/\n\n\nSTAMP: \nhttp://kiwi.cs.dal.ca/Software/STAMP\n\n\nUseful Links\n\n\n\n\nFASTQ Encoding:   \n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nQIIME Tutorials:\n\nhttp://qiime.org/tutorials/tutorial.html\n\n\n16S Tutorials:\n\nhttps://github.com/mlangill/microbiome_helper/wiki/16S-tutorial-(chemerin\n)\n\n\nSources of Data\n\n\n\n\n\n\n\n\nData : Mouse gut microbial composition affected by the protein chemerin.  \nhttps://www.dropbox.com/s/4fqgi6t3so69224/Sinal_Langille_raw_data.tar.gz\n\n\nhttps://www.dropbox.com/s/r2jqqc7brxg4jhx/16S_chemerin_tutorial.zip\n\n\n\n\n\n\nRDP_trainset16_022016.fa (20 MB) is a subset of the Ribosome Database Project (RDP) filtered to include only bacteria.\n\nhttps://www.dropbox.com/s/qnlsv7ve2lg6qfp/RDP_trainset16_022016.fa?dl=1\n \n\n\n\n\n\n\nOverview\n\n\n\n\nIn this tutorial we will look at the open source software package QIIME (pronounced \u2019chime\u2019). QIIME stands for Quantitative Insights Into Microbial Ecology. The package contains many tools that enable users to analyse and compare microbial communities. \n\n\nAfter completion of this tutorial, you should be able to perform a taxonomic analysis on a Illumina pair end 16S rRNA amplicon dataset\n\n\nDe novo OTU picking and diversity analysis using Illumina data\n\n\n\n\nIntroduction\n\n\nThe workflow for 16S analysis in general is as follows:\n\n\n\n\nSplit multiplexed reads to samples\n\n\nJoin overlapping read pairs\n\n\nFilter reads on quality and length\n\n\nFilter Chimera sequences\n\n\nAssign reads to samples\n\n\nPick operational taxonomic units (OTUs) for each sample\n\n\nAlpha diversity analysis and rarefaction\n\n\nBeta diversity analysis and Taxonomic composition\n\n\nPCA analysis\n\n\n\n\n16S analysis is a method of microbiome analysis (compared to shotgun metagenomics) that targets the 16S ribosomal RNA gene, as this gene is present in all prokaryotes. It features regions that are conserved among these organisms, as well as variable regions that allow distinction among organisms. These characteristics make this gene useful for analyzing microbial communities at reduced cost compared to metagenomic techniques. A similar workflow can be applied to eukaryotic micro-organisms using the 18S rRNA gene.\n\n\nThe tutorial dataset was originally used in a project to determine whether knocking out the protein chemerin affects gut microbial composition. Originally 116 mouse samples acquired from two different facilities were used for this project (only 24 samples were used in this tutorial dataset, for simplicity). \n\n\nThe Mapping file\n\n\nMetadata associated with each sample is indicated in the mapping file (map.txt). The mapping file associates the read data files for a sample to it\ns metadata. The mapping file can contain information on your experimental design. The format is very strict; columns are separated with a single TAB character; the header names have to be typed exactly as specified in the documentation. A good sample description is useful as it is used in the legends of the figures QIIME generates.\n\n\nIn the mapping (map.txt) file the genotypes of interest can be seen: wildtype (WT), chemerin knockout (chemerin_KO), chemerin receptor knockout (CMKLR1_KO) and a heterozygote for the receptor knockout (HET). Also of importance are the two source facilities: \nBZ\n and \nCJS\n. It is important to include as much metadata as possible, so that it can be easily explored later on.\n\n\nOpen Terminal and go to the dataset\ns directory:\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nls -lhtr\n\n\n\n\n\n\nfastq\n is the directory containing all the sequencing files, which we are going to process. The file \nmap.txt\n contains metadata about the samples. We can look at it with the less command (hit \nq\n to exit):\n\n\n1\nless -S map.txt\n\n\n\n\n\n\nThe first column is the sample IDs, the next 2 are blank (note the file is tab-delimited, meaning each column is separated by a tab, not just whitespace) and the 4\nth\n column contains FASTA filenames (these filenames are based on what we will produce in this pipeline). The rest of the columns are metadata about the samples.\n\n\nHere is what the first 4 lines should look like:\n\n\n1\n2\n3\n4\nSampleID       BarcodeSequence LinkerPrimerSequence    FileInput       Source  Mouse#  Cage#   genotype        SamplingWeek\n105CHE6WT                       105CHE6WT_S325_L001.assembled_filtered.nonchimera.fasta BZ      BZ25    7       WT      wk6\n106CHE6WT                       106CHE6WT_S336_L001.assembled_filtered.nonchimera.fasta BZ      BZ26    7       WT      wk6\n107CHE6KO                       107CHE6KO_S347_L001.assembled_filtered.nonchimera.fasta BZ      BZ27    7       chemerin_KO     wk6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe Barcode and LinkerPrimerSequence are absent for this tutorial, these would be used for assigning multiplexed reads to samples and for quality control. Our tutorial data set is already de-multiplexed. What QIIME could be used? \n\n\nExecute the following command and test the mapping file for potential errors:\n\n\n1\nvalidate_mapping_file.py -m map.txt -o map_output\n\n\n\n\n\n\nThere shouldn\u2019t be any errors, but if errors occur a corrected mapping file will be written to the directory map_output\n\n\nJoin directory of PE reads\n\n\nFirst we need to join the overlapping Illumina Pair End (PE) reads contained in the fastq directory for each sample.\n\n\n1\n2\ncd ~/Desktop/16S_chemerin_tutorial  \nrun_pear.pl -p 4 -o stitched_reads fastq/*fastq\n\n\n\n\n\n\n1\n2\n-p 4\n indicates this job should be run on 4 CPU \n\n-o stitched_reads\n indicates that the output folder)\n\n\n\n\n\n\nFour FASTQ files will be generated for each set of paired-end reads:\n    (1) assembled reads (used for downstream analyses)\n    (2) discarded reads (often abnormal reads, e.g. large run of Ns).\n    (3) unassembled forward reads\n    (4) unassembled reverse reads\n\n\nThe default log file \npear_summary_log.txt\n contains the percent of reads either assembled, discarded or unassembled.\n\n\n\n\nQuestion 1\n\n\nWhat percent of reads were successfully stitched for sample 40CMK6WT?\n\n\n\n\n\n\nAnswer\n\n\n96.2%\n\n\n\n\nFiltering reads by quality and length\n\n\nWe will now filter our reads on a quality score cut-off of 30 over 90% of bases and at a maximum length of 400 bp, which are considered reasonable filtering criteria (~2 min on 1 CPU):\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nread_filter.pl -q \n30\n -p \n90\n -l \n400\n -thread \n4\n -c both stitched_reads/*.assembled*fastq\n\n\n\n\n\n\nThe \n-c both\n option above checks for the default forward and reverse degenerate primer sequences to match exactly in each sequences (You can use whatever primer sequences you want. However, the primer sequences in this case are set by default in the script, which you can look at with \nread_filter.pl -h\n). If you don\nt set the \n-c\n option to either \nboth\n or \nforward\n then there wont be any primer matching.\n\n\nBy default this script will output filtered FASTQs in a folder called \nfiltered_reads\n and the percent of reads thrown out after each filtering step is recorded in \nread_filter_log.txt\n. This script is just a convenient way to run two popular tools for read filtering: \nFASTX-toolkit and BBMAP\n.\n\n\nIf you look in this logfile you will note that ~40% of reads were filtered out for each sample. You can also see the counts and percent of reads dropped at each step.\n\n\n\n\nQuestion 2\n\n\nHow many reads of sample 36CMK6WT were filtered out for not containing a match to the forward primer (which is the default setting in this case).\n\n\n\n\n\n\nAnswer\n\n\n88.96%\n\n\n\n\nConversion to FASTA and removal of chimeric reads\n\n\nThe next steps in the pipeline require the simple conversion of sequences from FASTQ to FASTA format, using the following command (\n 1 min on 1 CPU):\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nrun_fastq_to_fasta.pl -p \n4\n -o fasta_files filtered_reads/*fastq\n\n\n\n\n\n\nNote that this command removes any sequences containing \nN\n (a fully ambiguous base read), which is \n 1% of the reads after the read filtering steps above.\n\n\nDuring PCA amplification 16S rRNA sequences from different organisms can sometimes combine to form hybrid molecules called chimeric sequences. It\ns important to remove these so they aren\nt incorrectly called as novel Operational Taxonomic Units (OTUs). Unfortunately, not all chimeric reads will be removed during this step, which is something to keep in mind during the next steps.\n\n\nYou can run chimera checking with VSEARCH with this command (~3 min on 1 CPU):\n\n\n1\nchimera_filter.pl -type \n1\n -thread \n4\n -db /home/ubuntu/data/RDP_trainset16_022016.fa fasta_files/*fasta\n\n\n\n\n\n\nThis script will remove any reads called either ambiguously or as chimeric, and output the remaining reads in the \nnon_chimeras\n folder by default.\n\n\nBy default the logfile \nchimeraFilter_log.txt\n is generated containing the counts and percentages of reads filtered out for each sample.\n\n\n\n\nQuestion 3\n\n\nWhat is the mean percent of reads retained after this step, based on the output in the log file (\nnonChimeraCallsPercent\n column)?\n\n\n\n\n\n\nAnswer\n\n\n54.35%\n\n\n\n\n\n\nQuestion 4\n\n\nWhat percent of stitched reads was retained for sample 75CMK8KO after all the filtering steps \n\n\n\n\n\n\nHint\n\n\nyou will need to compare the original number of reads to the number of reads output by chimera_filter.pl)?\n\n\n\n\nAssign samples to the reads\n\n\nNow that we have adequately prepared the reads, we can now run OTU picking using QIIME. An Operational Taxonomic Unit (\nOTU\n) defines a taxonomic group based on sequence similarity among sampled organisms. QIIME software clusters sequence reads from microbial communities in order to classify its constituent micro-organisms into OTUs. QIIME requires FASTA files to be input in a specific format (specifically, sample names need to be at the beginning of each header line). We have provided the mapping file (\nmap.txt\n), which links filenames to sample names and metadata.\n\n\nAs we saw the map.txt (e.g. with \nless -S\n) had 2 columns without any data: \nBarcodeSequence\n and \nLinkerPrimerSequence\n. We don\nt need to use these columns today. These would normally be populated for multiplexed data.\n\n\nAlso, you will see that the \nFileInput\n column contains the names of each FASTA file, which is what we need to specify for the command below.\n\n\nThis command will correctly format the input FASTA files and output a single FASTA:\n\n\n1\nadd_qiime_labels.py -i non_chimeras/ -m map.txt -c FileInput -o combined_fasta \n\n\n\n\n\n\nIf you take a look at \ncombined_fasta/combined_seqs.fna\n you can see that the first column of header line is a sample name taken from the mapping file.\n\n\nDe novo OTU picking\n\n\nNow that the input file has been correctly formatted we can run the actual OTU picking program.\n\n\nSeveral parameters for this program can be specified into a text file, which will be read in by \npick_open_reference_otus.py\n:\n\n\n1\n2\n3\necho\n \npick_otus:threads 1\n \n clustering_params.txt\n\necho\n \npick_otus:sortmerna_coverage 0.8\n \n clustering_params.txt\n\necho\n \npick_otus:sortmerna_db /home/ubuntu/data/97_otus\n \n clustering_params.txt\n\n\n\n\n\n\nWe will be using the \nuclust method\n of \nopen-reference OTU picking\n. In open-reference OTU picking, reads are first clustered against a reference database; then, a certain percent (10% in the below command) of those reads that failed to be classified are sub-sampled to create a new reference database and the remaining unclassified reads are clustered against this new database. This \nde novo\n clustering step is repeated again by default using the below command (can be turned off to save time with the \n\u2013suppress_step4\n option).\n\n\nWe are actually also retaining singletons (i.e. OTUs identified by 1 read), which we will then remove in the next step. Note that \n$PWD\n is just a variable that contains the path to your current directory. This command takes ~7 min with 1 CPU. Lowering the \n-s\n parameter\ns value will greatly affect running speed.\n\n\n1\npick_open_reference_otus.py -i \n$PWD\n/combined_fasta/combined_seqs.fna -o \n$PWD\n/clustering/ -p \n$PWD\n/clustering_params.txt -m sortmerna_sumaclust -s \n0\n.1 -v --min_otu_size \n1\n\n\n\n\n\n\n\nRemove low confidence OTUs\n\n\nWe will now remove low confidence OTUs, i.e. those that are called by a low number of reads. It\ns difficult to choose a hard cut-off for how many reads are needed for an OTU to be confidently called, since of course OTUs are often at low frequency within a community. A reasonable approach is to remove any OTU identified by fewer than 0.1% of the reads, given that 0.1% is the estimated amount of sample bleed-through between runs on the Illumina Miseq:\n\n\n1\nremove_low_confidence_otus.py -i $PWD/clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o $PWD/clustering/otu_table_high_conf.biom\n\n\n\n\n\n\nSince we are just doing a test run with few sequences, the threshold is 1 OTU regardless. However, this is an important step with real datasets. Note: Sequence errors can give rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.\n\n\nWe can compare the summaries of these two BIOM files:\n\n\n1\n2\n3\nbiom summarize-table -i clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt\n\nbiom summarize-table -i clustering/otu_table_high_conf.biom -o clustering/otu_table_high_conf_summary.txt\n\n\n\n\n\n\nThe first four lines of clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt are:\n\n\n1\n2\n3\n4\n5\nNum samples: 24\nNum observations: 2420\nTotal count: 12014\nTable density (fraction of non-zero values): 0.097\nThis means that for the 24 separate samples, 2420 OTUs were called based on 12014 reads. Only 9.7% of the values in the sample x OTU table are non-zero, meaning that most OTUs are in a small number of samples.\n\n\n\n\n\n\nIn contrast, the first four lines of clustering/otu_table_high_conf_summary.txt are:\n\n\n1\n2\n3\n4\nNum samples: 24\nNum observations: 884\nTotal count: 10478\nTable density (fraction of non-zero values): 0.193\n\n\n\n\n\n\nAfter removing low-confidence OTUs, only \n36.5%\n were retained: \nthe number of OTUs dropped from 2420 to 884\n. This effect is generally even more drastic for bigger datasets. However, the numbers of reads only dropped from 12014 to 10478 (so \n87% of the reads were retained\n). You can also see that the table density increased, as we would expect.\n\n\nThe pipeline creates a Newick-formatted phylogenetic\ntree **(\n.tre)*\n in the clustering directory.\nYou can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by typing \u2019figtree\u2019 then hit the return key. \n\n\n1\nfigtree\n\n\n\n\nView the tree by opening the file \u2019*.tre\u2019 in the \u2019clustering\u2019 folder \n(Desktop-\nTaxonomy-\notus)\n. The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 6, which produces a far more useful tree. \n\n\nMegan can be opened from the terminal by typing \nMEGAN\n. If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN6_registration_for_academic_use.txt. From the File menu select Import -\n BIOM format.\nFind your biom file and import it.\n\n\nMegan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.\n\n\nThe Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.\n\n\nView OTU statistics\n\n\nYou can generate some statistics, e.g. the number of reads assigned, distribution among samples. Some of the statistics are useful for further downstream analysis, e.g. beta-diversity analysis.  \nWrite down the minimum value under Counts/sample summary\n. We need it for beta-diversity analysis.\nYou can look at the read depth per sample in \nclustering/otu_table_high_conf_summary.txt\n, here are the first five samples (they are sorted from smallest to largest):\n\n\n1\n2\n3\n4\n5\n6\nCounts/sample detail:\n106CHE6WT: 375.0\n111CHE6KO: 398.0\n39CMK6KO: 410.0\n113CHE6WT: 412.0\n108CHE6KO: 413.0\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWhat is the read depth for sample \n75CMK8KO\n?\n\n\n\n\n\n\nAnswer\n\n\n494\n\n\n\n\nWe need to subsample the number of reads for each sample to the same depth, which is necessary for several downstream analyses. This is called \nrarefaction\n, a technique that provides an indication of \nspecies richness\n for a given number of samples. First it indicates if you have sequence enough to identify all species. Second we want to rarify the read depth of samples to a similar number of reads for comparative analysis. There is actually quite a lot of debate about whether rarefaction is necessary (since it throws out data), but it is still the standard method used in microbiome studies. We want to rarify the read depth to the sample with the lowest \nreasonable\n number of reads. Of course, a \nreasonable\n read depth is quite subjective and depends on how much variation there is between samples.\n\n\nRarify reads\n\n\n1\n2\nmkdir final_otu_tables\nsingle_rarefaction.py -i clustering/otu_table_high_conf.biom -o final_otu_tables/otu_table.biom -d \n355\n\n\n\n\n\n\n\nVisualize taxonomic composition\n\n\n\n\nWe will now group sequences by taxonomic assignment at various levels. The following command produces a number of charts that can be viewed in a browser. The command takes about 5 minutes to complete\n\n\n1\nsummarize_taxa_through_plots.py -i final_otu_tables/otu_table.biom -o  wf_taxa_summary -m map.txt \n\n\n\n\n\n\nTo view the output, open a web browser from the Applications -\nInternet menu. You can use Google chrome, Firefox or Chromium. In  Firefox use the File menu to select \n\n\n1\n2\nDesktop -\n;Taxonomy -\n; wf_taxa_summary -\n; taxa_summary_plots \nand open either area_charts.html or bar_chars.html. \n\n\n\n\n\n\nI prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the samples. The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the top) are very useful for discovering how the communities in your samples differ from each other. \n\n\nAlpha diversity within samples and rarefaction curves\n\n\n\n\nAlpha diversity is the microbial diversity within a sample. QIIME can calculate a lot of metrics, but for our tutorial, we generate 3 metrics from the alpha rarefaction workflow: chao1 (estimates species richness); observed species metric (the count of unique OTUs); phylogenetic distance. The following workflow generates rarefaction plots to visualize alpha diversity.\n\n\nRun the following command from within your taxonomy directory, this should take a few minutes:\n\n\n1\nalpha_rarefaction.py -i final_otu_tables/otu_table.biom -o plots/alpha_rarefaction_plot -t clustering/rep_set.tre --min_rare_depth \n40\n --max_rare_depth \n355\n -m map.txt  --num_steps \n10\n\n\n\n\n\n\n\nFirst we are going to view the rarefaction curves in a web browser by opening the resulting HTML file to view the plots: \n\n\n1\nplots/alpha_rarefaction_plot/alpha_rarefaction_plots/rarefaction_plots.html\n\n\n\n\n\n\nChoose \nobserved_otus\n as the metric and \nSource\n as the category. You should see this plot:\n\n\n\n\nThere is no difference in the number of OTUs identified in the guts of mice from the BZ facility than the CJS facility, based on this dataset. However, since the rarefaction curves have not reached a plateau, it is likely that this comparison is just incorrect to make with so few reads. Indeed, with the full dataset you do see a difference in the number of OTUs.\n\n\nIn general the more reads you have, the more OTUs you will observe. If a rarefaction curve start to flatten, it means that you have probably sequenced at sufficient depth, in other words, producing more reads will not significantly add more OTUs. If on the other hand hasn\u2019t flattened, you have not sampled enough to capture enough of the microbial diversity and by extrapolating the curve you may be able to estimate how many more reads you will need. Consult the QIIME overview tutorial for further information.\n\n\nRun the following command from within your taxonomy directory, this should take a few minutes to generate a heatmap of the level three taxonomy:\n\n\n1\nmake_otu_heatmap.py -i final_otu_tables/otu_table_L3.biom -o final_otu_tables/otu_table_L3_heatmap.pdf -c Treatment -m map.txt\n\n\n\n\n\n\nBeta diversity and beta diversity plots\n\n\nBeta diversity analysis, is the assessment of differences between microbial communities/samples. As we have already observed, our samples contain different numbers of sequences. The first step is to remove sample heterogeneity by randomly selecting the same number of reads from every sample. This number corresponds to the \u2019minimum\u2019 number recorded when you looked at the OTU statistics.  Now run the following command\n\n\n1\nbeta_diversity_through_plots.py -i final_otu_tables/otu_table.biom -m map.txt -o bdiv_even -t otus/rep_set.tre -e \n355\n\n\n\n\n\n\n\nGood data quality and sample metadata is important for visualising metagenomics analysis. The output of these comparisons is a square matrix where a distance or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering.\n\n\nTesting for statistical differences\n\n\nSo what\ns the next step? Since we know that source facility is such an important factor, we could analyze samples from each facility separately. This will lower our statistical power to detect a signal, but otherwise we cannot easily test for a difference between genotypes.\n\n\nTo compare the genotypes within the two source facilities separately we fortunately don\nt need to re-run the OTU-picking. Instead, we can just take different subsets of samples from the final OTU table. First though we need to make two new mapping files with the samples we want to subset:\n\n\n1\n2\nhead -n \n1\n map.txt \n map_BZ.txt\n;\n awk \n{ if ( $3 == \nBZ\n ) { print $0 } }\n map.txt \nmap_BZ.txt\nhead -n \n1\n map.txt \n map_CJS.txt\n;\n awk \n{ if ( $3 == \nCJS\n ) { print $0 } }\n map.txt \nmap_CJS.txt\n\n\n\n\n\n\nThese commands are split into 2 parts (separated by \n;\n). The first part writes the header line to each new mapping file. The second part is an awk command that prints any line where the 3\nrd\n column equals the id of the source facility. Note that awk splits by any whitespace by default, which is why the source facility IDs are in the 3\nrd\n column according to awk, even though we know this isn\nt true when the file is tab-delimited.\n\n\nThe BIOM \nsubset-table\n command requires a text file with 1 sample name per line, which we can generate by these quick bash commands:\n\n\n1\n2\ntail -n +2  map_BZ.txt \n|\n awk \n{print $1}\n \n samples_BZ.txt\ntail -n +2  map_CJS.txt \n|\n awk \n{print $1}\n \n samples_CJS.txt\n\n\n\n\n\n\nThese commands mean that the first line (the header) should be ignored and then the first column should be printed to a new file. We can now take the two subsets of samples from the BIOM file:\n\n\n1\n2\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_BZ.txt -o final_otu_tables/otu_table_BZ.biom\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_CJS.txt -o final_otu_tables/otu_table_CJS.biom\n\n\n\n\nWe can now re-create the beta diversity plots for each subset:\n\n\n1\n2\nbeta_diversity_through_plots.py -m map_BZ.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_BZ.biom -o plots/bdiv_otu_BZ \nbeta_diversity_through_plots.py -m map_CJS.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_CJS.biom -o plots/bdiv_otu_CJS\n\n\n\n\nWe can now take a look at whether the genotypes separate in the re-generated weighted beta diversity PCoAs for each source facility separately.\n\n\nFor the BZ source facility:\n\n\n\n\nAnd for the CJS source facility:\n\n\n\n\nJust by looking at these PCoA plots it\ns clear that if there is any difference it\ns subtle. To statistically evaluate whether the weighted UniFrac beta diversities differ among genotypes within each source facility, you can run an analysis of similarity (ANOSIM) test. These commands will run the ANOSIM test and change the output filename:\n\n\n1\n2\ncompare_categories.py --method anosim -i plots/bdiv_otu_BZ/weighted_unifrac_dm.txt -m map_BZ.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_BZ.txt \n\n\n\n\n\n1\n2\ncompare_categories.py --method anosim -i plots/bdiv_otu_CJS/weighted_unifrac_dm.txt -m map_CJS.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_CJS.txt\n\n\n\n\n\nYou can take a look at the output files to see significance values and test statistics. The P-values for both tests are \n 0.05, so there is no significant difference in the UniFrac beta diversities of different genotypes within each source facility.\n\n\nUniFrac beta diversity analysis\n\n\nUniFrac is a particular beta-diversity measure that analyzes dissimilarity between samples, sites, or communities. We will now create UniFrac beta diversity (both \nweighted\n and \nunweighted\n) principal coordinates analysis (\nPCoA\n) plots. PCoA plots are related to principal components analysis (PCA) plots, but are based on any dissimilarity matrix rather than just a covariance/correlation matrix. \n\n\nNote\n the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then it\ns likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).\n\n\nQIIME \nbeta_diversity_through_plots.py\n takes the OTU table as input, as well as file which contains the phylogenetic relatedness between all clustered OTUs. One HTML file will be generated for the weighted and unweighted beta diversity distances:\n\n\n1\n2\nplots/bdiv_otu/weighted_unifrac_emperor_pcoa_plot/index.html\nplots/bdiv_otu/unweighted_unifrac_emperor_pcoa_plot/index.html\n\n\n\n\n\n\nOpen the weighted HTML file in your browser and take a look, you should see a PCoA very similar to this:\n\n\n\n\nThe actual metadata we are most interested in for this dataset is the \ngenotype\n column of the mapping file, which contains the different genotypes I described at the beginning of this tutorial. Go to the \nColors\n tab of the Emperor plot (which is what we were just looking at) and change the selection from \nBarcodeSequence\n (default) to \ngenotype\n. You should see a similar plot to this:\n\n\n\n\nThe WT genotype is spread out across both knockout genotypes, which is not what we would have predicted.\n\n\nYou\nll see what\ns really driving the differences in beta diversity when you change the selection under the \nColors\n tab from \ngenotype\n to \nSource\n:\n\n\n\n\nUsing STAMP to test for particular differences\n\n\nOften we\nre interested in figuring out which particular taxa (or other feature such as functions) differs in relative abundance between groups. There are many ways this can be done, but one common method is to use the easy-to-use program \nSTAMP\n. We\nll run STAMP on the full OTU table to figure out which genera differ between the two source facilities as an example.\n\n\nBefore running STAMP we need to convert our OTU table into a format that STAMP can read:\n\n\n1\nbiom_to_stamp.py -m taxonomy final_otu_tables/otu_table.biom \nfinal_otu_tables/otu_table.spf\n\n\n\n\n\n\nIf you take a look at \nfinal_otu_tables/otu_table.spf\n with less you\nll see that it\ns just a simple tab-delimited table.\n\n\nNow we\nre ready to open up STAMP, which you can either do by typing STAMP on the command-line or by clicking the side-bar icon.\n\n\nLoad \notu_table.spf\n as the Profile file and \nmap.txt\n as the Group metadata file.\n\n\nAs a reminder, the full paths of these files should be:\n\n1\n/home/ubuntu/16S_chemerin_tutorial/final_otu_tables/otu_table.spf and /home/ubuntu/16S_chemerin_tutorial/map.txt\n\n\n\n\n\nChange the Group field to \nSource\n and the profile level to \nLevel_6\n (which corresponds to the genus level). Change the heading from \nMultiple groups\n to \nTwo groups\n. The statistical test to \nWelch\ns t-test\n and the multiple test correction to \nBenjamini-Hochberg FDR\n\n\nChange the plot type to \nBar plot\n. Look at the barplot for Prevotella and save it to a file.\n\n\n\n\nQuestion\n\n\nCan you see how many genera are significant by clicking \nShow only active features\n?\n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nThe QIIME overview tutorial at\n(\nhttp://qiime.org/tutorials/tutorial.html\n) has a number of additional steps that you may find interesting; so feel free to try some of them out. Note hat we have not installed Cytoscape, so we cannot visualize OTU networks.\n\n\nWe will end this tutorial with a summary of what we have done and how well our analysis compares with the one in the paper.\n\n\nHopefully you will have acquired new skills that allow you to tackle your own taxonomic analyses. There are many more tutorials on the QIIME website that can help you pick the best strategy for your project (\nhttp://qiime.org/tutorials/\n) and \nhttps://github.com/mlangill/microbiome_helper/wiki/\n. There are alternatives that might suit your need better (e.g. VAMPS at \nhttp://vamps.mbl.ed\n; mothur at \nhttp://www.mothur.org\n) and others.", 
            "title": "Metagenomics Taxonomic"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#an-introduction-to-taxonomic-analysis-of-amplicon-and-shotgun-data-using-qiime", 
            "text": "", 
            "title": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Understand the open source software package QIIME for analysis    Perform a taxonomic analysis on a 16S rRNA amplicon dataset", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#resources-you-will-be-using", 
            "text": "", 
            "title": "Resources you will be using"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#tools-used-16s-analysis", 
            "text": "QIIME :   http://qiime.org/  PEAR:  http://sco.h-its.org/exelixis/web/software/pear/doc.html  FASTX toolkit (v0.0.14):  http://hannonlab.cshl.edu/fastx_toolkit/download.html  BBMap:  http://sourceforge.net/projects/bbmap  VSEARCH:  https://github.com/torognes/vsearch  SortMeRNA:  http://bioinfo.lifl.fr/RNA/sortmerna/  STAMP:  http://kiwi.cs.dal.ca/Software/STAMP", 
            "title": "Tools Used 16S Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#useful-links", 
            "text": "FASTQ Encoding:     http://en.wikipedia.org/wiki/FASTQ_format#Encoding  QIIME Tutorials: http://qiime.org/tutorials/tutorial.html  16S Tutorials: https://github.com/mlangill/microbiome_helper/wiki/16S-tutorial-(chemerin )", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#sources-of-data", 
            "text": "Data : Mouse gut microbial composition affected by the protein chemerin.   https://www.dropbox.com/s/4fqgi6t3so69224/Sinal_Langille_raw_data.tar.gz  https://www.dropbox.com/s/r2jqqc7brxg4jhx/16S_chemerin_tutorial.zip    RDP_trainset16_022016.fa (20 MB) is a subset of the Ribosome Database Project (RDP) filtered to include only bacteria. https://www.dropbox.com/s/qnlsv7ve2lg6qfp/RDP_trainset16_022016.fa?dl=1", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#overview", 
            "text": "In this tutorial we will look at the open source software package QIIME (pronounced \u2019chime\u2019). QIIME stands for Quantitative Insights Into Microbial Ecology. The package contains many tools that enable users to analyse and compare microbial communities.   After completion of this tutorial, you should be able to perform a taxonomic analysis on a Illumina pair end 16S rRNA amplicon dataset", 
            "title": "Overview"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#de-novo-otu-picking-and-diversity-analysis-using-illumina-data", 
            "text": "", 
            "title": "De novo OTU picking and diversity analysis using Illumina data"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#introduction", 
            "text": "The workflow for 16S analysis in general is as follows:   Split multiplexed reads to samples  Join overlapping read pairs  Filter reads on quality and length  Filter Chimera sequences  Assign reads to samples  Pick operational taxonomic units (OTUs) for each sample  Alpha diversity analysis and rarefaction  Beta diversity analysis and Taxonomic composition  PCA analysis   16S analysis is a method of microbiome analysis (compared to shotgun metagenomics) that targets the 16S ribosomal RNA gene, as this gene is present in all prokaryotes. It features regions that are conserved among these organisms, as well as variable regions that allow distinction among organisms. These characteristics make this gene useful for analyzing microbial communities at reduced cost compared to metagenomic techniques. A similar workflow can be applied to eukaryotic micro-organisms using the 18S rRNA gene.  The tutorial dataset was originally used in a project to determine whether knocking out the protein chemerin affects gut microbial composition. Originally 116 mouse samples acquired from two different facilities were used for this project (only 24 samples were used in this tutorial dataset, for simplicity).", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#the-mapping-file", 
            "text": "Metadata associated with each sample is indicated in the mapping file (map.txt). The mapping file associates the read data files for a sample to it s metadata. The mapping file can contain information on your experimental design. The format is very strict; columns are separated with a single TAB character; the header names have to be typed exactly as specified in the documentation. A good sample description is useful as it is used in the legends of the figures QIIME generates.  In the mapping (map.txt) file the genotypes of interest can be seen: wildtype (WT), chemerin knockout (chemerin_KO), chemerin receptor knockout (CMKLR1_KO) and a heterozygote for the receptor knockout (HET). Also of importance are the two source facilities:  BZ  and  CJS . It is important to include as much metadata as possible, so that it can be easily explored later on.  Open Terminal and go to the dataset s directory:  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nls -lhtr   fastq  is the directory containing all the sequencing files, which we are going to process. The file  map.txt  contains metadata about the samples. We can look at it with the less command (hit  q  to exit):  1 less -S map.txt   The first column is the sample IDs, the next 2 are blank (note the file is tab-delimited, meaning each column is separated by a tab, not just whitespace) and the 4 th  column contains FASTA filenames (these filenames are based on what we will produce in this pipeline). The rest of the columns are metadata about the samples.  Here is what the first 4 lines should look like:  1\n2\n3\n4 SampleID       BarcodeSequence LinkerPrimerSequence    FileInput       Source  Mouse#  Cage#   genotype        SamplingWeek\n105CHE6WT                       105CHE6WT_S325_L001.assembled_filtered.nonchimera.fasta BZ      BZ25    7       WT      wk6\n106CHE6WT                       106CHE6WT_S336_L001.assembled_filtered.nonchimera.fasta BZ      BZ26    7       WT      wk6\n107CHE6KO                       107CHE6KO_S347_L001.assembled_filtered.nonchimera.fasta BZ      BZ27    7       chemerin_KO     wk6    Note   The Barcode and LinkerPrimerSequence are absent for this tutorial, these would be used for assigning multiplexed reads to samples and for quality control. Our tutorial data set is already de-multiplexed. What QIIME could be used?   Execute the following command and test the mapping file for potential errors:  1 validate_mapping_file.py -m map.txt -o map_output   There shouldn\u2019t be any errors, but if errors occur a corrected mapping file will be written to the directory map_output", 
            "title": "The Mapping file"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#join-directory-of-pe-reads", 
            "text": "First we need to join the overlapping Illumina Pair End (PE) reads contained in the fastq directory for each sample.  1\n2 cd ~/Desktop/16S_chemerin_tutorial  \nrun_pear.pl -p 4 -o stitched_reads fastq/*fastq   1\n2 -p 4  indicates this job should be run on 4 CPU  -o stitched_reads  indicates that the output folder)   Four FASTQ files will be generated for each set of paired-end reads:\n    (1) assembled reads (used for downstream analyses)\n    (2) discarded reads (often abnormal reads, e.g. large run of Ns).\n    (3) unassembled forward reads\n    (4) unassembled reverse reads  The default log file  pear_summary_log.txt  contains the percent of reads either assembled, discarded or unassembled.   Question 1  What percent of reads were successfully stitched for sample 40CMK6WT?    Answer  96.2%", 
            "title": "Join directory of PE reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#filtering-reads-by-quality-and-length", 
            "text": "We will now filter our reads on a quality score cut-off of 30 over 90% of bases and at a maximum length of 400 bp, which are considered reasonable filtering criteria (~2 min on 1 CPU):  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nread_filter.pl -q  30  -p  90  -l  400  -thread  4  -c both stitched_reads/*.assembled*fastq   The  -c both  option above checks for the default forward and reverse degenerate primer sequences to match exactly in each sequences (You can use whatever primer sequences you want. However, the primer sequences in this case are set by default in the script, which you can look at with  read_filter.pl -h ). If you don t set the  -c  option to either  both  or  forward  then there wont be any primer matching.  By default this script will output filtered FASTQs in a folder called  filtered_reads  and the percent of reads thrown out after each filtering step is recorded in  read_filter_log.txt . This script is just a convenient way to run two popular tools for read filtering:  FASTX-toolkit and BBMAP .  If you look in this logfile you will note that ~40% of reads were filtered out for each sample. You can also see the counts and percent of reads dropped at each step.   Question 2  How many reads of sample 36CMK6WT were filtered out for not containing a match to the forward primer (which is the default setting in this case).    Answer  88.96%", 
            "title": "Filtering reads by quality and length"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#conversion-to-fasta-and-removal-of-chimeric-reads", 
            "text": "The next steps in the pipeline require the simple conversion of sequences from FASTQ to FASTA format, using the following command (  1 min on 1 CPU):  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nrun_fastq_to_fasta.pl -p  4  -o fasta_files filtered_reads/*fastq   Note that this command removes any sequences containing  N  (a fully ambiguous base read), which is   1% of the reads after the read filtering steps above.  During PCA amplification 16S rRNA sequences from different organisms can sometimes combine to form hybrid molecules called chimeric sequences. It s important to remove these so they aren t incorrectly called as novel Operational Taxonomic Units (OTUs). Unfortunately, not all chimeric reads will be removed during this step, which is something to keep in mind during the next steps.  You can run chimera checking with VSEARCH with this command (~3 min on 1 CPU):  1 chimera_filter.pl -type  1  -thread  4  -db /home/ubuntu/data/RDP_trainset16_022016.fa fasta_files/*fasta   This script will remove any reads called either ambiguously or as chimeric, and output the remaining reads in the  non_chimeras  folder by default.  By default the logfile  chimeraFilter_log.txt  is generated containing the counts and percentages of reads filtered out for each sample.   Question 3  What is the mean percent of reads retained after this step, based on the output in the log file ( nonChimeraCallsPercent  column)?    Answer  54.35%    Question 4  What percent of stitched reads was retained for sample 75CMK8KO after all the filtering steps     Hint  you will need to compare the original number of reads to the number of reads output by chimera_filter.pl)?", 
            "title": "Conversion to FASTA and removal of chimeric reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#assign-samples-to-the-reads", 
            "text": "Now that we have adequately prepared the reads, we can now run OTU picking using QIIME. An Operational Taxonomic Unit ( OTU ) defines a taxonomic group based on sequence similarity among sampled organisms. QIIME software clusters sequence reads from microbial communities in order to classify its constituent micro-organisms into OTUs. QIIME requires FASTA files to be input in a specific format (specifically, sample names need to be at the beginning of each header line). We have provided the mapping file ( map.txt ), which links filenames to sample names and metadata.  As we saw the map.txt (e.g. with  less -S ) had 2 columns without any data:  BarcodeSequence  and  LinkerPrimerSequence . We don t need to use these columns today. These would normally be populated for multiplexed data.  Also, you will see that the  FileInput  column contains the names of each FASTA file, which is what we need to specify for the command below.  This command will correctly format the input FASTA files and output a single FASTA:  1 add_qiime_labels.py -i non_chimeras/ -m map.txt -c FileInput -o combined_fasta    If you take a look at  combined_fasta/combined_seqs.fna  you can see that the first column of header line is a sample name taken from the mapping file.", 
            "title": "Assign samples to the reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#de-novo-otu-picking", 
            "text": "Now that the input file has been correctly formatted we can run the actual OTU picking program.  Several parameters for this program can be specified into a text file, which will be read in by  pick_open_reference_otus.py :  1\n2\n3 echo   pick_otus:threads 1    clustering_params.txt echo   pick_otus:sortmerna_coverage 0.8    clustering_params.txt echo   pick_otus:sortmerna_db /home/ubuntu/data/97_otus    clustering_params.txt   We will be using the  uclust method  of  open-reference OTU picking . In open-reference OTU picking, reads are first clustered against a reference database; then, a certain percent (10% in the below command) of those reads that failed to be classified are sub-sampled to create a new reference database and the remaining unclassified reads are clustered against this new database. This  de novo  clustering step is repeated again by default using the below command (can be turned off to save time with the  \u2013suppress_step4  option).  We are actually also retaining singletons (i.e. OTUs identified by 1 read), which we will then remove in the next step. Note that  $PWD  is just a variable that contains the path to your current directory. This command takes ~7 min with 1 CPU. Lowering the  -s  parameter s value will greatly affect running speed.  1 pick_open_reference_otus.py -i  $PWD /combined_fasta/combined_seqs.fna -o  $PWD /clustering/ -p  $PWD /clustering_params.txt -m sortmerna_sumaclust -s  0 .1 -v --min_otu_size  1", 
            "title": "De novo OTU picking"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#remove-low-confidence-otus", 
            "text": "We will now remove low confidence OTUs, i.e. those that are called by a low number of reads. It s difficult to choose a hard cut-off for how many reads are needed for an OTU to be confidently called, since of course OTUs are often at low frequency within a community. A reasonable approach is to remove any OTU identified by fewer than 0.1% of the reads, given that 0.1% is the estimated amount of sample bleed-through between runs on the Illumina Miseq:  1 remove_low_confidence_otus.py -i $PWD/clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o $PWD/clustering/otu_table_high_conf.biom   Since we are just doing a test run with few sequences, the threshold is 1 OTU regardless. However, this is an important step with real datasets. Note: Sequence errors can give rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.  We can compare the summaries of these two BIOM files:  1\n2\n3 biom summarize-table -i clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt\n\nbiom summarize-table -i clustering/otu_table_high_conf.biom -o clustering/otu_table_high_conf_summary.txt   The first four lines of clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt are:  1\n2\n3\n4\n5 Num samples: 24\nNum observations: 2420\nTotal count: 12014\nTable density (fraction of non-zero values): 0.097\nThis means that for the 24 separate samples, 2420 OTUs were called based on 12014 reads. Only 9.7% of the values in the sample x OTU table are non-zero, meaning that most OTUs are in a small number of samples.   In contrast, the first four lines of clustering/otu_table_high_conf_summary.txt are:  1\n2\n3\n4 Num samples: 24\nNum observations: 884\nTotal count: 10478\nTable density (fraction of non-zero values): 0.193   After removing low-confidence OTUs, only  36.5%  were retained:  the number of OTUs dropped from 2420 to 884 . This effect is generally even more drastic for bigger datasets. However, the numbers of reads only dropped from 12014 to 10478 (so  87% of the reads were retained ). You can also see that the table density increased, as we would expect.  The pipeline creates a Newick-formatted phylogenetic\ntree **( .tre)*  in the clustering directory.\nYou can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by typing \u2019figtree\u2019 then hit the return key.   1 figtree  \nView the tree by opening the file \u2019*.tre\u2019 in the \u2019clustering\u2019 folder  (Desktop- Taxonomy- otus) . The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 6, which produces a far more useful tree.   Megan can be opened from the terminal by typing  MEGAN . If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN6_registration_for_academic_use.txt. From the File menu select Import -  BIOM format.\nFind your biom file and import it.  Megan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.  The Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.", 
            "title": "Remove low confidence OTUs"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#view-otu-statistics", 
            "text": "You can generate some statistics, e.g. the number of reads assigned, distribution among samples. Some of the statistics are useful for further downstream analysis, e.g. beta-diversity analysis.   Write down the minimum value under Counts/sample summary . We need it for beta-diversity analysis.\nYou can look at the read depth per sample in  clustering/otu_table_high_conf_summary.txt , here are the first five samples (they are sorted from smallest to largest):  1\n2\n3\n4\n5\n6 Counts/sample detail:\n106CHE6WT: 375.0\n111CHE6KO: 398.0\n39CMK6KO: 410.0\n113CHE6WT: 412.0\n108CHE6KO: 413.0    Question 5  What is the read depth for sample  75CMK8KO ?    Answer  494   We need to subsample the number of reads for each sample to the same depth, which is necessary for several downstream analyses. This is called  rarefaction , a technique that provides an indication of  species richness  for a given number of samples. First it indicates if you have sequence enough to identify all species. Second we want to rarify the read depth of samples to a similar number of reads for comparative analysis. There is actually quite a lot of debate about whether rarefaction is necessary (since it throws out data), but it is still the standard method used in microbiome studies. We want to rarify the read depth to the sample with the lowest  reasonable  number of reads. Of course, a  reasonable  read depth is quite subjective and depends on how much variation there is between samples.", 
            "title": "View OTU statistics"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#rarify-reads", 
            "text": "1\n2 mkdir final_otu_tables\nsingle_rarefaction.py -i clustering/otu_table_high_conf.biom -o final_otu_tables/otu_table.biom -d  355", 
            "title": "Rarify reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#visualize-taxonomic-composition", 
            "text": "We will now group sequences by taxonomic assignment at various levels. The following command produces a number of charts that can be viewed in a browser. The command takes about 5 minutes to complete  1 summarize_taxa_through_plots.py -i final_otu_tables/otu_table.biom -o  wf_taxa_summary -m map.txt    To view the output, open a web browser from the Applications - Internet menu. You can use Google chrome, Firefox or Chromium. In  Firefox use the File menu to select   1\n2 Desktop - ;Taxonomy - ; wf_taxa_summary - ; taxa_summary_plots \nand open either area_charts.html or bar_chars.html.    I prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the samples. The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the top) are very useful for discovering how the communities in your samples differ from each other.", 
            "title": "Visualize taxonomic composition"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#alpha-diversity-within-samples-and-rarefaction-curves", 
            "text": "Alpha diversity is the microbial diversity within a sample. QIIME can calculate a lot of metrics, but for our tutorial, we generate 3 metrics from the alpha rarefaction workflow: chao1 (estimates species richness); observed species metric (the count of unique OTUs); phylogenetic distance. The following workflow generates rarefaction plots to visualize alpha diversity.  Run the following command from within your taxonomy directory, this should take a few minutes:  1 alpha_rarefaction.py -i final_otu_tables/otu_table.biom -o plots/alpha_rarefaction_plot -t clustering/rep_set.tre --min_rare_depth  40  --max_rare_depth  355  -m map.txt  --num_steps  10    First we are going to view the rarefaction curves in a web browser by opening the resulting HTML file to view the plots:   1 plots/alpha_rarefaction_plot/alpha_rarefaction_plots/rarefaction_plots.html   Choose  observed_otus  as the metric and  Source  as the category. You should see this plot:   There is no difference in the number of OTUs identified in the guts of mice from the BZ facility than the CJS facility, based on this dataset. However, since the rarefaction curves have not reached a plateau, it is likely that this comparison is just incorrect to make with so few reads. Indeed, with the full dataset you do see a difference in the number of OTUs.  In general the more reads you have, the more OTUs you will observe. If a rarefaction curve start to flatten, it means that you have probably sequenced at sufficient depth, in other words, producing more reads will not significantly add more OTUs. If on the other hand hasn\u2019t flattened, you have not sampled enough to capture enough of the microbial diversity and by extrapolating the curve you may be able to estimate how many more reads you will need. Consult the QIIME overview tutorial for further information.  Run the following command from within your taxonomy directory, this should take a few minutes to generate a heatmap of the level three taxonomy:  1 make_otu_heatmap.py -i final_otu_tables/otu_table_L3.biom -o final_otu_tables/otu_table_L3_heatmap.pdf -c Treatment -m map.txt", 
            "title": "Alpha diversity within samples and rarefaction curves"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#beta-diversity-and-beta-diversity-plots", 
            "text": "Beta diversity analysis, is the assessment of differences between microbial communities/samples. As we have already observed, our samples contain different numbers of sequences. The first step is to remove sample heterogeneity by randomly selecting the same number of reads from every sample. This number corresponds to the \u2019minimum\u2019 number recorded when you looked at the OTU statistics.  Now run the following command  1 beta_diversity_through_plots.py -i final_otu_tables/otu_table.biom -m map.txt -o bdiv_even -t otus/rep_set.tre -e  355    Good data quality and sample metadata is important for visualising metagenomics analysis. The output of these comparisons is a square matrix where a distance or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering.", 
            "title": "Beta diversity and beta diversity plots"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#testing-for-statistical-differences", 
            "text": "So what s the next step? Since we know that source facility is such an important factor, we could analyze samples from each facility separately. This will lower our statistical power to detect a signal, but otherwise we cannot easily test for a difference between genotypes.  To compare the genotypes within the two source facilities separately we fortunately don t need to re-run the OTU-picking. Instead, we can just take different subsets of samples from the final OTU table. First though we need to make two new mapping files with the samples we want to subset:  1\n2 head -n  1  map.txt   map_BZ.txt ;  awk  { if ( $3 ==  BZ  ) { print $0 } }  map.txt  map_BZ.txt\nhead -n  1  map.txt   map_CJS.txt ;  awk  { if ( $3 ==  CJS  ) { print $0 } }  map.txt  map_CJS.txt   These commands are split into 2 parts (separated by  ; ). The first part writes the header line to each new mapping file. The second part is an awk command that prints any line where the 3 rd  column equals the id of the source facility. Note that awk splits by any whitespace by default, which is why the source facility IDs are in the 3 rd  column according to awk, even though we know this isn t true when the file is tab-delimited.  The BIOM  subset-table  command requires a text file with 1 sample name per line, which we can generate by these quick bash commands:  1\n2 tail -n +2  map_BZ.txt  |  awk  {print $1}    samples_BZ.txt\ntail -n +2  map_CJS.txt  |  awk  {print $1}    samples_CJS.txt   These commands mean that the first line (the header) should be ignored and then the first column should be printed to a new file. We can now take the two subsets of samples from the BIOM file:  1\n2 biom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_BZ.txt -o final_otu_tables/otu_table_BZ.biom\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_CJS.txt -o final_otu_tables/otu_table_CJS.biom  \nWe can now re-create the beta diversity plots for each subset:  1\n2 beta_diversity_through_plots.py -m map_BZ.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_BZ.biom -o plots/bdiv_otu_BZ \nbeta_diversity_through_plots.py -m map_CJS.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_CJS.biom -o plots/bdiv_otu_CJS  \nWe can now take a look at whether the genotypes separate in the re-generated weighted beta diversity PCoAs for each source facility separately.  For the BZ source facility:   And for the CJS source facility:   Just by looking at these PCoA plots it s clear that if there is any difference it s subtle. To statistically evaluate whether the weighted UniFrac beta diversities differ among genotypes within each source facility, you can run an analysis of similarity (ANOSIM) test. These commands will run the ANOSIM test and change the output filename:  1\n2 compare_categories.py --method anosim -i plots/bdiv_otu_BZ/weighted_unifrac_dm.txt -m map_BZ.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_BZ.txt    1\n2 compare_categories.py --method anosim -i plots/bdiv_otu_CJS/weighted_unifrac_dm.txt -m map_CJS.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_CJS.txt   You can take a look at the output files to see significance values and test statistics. The P-values for both tests are   0.05, so there is no significant difference in the UniFrac beta diversities of different genotypes within each source facility.", 
            "title": "Testing for statistical differences"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#unifrac-beta-diversity-analysis", 
            "text": "UniFrac is a particular beta-diversity measure that analyzes dissimilarity between samples, sites, or communities. We will now create UniFrac beta diversity (both  weighted  and  unweighted ) principal coordinates analysis ( PCoA ) plots. PCoA plots are related to principal components analysis (PCA) plots, but are based on any dissimilarity matrix rather than just a covariance/correlation matrix.   Note  the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then it s likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).  QIIME  beta_diversity_through_plots.py  takes the OTU table as input, as well as file which contains the phylogenetic relatedness between all clustered OTUs. One HTML file will be generated for the weighted and unweighted beta diversity distances:  1\n2 plots/bdiv_otu/weighted_unifrac_emperor_pcoa_plot/index.html\nplots/bdiv_otu/unweighted_unifrac_emperor_pcoa_plot/index.html   Open the weighted HTML file in your browser and take a look, you should see a PCoA very similar to this:   The actual metadata we are most interested in for this dataset is the  genotype  column of the mapping file, which contains the different genotypes I described at the beginning of this tutorial. Go to the  Colors  tab of the Emperor plot (which is what we were just looking at) and change the selection from  BarcodeSequence  (default) to  genotype . You should see a similar plot to this:   The WT genotype is spread out across both knockout genotypes, which is not what we would have predicted.  You ll see what s really driving the differences in beta diversity when you change the selection under the  Colors  tab from  genotype  to  Source :", 
            "title": "UniFrac beta diversity analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#using-stamp-to-test-for-particular-differences", 
            "text": "Often we re interested in figuring out which particular taxa (or other feature such as functions) differs in relative abundance between groups. There are many ways this can be done, but one common method is to use the easy-to-use program  STAMP . We ll run STAMP on the full OTU table to figure out which genera differ between the two source facilities as an example.  Before running STAMP we need to convert our OTU table into a format that STAMP can read:  1 biom_to_stamp.py -m taxonomy final_otu_tables/otu_table.biom  final_otu_tables/otu_table.spf   If you take a look at  final_otu_tables/otu_table.spf  with less you ll see that it s just a simple tab-delimited table.  Now we re ready to open up STAMP, which you can either do by typing STAMP on the command-line or by clicking the side-bar icon.  Load  otu_table.spf  as the Profile file and  map.txt  as the Group metadata file.  As a reminder, the full paths of these files should be: 1 /home/ubuntu/16S_chemerin_tutorial/final_otu_tables/otu_table.spf and /home/ubuntu/16S_chemerin_tutorial/map.txt   Change the Group field to  Source  and the profile level to  Level_6  (which corresponds to the genus level). Change the heading from  Multiple groups  to  Two groups . The statistical test to  Welch s t-test  and the multiple test correction to  Benjamini-Hochberg FDR  Change the plot type to  Bar plot . Look at the barplot for Prevotella and save it to a file.   Question  Can you see how many genera are significant by clicking  Show only active features ?    Answer  TBD    Bonus exercise  Bonus exercise for fast learners.    The QIIME overview tutorial at\n( http://qiime.org/tutorials/tutorial.html ) has a number of additional steps that you may find interesting; so feel free to try some of them out. Note hat we have not installed Cytoscape, so we cannot visualize OTU networks.  We will end this tutorial with a summary of what we have done and how well our analysis compares with the one in the paper.  Hopefully you will have acquired new skills that allow you to tackle your own taxonomic analyses. There are many more tutorials on the QIIME website that can help you pick the best strategy for your project ( http://qiime.org/tutorials/ ) and  https://github.com/mlangill/microbiome_helper/wiki/ . There are alternatives that might suit your need better (e.g. VAMPS at  http://vamps.mbl.ed ; mothur at  http://www.mothur.org ) and others.", 
            "title": "Using STAMP to test for particular differences"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/", 
            "text": "Mining EBI Metagenomics (EMG) output with R\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nLoad requred R packages and input data for analysis from EMG\n\n\n\n\n\n\nKnow how analysis the data with R statistical packages\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nRStudio:  \nhttps://www.rstudio.com/\n\n\nData\n\n\nAmerica Gut Project: \nEMG Project ERP012803\n\n\nAuthor: B T.F. Alako - Cochrane team: European Nucleotide Archive. \n\n\nUseful Links\n\n\nEBI Metagenomics resource (EMG) :   \nEBI MG Portal\n\n\nPart 1: Exploratory analysis of EMG portal contingency tables\n\n\n\n\nIntroduction\n\n\nThe American Gut project is the largest crowdsourced citizen science project to date. Fecal, oral, skin, and other body site samples collected from thousands of participants represent the largest human microbiome cohort in existence. Detailed health and lifestyle and diet data associated with each sample is enabling us to deeply examine associations between the human microbiome and factors such as diet (from vegan to near carnivore and everything in between), season, amount of sleep, and disease states such as IBD, diabetes, or autism spectrum disorder-as well as many other factors not listed here. The American Gut project also encompasses the British Gut and Australian Gut projects, widening the cohort beyond North America. As the project continues to grow, we will be able to identify significant associations that would not be possible with smaller, geographically and health/disease status-limited cohorts. \nEMG Project ERP012803\n\n\nWe will explore the data generated by the study above. The data is available at: \nEMG Project ERP012803\n\n\nLoading of R packages\n\n\nFirst we need to load the required R packages for analysis.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\nstringr\n)))\n      \n# String format\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\nade4\n)))\n         \n# Multivariate analysis\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\nggplot2\n)))\n      \n# Fancy plotting\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\ngrid\n)))\n         \n# Has the viewport function\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\ndplyr\n)))\n        \n# Data manipulation\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\ntidyr\n)))\n        \n# Data manipulation\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\nFactoMineR\n)))\n   \n# Multivariate analysis\n\n\nsuppressWarnings\n(\nsuppressMessages\n(\nrequire\n(\nfactoextra\n)))\n   \n# Visualize result of multivariate analysis\n\n\n\n\n\n\n\nLoad EMG input data for analysis\n\n\nWe can then load the analysis summary and metadata files for ERP0123803.\n\n\n1\n2\n3\n4\nERP012803 \n-\n \nhttps://www.ebi.ac.uk/metagenomics/projects/ERP012803/download/2.0/export?contentType=text\nexportValue=taxonomy_abundances\n\nERP012803.meta \n-\n \nhttps://www.ebi.ac.uk/metagenomics/projects/ERP012803/overview/doExport\n;\n\ngutproject \n-\n tbl_df\n(\nread.delim\n(\nfile\n=\nERP012803\n))\n\nmeta \n-\n  read.csv\n(\nfile\n=\nERP012803.meta\n)\n\n\n\n\n\n\n\nPreprocess the data\n\n\n\n\nFilter to only retain full taxonomic lineages with a species name\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\ngutproject \n-\n gutproject \n%\n%\n \n              rename\n(\ntaxonomy\n=\nX.SampleID\n)\n \n%\n%\n\n                \n#mutate(taxonomy=gsub(\n.*?s__\n,\n,taxonomy)) %\n% \n\n                mutate\n(\ntaxonomy\n=\ngsub\n(\n.*?g__\n,\n,\ntaxonomy\n))\n \n%\n%\n\n                mutate\n(\ntaxonomy\n=\ngsub\n(\n;s__\n,\n_\n,\n taxonomy\n))\n \n%\n%\n\n                  filter\n(\ntaxonomy \n!=\n \n|\n taxonomy\n==\n_\n \n)\n \n%\n%\n\n                    filter\n(\n!\ngrepl\n(\n_$\n,\n taxonomy\n))\n\n\n\n\n\n - Merge counts of species in the same experimental sample\n\n\n1\n2\n3\n4\ngutproject \n-\n gutproject \n%\n%\n \n                group_by\n(\ntaxonomy\n)\n \n%\n%\n\n                  summarize_each\n(\nfuns\n(\nsum\n))\n \n%\n%\n\n                    ungroup\n()\n\n\n\n\n\n - Use the species name as the rowname, this replaces the default numerical name \n\n\n\n\nNote\n\n\n\n\nYou can ignore warning: Setting row names on a tibble is deprecated.\n\n\n\n\nTransform the wide format data into the long format for the purpose of appending descriptive information\n\n\n\n\n1\n2\n3\ngutproject.long  \n-\n gutproject \n%\n%\n \n                      gather\n(\nsampleb\n,\n freq\n,\n ERR1072624\n:\nERR1160857\n)\n \n%\n%\n\n                        filter\n(\ntaxonomy \n!=\nRoot\n)\n\n\n\n\n\n - Retain only the sample description and the Run id from the metadata\n\n\n1\nmeta \n-\n meta \n%\n%\n select\n(\nSample.Description\n,\n Run.ID\n)\n\n\n\n\n\n - What is the composition of the data.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nmeta.strip \n-\n  tbl_df\n(\nmeta\n)\n \n%\n%\n mutate\n(\nSample.Description\n=\ngsub\n(\n \n,\n_\n,\nSample.Description\n),\n\n                         Sample.Description\n=\ngsub\n(\nAmerican_Gut_Project_\n,\n,\n Sample.Description\n),\n \n                         Sample.Description\n=\ngsub\n(\nAmerican_\n,\n,\n Sample.Description\n)\n\n                         \n)\n\nmeta.strip \n-\n meta.strip \n%\n%\n group_by\n(\nSample.Description\n)\n \n%\n%\n\n              mutate\n(\ncount\n=\nn\n())\n \n%\n%\n ungroup\n()\n \n%\n%\n\n              select\n(\nSample.Description\n,\n count\n)\n  \n%\n%\n \nunique\n()\n\n\nmeta.strip \n%\n%\n ggplot\n(\naes\n(\nx\n=\nSample.Description\n,\n y\n=\ncount\n))\n \n+\n  geom_bar\n(\nstat\n=\nidentity\n)\n \n+\n \n                geom_text\n(\naes\n(\nlabel\n=\ncount\n))\n  \n+\n theme_bw\n()\n \n+\n\n                      theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n \n+\n \n                      ggtitle\n(\nAmerican gut project\n)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the data mostly composed of?\n\n\n\n\nAppend metadata to the gut data\n\n\n\n\n1\n2\ngutproject.df \n-\n \nmerge\n(\ngutproject.long\n,\n meta\n,\n by.x\n=\nsampleb\n,\n by.y\n=\nRun.ID\n)\n\ngutproject.df \n-\n tbl_df\n(\ngutproject.df\n)\n\n\n\n\n\n - Clean up the description of sample\n\n\n1\n2\n3\n4\n5\ngutproject.df \n-\n gutproject.df \n%\n%\n \n                  mutate\n(\nSample.Description\n=\ngsub\n(\n \n,\n_\n,\nSample.Description\n),\n \n                   Sample.Description\n=\ngsub\n(\nAmerican_Gut_Project_\n,\n,\n Sample.Description\n),\n\n                   Sample.Description\n=\ngsub\n(\nAmerican_\n,\n,\n Sample.Description\n))\n \n%\n%\n\n                    ungroup\n()\n \n\n\n\n\n - Merge taxonomy occurences count per data source\n\n\n1\n2\n3\n4\n5\n6\n7\n8\ngutproject.df \n-\n gutproject.df \n%\n%\n\n                  group_by\n(\nSample.Description\n,\ntaxonomy\n)\n \n%\n%\n \n                    summarise\n(\ncum\n=\nsum\n(\nfreq\n))\n \n%\n%\n ungroup\n()\n\ngutproject.df \n-\n gutproject.df \n%\n%\n\n                  select\n(\ntaxonomy\n,\nSample.Description\n,\ncum\n)\n\n                  \n#filter(Sample.Description !=\nStool_sample\n) %\n%\n\n                  \n#filter(Sample.Description !=\nStool_sample\n \n Sample.Description !=\nMouth_sample\n )\n\n                  \n#filter( Sample.Description     !=\nVaginal_mucus_sample\n )\n\n\n\n\n\n - Transform the long format to wide format for the purpose of Mutivariate analysis\n\n\n1\n2\n3\ngutproject.df \n-\n gutproject.df \n%\n%\n\n                    spread\n(\nSample.Description\n,\n cum\n,\n fill\n=\n0\n)\n \n%\n%\n\n                      \nas.data.frame\n()\n\n\n\n\n\n\n\n1\n2\ngutproject.mat \n-\n gutproject.df \n%\n%\n select\n(\n-\nc\n(\n1\n))\n \n%\n%\n \nas.matrix\n()\n\n\nrownames\n(\ngutproject.mat\n)\n \n-\n \ngsub\n(\n\\\\[|\\\\]\n,\n,\n perl\n=\nTRUE\n,\n gutproject.df\n$\ntaxonomy\n)\n\n\n\n\n\n - Clean up the matrix\n\n\n\n\nQuestion\n\n\n\n\nIs there any association between the species and the data source?\n\n\n\n\nPerform a chi-square test of independence, this evaluates whether there is a significant dependence between species and biological samples\n\n\n\n\n1\n2\nchisq \n-\n chisq.test\n(\ngutproject.mat\n)\n\nchisq\n\n\n\n\nThe results should look like this.\n\n1\n2\n3\nPearson\ns Chi-squared test\ndata:  gutproject.mat\nX-squared = 106730000, df = 8442, p-value \n 2.2e-16\n\n\n\n\nThe species and biological samples are statistically significantly associated (p-value=0)\n\n\nPerform Correspondence Analysis (CA)\n\n\nCorrespondence Analysis (CA) is a generalized form of Principal Component Analysis for categorical data. We use CA to reduce the dimension of the data without loosing the most important information. CA is used to graphically visualize rows points and column points in a low dimensional space\n\n\n1\ngutproject.ca \n-\n CA\n(\ngutproject.mat\n,\n graph\n=\nFALSE\n)\n\n\n\n\n\n\n\n\n\nExplore the content of the CA output\n\n\n\n\n1\nsummary\n(\ngutproject.ca\n,\n nb.dec \n=\n \n2\n,\n nbelements \n=\n \n4\n,\n ncp\n=\n \n3\n)\n\n\n\n\n\n\n\nThe result of the function summary() contains the chi-square statistics and 3 tables:\n\n\n\n\nTable 1- Eigenvalues, displays the variances and the percentage of vaiances retained by each dimension.\n\n\nTable 2, Contains the coordinates, the contribution and the cos2 (quality of representation [0-1] of the first 4 active rows variable on the dimension 1, 2 and 3\n\n\nTable 3: displays the coordinates, the contribution and the cos2 of the first 4 active column variables on the dimension 1, 2 and 3\n\n\n\n\n\n\nQuestion\n\n\n\n\nCan you interprete the CA result?\n\n\n\n\nHint\n\n\n\n\ncorrelation coef., chi-square statistic\n\n\nCorrelation coefficient\n\n\nEigen values is the amount of information retained by each axis.\n\n\n1\n2\n3\n4\neig \n-\n get_eigenvalue\n(\ngutproject.ca\n)\n\ntrace \n-\n \nsum\n(\neig\n$\neigenvalue\n)\n\ncor.coef \n-\n \nsqrt\n(\ntrace\n)\n\ncor.coef\n\n\n\n\n\n\n1\n[1] 1.378614\n\n\n\n\n\n\nAs a rule of thumb a correlation above 0.2 can be considered important (Bendixen 1995, 567; Healey 2013, 289-290)\nThe correlation coef. is 1.38 in the American gut project dataset, indicating a strong association between row (species) and column(biological samples variables.). \nA more rigorous approach for examining the association is to use Chi-square statistics. \nThe association is highly significant (p-value=0) \n\n\nExplore the percentages of inertia explained by the CA dimensions, the scree plot\n\n\n1\nfviz_screeplot\n(\ngutproject.ca\n,\n barfill\n=\nwhite\n,\n addlabels\n=\nTRUE\n)\n \n+\n theme_bw\n()\n\n\n\n\n\nThe point at which the scree plot shows a bend mignt indicate an optimal dimensionality.\n\n\nHierarchical Clustering of principal components\n\n\nCompute hierarchical clustering of species of CA results\n\n\n1\ngutproject.hcpc \n-\n HCPC\n(\ngutproject.ca\n,\n graph \n=\n \nTRUE\n \n,\n nb.clust\n=\n4\n,\n order\n=\nTRUE\n \n)\n\n\n\n\n\n - Visualize the CA results\n\n\nSymmetric biplot, whereby both rows (blue) and columns (red) are represented in the same space using the principal coordinates. Only the distance between row points or the distance between column points can be reliably interpreted. \nWith a symmetric plot, only general statements can be drawn about the pattern. \nThe inter-distance between rows and column can not be interpreted.\n\n\n1\nfviz_ca_biplot\n(\ngutproject.ca\n,\n repel\n=\nTRUE\n,\n select.row \n=\n \nlist\n(\ncontrib \n=\n \n95\n),\n select.col \n=\n \nlist\n(\ncontrib \n=\n \n10\n),\n geom\n=\ntext\n)\n \n+\n theme_minimal\n()\n\n\n\n\n\n - Remove labels and add cluster centers\n\n\nA 3D map of the hierarchical clustering of the principal component.\n\n\n1\n2\n3\nplot\n(\ngutproject.hcpc\n,\n choice \n=\n3D.map\n,\n draw.tree \n=\n \nFALSE\n,\n\n     ind.names \n=\n \nFALSE\n,\n centers.plot \n=\n \nTRUE\n,\n \n     title\n=\nHierarchical clustering of the Principal Components\n)\n\n\n\n\n\n * Hierarchical Clustering of principal component.\n\n\nCompute hierarchical clustering of biological samples of CA results\n\n\n1\nres.hcpc \n-\n HCPC\n(\ngutproject.ca\n,\n cluster.CA \n=\n \ncolumns\n,\n graph \n=\n \nTRUE\n \n,\n nb.clust\n=\n4\n,\n order\n=\nTRUE\n \n)\n\n\n\n\n\nBiological sample cluster in the dataset\n using fviz_cluster and theme_bw()\n\n\nTranspose the row and column in the gut data and perform a new correspondance analysis\n\n\n1\ngutproject.ca \n-\n CA\n(\nt\n(\ngutproject.mat\n),\n ncp\n=\n4\n)\n\n\n\n\n\nSymmetric biplot \n\n\n1\n2\n3\nfviz_ca_biplot\n(\ngutproject.ca\n,\n repel\n=\nTRUE\n,\n select.row \n=\n \nlist\n(\ncontrib \n=\n \n12\n),\n \n               select.col \n=\n \nlist\n(\ncontrib \n=\n \n95\n),\n geom\n=\ntext\n)\n \n+\n\n  theme_minimal\n()\n\n\n\n\n\n - Remove labels and add cluster centers\n\n\n3D hierarchical clustering of the principal components.\n\n\n1\n2\nplot\n(\nres.hcpc\n,\n choice \n=\n3D.map\n,\n draw.tree \n=\n \nFALSE\n,\n\n     ind.names \n=\n \nFALSE\n,\n centers.plot \n=\n \nTRUE\n,\n title\n=\nHierarchical clustering of the Principal Components\n)", 
            "title": "Mining EMG with R part 1"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#mining-ebi-metagenomics-emg-output-with-r", 
            "text": "", 
            "title": "Mining EBI Metagenomics (EMG) output with R"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Load requred R packages and input data for analysis from EMG    Know how analysis the data with R statistical packages", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#tools-used", 
            "text": "RStudio:   https://www.rstudio.com/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#data", 
            "text": "America Gut Project:  EMG Project ERP012803  Author: B T.F. Alako - Cochrane team: European Nucleotide Archive.", 
            "title": "Data"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#useful-links", 
            "text": "EBI Metagenomics resource (EMG) :    EBI MG Portal", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#part-1-exploratory-analysis-of-emg-portal-contingency-tables", 
            "text": "", 
            "title": "Part 1: Exploratory analysis of EMG portal contingency tables"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#introduction", 
            "text": "The American Gut project is the largest crowdsourced citizen science project to date. Fecal, oral, skin, and other body site samples collected from thousands of participants represent the largest human microbiome cohort in existence. Detailed health and lifestyle and diet data associated with each sample is enabling us to deeply examine associations between the human microbiome and factors such as diet (from vegan to near carnivore and everything in between), season, amount of sleep, and disease states such as IBD, diabetes, or autism spectrum disorder-as well as many other factors not listed here. The American Gut project also encompasses the British Gut and Australian Gut projects, widening the cohort beyond North America. As the project continues to grow, we will be able to identify significant associations that would not be possible with smaller, geographically and health/disease status-limited cohorts.  EMG Project ERP012803  We will explore the data generated by the study above. The data is available at:  EMG Project ERP012803", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#loading-of-r-packages", 
            "text": "First we need to load the required R packages for analysis.  1\n2\n3\n4\n5\n6\n7\n8 suppressWarnings ( suppressMessages ( require ( stringr )))        # String format  suppressWarnings ( suppressMessages ( require ( ade4 )))           # Multivariate analysis  suppressWarnings ( suppressMessages ( require ( ggplot2 )))        # Fancy plotting  suppressWarnings ( suppressMessages ( require ( grid )))           # Has the viewport function  suppressWarnings ( suppressMessages ( require ( dplyr )))          # Data manipulation  suppressWarnings ( suppressMessages ( require ( tidyr )))          # Data manipulation  suppressWarnings ( suppressMessages ( require ( FactoMineR )))     # Multivariate analysis  suppressWarnings ( suppressMessages ( require ( factoextra )))     # Visualize result of multivariate analysis", 
            "title": "Loading of R packages"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#load-emg-input-data-for-analysis", 
            "text": "We can then load the analysis summary and metadata files for ERP0123803.  1\n2\n3\n4 ERP012803  -   https://www.ebi.ac.uk/metagenomics/projects/ERP012803/download/2.0/export?contentType=text exportValue=taxonomy_abundances \nERP012803.meta  -   https://www.ebi.ac.uk/metagenomics/projects/ERP012803/overview/doExport ; \ngutproject  -  tbl_df ( read.delim ( file = ERP012803 )) \nmeta  -   read.csv ( file = ERP012803.meta )", 
            "title": "Load EMG input data for analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#preprocess-the-data", 
            "text": "Filter to only retain full taxonomic lineages with a species name   1\n2\n3\n4\n5\n6\n7 gutproject  -  gutproject  % %  \n              rename ( taxonomy = X.SampleID )   % % \n                 #mutate(taxonomy=gsub( .*?s__ , ,taxonomy)) % %  \n                mutate ( taxonomy = gsub ( .*?g__ , , taxonomy ))   % % \n                mutate ( taxonomy = gsub ( ;s__ , _ ,  taxonomy ))   % % \n                  filter ( taxonomy  !=   |  taxonomy == _   )   % % \n                    filter ( ! grepl ( _$ ,  taxonomy ))   \n - Merge counts of species in the same experimental sample  1\n2\n3\n4 gutproject  -  gutproject  % %  \n                group_by ( taxonomy )   % % \n                  summarize_each ( funs ( sum ))   % % \n                    ungroup ()   \n - Use the species name as the rowname, this replaces the default numerical name    Note   You can ignore warning: Setting row names on a tibble is deprecated.   Transform the wide format data into the long format for the purpose of appending descriptive information   1\n2\n3 gutproject.long   -  gutproject  % %  \n                      gather ( sampleb ,  freq ,  ERR1072624 : ERR1160857 )   % % \n                        filter ( taxonomy  != Root )   \n - Retain only the sample description and the Run id from the metadata  1 meta  -  meta  % %  select ( Sample.Description ,  Run.ID )   \n - What is the composition of the data.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 meta.strip  -   tbl_df ( meta )   % %  mutate ( Sample.Description = gsub (   , _ , Sample.Description ), \n                         Sample.Description = gsub ( American_Gut_Project_ , ,  Sample.Description ),  \n                         Sample.Description = gsub ( American_ , ,  Sample.Description ) \n                          ) \nmeta.strip  -  meta.strip  % %  group_by ( Sample.Description )   % % \n              mutate ( count = n ())   % %  ungroup ()   % % \n              select ( Sample.Description ,  count )    % %   unique () \n\nmeta.strip  % %  ggplot ( aes ( x = Sample.Description ,  y = count ))   +   geom_bar ( stat = identity )   +  \n                geom_text ( aes ( label = count ))    +  theme_bw ()   + \n                      theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))   +  \n                      ggtitle ( American gut project )     Question   What is the data mostly composed of?   Append metadata to the gut data   1\n2 gutproject.df  -   merge ( gutproject.long ,  meta ,  by.x = sampleb ,  by.y = Run.ID ) \ngutproject.df  -  tbl_df ( gutproject.df )   \n - Clean up the description of sample  1\n2\n3\n4\n5 gutproject.df  -  gutproject.df  % %  \n                  mutate ( Sample.Description = gsub (   , _ , Sample.Description ),  \n                   Sample.Description = gsub ( American_Gut_Project_ , ,  Sample.Description ), \n                   Sample.Description = gsub ( American_ , ,  Sample.Description ))   % % \n                    ungroup ()    \n - Merge taxonomy occurences count per data source  1\n2\n3\n4\n5\n6\n7\n8 gutproject.df  -  gutproject.df  % % \n                  group_by ( Sample.Description , taxonomy )   % %  \n                    summarise ( cum = sum ( freq ))   % %  ungroup () \ngutproject.df  -  gutproject.df  % % \n                  select ( taxonomy , Sample.Description , cum ) \n                   #filter(Sample.Description != Stool_sample ) % % \n                   #filter(Sample.Description != Stool_sample    Sample.Description != Mouth_sample  ) \n                   #filter( Sample.Description     != Vaginal_mucus_sample  )   \n - Transform the long format to wide format for the purpose of Mutivariate analysis  1\n2\n3 gutproject.df  -  gutproject.df  % % \n                    spread ( Sample.Description ,  cum ,  fill = 0 )   % % \n                       as.data.frame ()    1\n2 gutproject.mat  -  gutproject.df  % %  select ( - c ( 1 ))   % %   as.matrix ()  rownames ( gutproject.mat )   -   gsub ( \\\\[|\\\\] , ,  perl = TRUE ,  gutproject.df $ taxonomy )   \n - Clean up the matrix   Question   Is there any association between the species and the data source?   Perform a chi-square test of independence, this evaluates whether there is a significant dependence between species and biological samples   1\n2 chisq  -  chisq.test ( gutproject.mat ) \nchisq  \nThe results should look like this. 1\n2\n3 Pearson s Chi-squared test\ndata:  gutproject.mat\nX-squared = 106730000, df = 8442, p-value   2.2e-16  \nThe species and biological samples are statistically significantly associated (p-value=0)", 
            "title": "Preprocess the data"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#perform-correspondence-analysis-ca", 
            "text": "Correspondence Analysis (CA) is a generalized form of Principal Component Analysis for categorical data. We use CA to reduce the dimension of the data without loosing the most important information. CA is used to graphically visualize rows points and column points in a low dimensional space  1 gutproject.ca  -  CA ( gutproject.mat ,  graph = FALSE )     Explore the content of the CA output   1 summary ( gutproject.ca ,  nb.dec  =   2 ,  nbelements  =   4 ,  ncp =   3 )    The result of the function summary() contains the chi-square statistics and 3 tables:   Table 1- Eigenvalues, displays the variances and the percentage of vaiances retained by each dimension.  Table 2, Contains the coordinates, the contribution and the cos2 (quality of representation [0-1] of the first 4 active rows variable on the dimension 1, 2 and 3  Table 3: displays the coordinates, the contribution and the cos2 of the first 4 active column variables on the dimension 1, 2 and 3    Question   Can you interprete the CA result?   Hint   correlation coef., chi-square statistic", 
            "title": "Perform Correspondence Analysis (CA)"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#correlation-coefficient", 
            "text": "Eigen values is the amount of information retained by each axis.  1\n2\n3\n4 eig  -  get_eigenvalue ( gutproject.ca ) \ntrace  -   sum ( eig $ eigenvalue ) \ncor.coef  -   sqrt ( trace ) \ncor.coef   1 [1] 1.378614   As a rule of thumb a correlation above 0.2 can be considered important (Bendixen 1995, 567; Healey 2013, 289-290)\nThe correlation coef. is 1.38 in the American gut project dataset, indicating a strong association between row (species) and column(biological samples variables.). \nA more rigorous approach for examining the association is to use Chi-square statistics. \nThe association is highly significant (p-value=0)   Explore the percentages of inertia explained by the CA dimensions, the scree plot  1 fviz_screeplot ( gutproject.ca ,  barfill = white ,  addlabels = TRUE )   +  theme_bw ()   \nThe point at which the scree plot shows a bend mignt indicate an optimal dimensionality.", 
            "title": "Correlation coefficient"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part1/#hierarchical-clustering-of-principal-components", 
            "text": "Compute hierarchical clustering of species of CA results  1 gutproject.hcpc  -  HCPC ( gutproject.ca ,  graph  =   TRUE   ,  nb.clust = 4 ,  order = TRUE   )   \n - Visualize the CA results  Symmetric biplot, whereby both rows (blue) and columns (red) are represented in the same space using the principal coordinates. Only the distance between row points or the distance between column points can be reliably interpreted. \nWith a symmetric plot, only general statements can be drawn about the pattern. \nThe inter-distance between rows and column can not be interpreted.  1 fviz_ca_biplot ( gutproject.ca ,  repel = TRUE ,  select.row  =   list ( contrib  =   95 ),  select.col  =   list ( contrib  =   10 ),  geom = text )   +  theme_minimal ()   \n - Remove labels and add cluster centers  A 3D map of the hierarchical clustering of the principal component.  1\n2\n3 plot ( gutproject.hcpc ,  choice  = 3D.map ,  draw.tree  =   FALSE , \n     ind.names  =   FALSE ,  centers.plot  =   TRUE ,  \n     title = Hierarchical clustering of the Principal Components )   \n * Hierarchical Clustering of principal component.  Compute hierarchical clustering of biological samples of CA results  1 res.hcpc  -  HCPC ( gutproject.ca ,  cluster.CA  =   columns ,  graph  =   TRUE   ,  nb.clust = 4 ,  order = TRUE   )   \nBiological sample cluster in the dataset\n using fviz_cluster and theme_bw()  Transpose the row and column in the gut data and perform a new correspondance analysis  1 gutproject.ca  -  CA ( t ( gutproject.mat ),  ncp = 4 )   \nSymmetric biplot   1\n2\n3 fviz_ca_biplot ( gutproject.ca ,  repel = TRUE ,  select.row  =   list ( contrib  =   12 ),  \n               select.col  =   list ( contrib  =   95 ),  geom = text )   + \n  theme_minimal ()   \n - Remove labels and add cluster centers  3D hierarchical clustering of the principal components.  1\n2 plot ( res.hcpc ,  choice  = 3D.map ,  draw.tree  =   FALSE , \n     ind.names  =   FALSE ,  centers.plot  =   TRUE ,  title = Hierarchical clustering of the Principal Components )", 
            "title": "Hierarchical Clustering of principal components"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/", 
            "text": "Part 2: Functional analysis of EMG genomic and transcriptomic data using R\n\n\n\n\nGut microbiota disturbance during antibiotic therapy: a multi-omic approach\n\n\nIntroduction\n\n\nAntibiotic usage strongly affects microbial intestinal metabolism and thereby impacts human health. Understanding this process and the underlying mechanisms remains a major research goal. Accordingly, we conducted the first comparative omic investigation of gut microbial communities in faecal samples taken at multiple time points from an individual subjected to Beta-lactam therapy.\n\nPerez-Cobas AE, et al. 2013 Gut 62(11), 1591-1601\n\n\nWe will explore the data generated by the study above. The data is available at:\n\nhttps://www.ebi.ac.uk/metagenomics/projects/ERP001506\n\n\nGather the necessary analysis results file from EMG portal\n\n\nRead in the different contigency tables.\nR provides a very versatile data structure that allows us to store various data types, namely the list.\n\n\n1\nanalysis_files \n-\n \nc\n(\nBP_GO_abundances\n,\nBP_GO-slim_abundances\n,\nCC_GO_abundances\n,\nCC_GO-slim_abundances\n,\nGO_abundances\n,\nGO-slim_abundances\n,\nIPR_abundances\n,\nMF_GO_abundances\n,\nMF_GO-slim_abundances\n,\nphylum_taxonomy_abundances\n,\ntaxonomy_abundances\n)\n\n\n\n\n\n\n\nRead in all the contingency tables in a list making use of the lapply function\n\n\n\n\nQuestion\n\n\n\n\nHow can we access elements in the list?\n\n\n1\nattributes\n(\nresults\n)\n\n\n\n\n\nThis approach enables us to assign a name to each entry in the list for unambiguous access to different contingency tables\n\n1\n2\n3\ncolheader \n-\n \ngsub\n(\nanalysis_files\n,\n pattern\n=\n-\n,\n replacement \n=\n_\n)\n\n\nnames\n(\nresults\n)\n \n-\n colheader\n\nattributes\n(\nresults\n)\n\n\n\n\n\nLet\ns have a look at results object\n\n\n1\nstr\n(\nresults\n,\n list.len\n=\n4\n)\n\n\n\n\n\n\n\nEach entry in the list can now be accessed by name. Give each row an explicit name before manipulating the table contents with dplyr functions. The main advantage of tbl_df (return a dataframe from a dataframe) is that it only displays a few rows and all the columns fit on the current screen, with the rest of the data displayed as text. We will be making extensive use of the dplyr package, which allows operations to be chained (\n%\n%\n); it is analogous to the linux pipe command (|), see here for more details: \nhttps://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\n  \n\n\n1. Taxonomy: who is there and in what proportion?\n\n\nWe can optimise the display of the first few columns or so\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\ntaxonomy \n-\n tbl_df\n(\nresults\n$\nphylum_taxonomy_abundances\n)\n\ntaxonomy \n-\n taxonomy \n%\n%\n mutate\n(\nphylum\n=\nas.character\n(\nphylum\n))\n\ntax_g \n-\n taxonomy \n%\n%\n\nselect\n(\nmatches\n(\n_G|um\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetagenomics\n)\n\ntax_t \n-\n taxonomy \n%\n%\n\nselect\n(\nmatches\n(\n_T|um\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetatranscriptomics\n)\n\ntax_a \n-\n taxonomy \n%\n%\n\nselect\n(\nmatches\n(\n_A|um\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nAmplicon\n)\n\n\nrenameSample \n-\n \nfunction\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nlocaldf\n){\n\nstrippedcol \n-\n \ncolnames\n(\nlocaldf\n)\n\n\nfor\n \n(\ni \nin\n \n1\n:\nnrow\n(\nmetadata\n)){\n\nstrippedcol\n[\nwhich\n(\nstrippedcol\n==\nas.character\n(\nmetadata\n[\ni\n,\n2\n]))]\n \n-\n \nas.character\n(\nmetadata\n[\ni\n,\n1\n])\n\n\n}\n\n\nreturn\n(\nstrippedcol\n)\n\n\n}\n\n\n\ncolnames\n(\ntax_g\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ntax_g\n)\n\n\ncolnames\n(\ntax_t\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ntax_t\n)\n\n\ncolnames\n(\ntax_a\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ntax_a\n)\n\n\n\n\n\nThen transform a wide contingency table into a long one for the purposes of plotting.\n\n\n1\n2\n3\n4\n5\n6\ntax_metag \n-\n tax_g \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\ntax_g\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\ntax_metat \n-\n tax_t \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\ntax_t\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\nWe can merge both metatranscriptomic and metagenomic relative occurrences of operational taxonomic units (OTU) per condition to visualise the community dynamic during treatment.\n\n\n\n\nQuestion\n\n\n\n\nWhat conclusion would you draw from this distribution?\n\n\nThe heatmap is of absolute Species counts per datatypes\n\n\n1\n2\n3\n4\ngenomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\nphylum\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\ncount\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\n\n\n2. Function: who does what, in what relative proportion?\n\n\nRecall the various functional annotations in the created list.\n\n\n1\nattributes\n(\nresults\n)\n\n\n\n\n\n\n\nAs an initial exploration, we will focus on a broad GO annotation (GO-slim)\n\n\n** (A) GO Biological processes**: relative occurrence during treatment\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nbp_slim \n-\n tbl_df\n(\nresults\n$\nBP_GO_slim_abundances\n)\n\nbp_slim \n-\n bp_slim \n%\n%\n mutate\n(\ndescription\n=\nas.character\n(\ndescription\n))\n\nbp_slim_g \n-\n bp_slim \n%\n%\n\nselect\n(\nmatches\n(\n_G|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetagenomics\n)\n\nbp_slim_t \n-\n bp_slim \n%\n%\n\nselect\n(\nmatches\n(\n_T|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetatranscriptomics\n)\n\nbp_slim_a \n-\n bp_slim \n%\n%\n\nselect\n(\nmatches\n(\n_A|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nAmplicon\n)\n\n\n\n\n\n\n\nGive meaningful names to conditions, as for the taxonomic case above.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\ncolnames\n(\nbp_slim_g\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nbp_slim_g\n)\n\n\ncolnames\n(\nbp_slim_t\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nbp_slim_t\n)\n\n\ncolnames\n(\nbp_slim_a\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nbp_slim_a\n)\n\nbp_slim_metagenomics \n-\n bp_slim_g \n%\n%\n\ngather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nbp_slim_g\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\nbp_slim_metatransciptomics \n-\n bp_slim_t \n%\n%\n\ngather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nbp_slim_t\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\n\n\nMerge both metatranscriptomic and metagenomic relative occurences of biological process terms per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.\n\n\n1\nbp_genomic_transcriptomics \n-\n \nrbind\n(\nbp_slim_metagenomics\n,\n bp_slim_metatransciptomics\n)\n\n\n\n\n\nBar plot of relative abundance of Biological Processes\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this plot?\n\n\nHeat map of absolute biological process counts per datatype\n\n\n1\n2\n3\n4\nbp_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\ncount\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\n\nHeat map of biological process proportions per datatype\n\n\n1\n2\n3\n4\nbp_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\nprop\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\n\n(B) GO Cellular Compartment\n: relative occurence during treatment\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\ncc_slim \n-\n tbl_df\n(\nresults\n$\nCC_GO_slim_abundances\n)\n\ncc_slim \n-\n cc_slim \n%\n%\n mutate\n(\ndescription\n=\nas.character\n(\ndescription\n))\n\ncc_slim_g \n-\n cc_slim \n%\n%\n\nselect\n(\nmatches\n(\n_G|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetagenomics\n)\n\ncc_slim_t \n-\n cc_slim \n%\n%\n\nselect\n(\nmatches\n(\n_T|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetatranscriptomics\n)\n\ncc_slim_a \n-\n cc_slim \n%\n%\n\nselect\n(\nmatches\n(\n_A|description\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nAmplicon\n)\n\n\n\n\n\nGive meaningful names to conditions as for the taxonomic case above\n\n\n1\n2\n3\ncolnames\n(\ncc_slim_g\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ncc_slim_g\n)\n\n\ncolnames\n(\ncc_slim_t\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ncc_slim_t\n)\n\n\ncolnames\n(\ncc_slim_a\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\ncc_slim_a\n)\n\n\n\n\n\nCalculate the relative occurrence of cellular compartments per day of treatment for both metagenomic and transcriptomic data types.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\ncc_slim_metagenomics \n-\n cc_slim_g \n%\n%\n\ngather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\ncc_slim_g\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\ncc_slim_metatransciptomics \n-\n cc_slim_t \n%\n%\n\ngather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\ncc_slim_t\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\nMerge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.\n\n\n1\ncc_genomic_transcriptomics \n-\n \nrbind\n(\ncc_slim_metagenomics\n,\n cc_slim_metatransciptomics\n)\n\n\n\n\n\n\n\nBar plot of relative abundances of cellular compartment\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this plot?\n\n\nHeat map of absolute cellular compartment term counts per datatype\n\n\n1\n2\n3\n4\ncc_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\ncount\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\n\n\nHeat map of Cellular compartment term proportions per datatype\n\n\n1\n2\n3\n4\ncc_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\nprop\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\n\n\n(C) GO Molecular function\n: relative occurence during treatment\n\n\nExtract the data from the data structure \nresults\n created earlier.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nmf_slim \n-\n tbl_df\n(\nresults\n$\nMF_GO_slim_abundances\n)\n\nmf_slim \n-\n mf_slim \n%\n%\n mutate\n(\ndescription\n=\nas.character\n(\ndescription\n))\n\nmf_slim_g \n-\n mf_slim \n%\n%\n \n            select\n(\nmatches\n(\n_G|description\n))\n \n%\n%\n \n            mutate\n(\nDataType\n=\nMetagenomics\n)\n\nmf_slim_t \n-\n mf_slim \n%\n%\n \n            select\n(\nmatches\n(\n_T|description\n))\n \n%\n%\n \n            mutate\n(\nDataType\n=\nMetatranscriptomics\n)\n \nmf_slim_a \n-\n mf_slim \n%\n%\n \n            select\n(\nmatches\n(\n_A|description\n))\n  \n%\n%\n \n            mutate\n(\nDataType\n=\nAmplicon\n)\n\n\n\n\n\n\n\nGive meaningful names to conditions as for the taxonomic case above\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ncolnames\n(\nmf_slim_g\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nmf_slim_g\n)\n\n\ncolnames\n(\nmf_slim_t\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nmf_slim_t\n)\n\n\ncolnames\n(\nmf_slim_a\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nmf_slim_a\n)\n\n\nmf_slim_metagenomics \n-\n mf_slim_g \n%\n%\n \n                        gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nmf_slim_g\n)\n-1\n)\n \n%\n%\n\n                        group_by\n(\n condition\n)\n \n%\n%\n \n                        mutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\nmf_slim_metatransciptomics \n-\n mf_slim_t \n%\n%\n \n                              gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nmf_slim_t\n)\n-1\n)\n \n%\n%\n\n                              group_by\n(\n condition\n)\n \n%\n%\n\n                              mutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\n\n\nMerge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundances of biological process terms that are dynamic during treatment.  \n\n\n1\nmf_genomic_transcriptomics \n-\n \nrbind\n(\nmf_slim_metagenomics\n,\n mf_slim_metatransciptomics\n)\n\n\n\n\n\n\n\nBarplot of relative abundance Molecular function\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nmf_genomic_transcriptomics \n%\n%\n arrange\n(\ndesc\n(\nprop\n))\n \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\n y\n=\nprop\n,\nfill\n=\ndescription\n))\n \n+\n\ngeom_bar\n(\nstat\n=\nidentity\n,\n position\n=\nfill\n,\n aes\n(\nfill \n=\n description\n))\n \n+\n\nfacet_wrap\n(\n~\nDataType\n,\n ncol\n=\n1\n)\n \n+\n\nggtitle\n(\nMolecular function relative abundance through\n\n\n\\n treatment per datatype\n)\n \n+\n\nylab\n(\nproportion abundance\n)\n \n+\n\ntheme_light\n()\n \n+\n\ntheme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n \n+\n\ncoord_flip\n()\n \n+\n\ntheme\n(\nlegend.position\n=\nnone\n)\n# + guides(fill=guide_legend(ncol=2))\n\n\n\n\n\nHeatmap of Absolute Molecular Function term counts through treatment\n\n\n1\n2\n3\n4\nmf_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n \n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\ncount\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n\n\n\n\n\nHeat map of relative occurrence of molecular function term counts throughout treatment\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this plot?", 
            "title": "Mining EMG with R part 2A"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#part-2-functional-analysis-of-emg-genomic-and-transcriptomic-data-using-r", 
            "text": "", 
            "title": "Part 2: Functional analysis of EMG genomic and transcriptomic data using R"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#gut-microbiota-disturbance-during-antibiotic-therapy-a-multi-omic-approach", 
            "text": "", 
            "title": "Gut microbiota disturbance during antibiotic therapy: a multi-omic approach"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#introduction", 
            "text": "Antibiotic usage strongly affects microbial intestinal metabolism and thereby impacts human health. Understanding this process and the underlying mechanisms remains a major research goal. Accordingly, we conducted the first comparative omic investigation of gut microbial communities in faecal samples taken at multiple time points from an individual subjected to Beta-lactam therapy. Perez-Cobas AE, et al. 2013 Gut 62(11), 1591-1601  We will explore the data generated by the study above. The data is available at: https://www.ebi.ac.uk/metagenomics/projects/ERP001506", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#gather-the-necessary-analysis-results-file-from-emg-portal", 
            "text": "Read in the different contigency tables.\nR provides a very versatile data structure that allows us to store various data types, namely the list.  1 analysis_files  -   c ( BP_GO_abundances , BP_GO-slim_abundances , CC_GO_abundances , CC_GO-slim_abundances , GO_abundances , GO-slim_abundances , IPR_abundances , MF_GO_abundances , MF_GO-slim_abundances , phylum_taxonomy_abundances , taxonomy_abundances )    Read in all the contingency tables in a list making use of the lapply function   Question   How can we access elements in the list?  1 attributes ( results )   \nThis approach enables us to assign a name to each entry in the list for unambiguous access to different contingency tables 1\n2\n3 colheader  -   gsub ( analysis_files ,  pattern = - ,  replacement  = _ )  names ( results )   -  colheader attributes ( results )   \nLet s have a look at results object  1 str ( results ,  list.len = 4 )    Each entry in the list can now be accessed by name. Give each row an explicit name before manipulating the table contents with dplyr functions. The main advantage of tbl_df (return a dataframe from a dataframe) is that it only displays a few rows and all the columns fit on the current screen, with the rest of the data displayed as text. We will be making extensive use of the dplyr package, which allows operations to be chained ( % % ); it is analogous to the linux pipe command (|), see here for more details:  https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html", 
            "title": "Gather the necessary analysis results file from EMG portal"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#1-taxonomy-who-is-there-and-in-what-proportion", 
            "text": "We can optimise the display of the first few columns or so   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 taxonomy  -  tbl_df ( results $ phylum_taxonomy_abundances ) \ntaxonomy  -  taxonomy  % %  mutate ( phylum = as.character ( phylum )) \ntax_g  -  taxonomy  % % \nselect ( matches ( _G|um ))   % % \nmutate ( DataType = Metagenomics ) \ntax_t  -  taxonomy  % % \nselect ( matches ( _T|um ))   % % \nmutate ( DataType = Metatranscriptomics ) \ntax_a  -  taxonomy  % % \nselect ( matches ( _A|um ))   % % \nmutate ( DataType = Amplicon ) \n\nrenameSample  -   function ( metadata = metadata ,  localdf = localdf ){ \nstrippedcol  -   colnames ( localdf )  for   ( i  in   1 : nrow ( metadata )){ \nstrippedcol [ which ( strippedcol == as.character ( metadata [ i , 2 ]))]   -   as.character ( metadata [ i , 1 ])  }  return ( strippedcol )  }  colnames ( tax_g )   -  renameSample ( metadata = metadata ,  localdf = tax_g )  colnames ( tax_t )   -  renameSample ( metadata = metadata ,  localdf = tax_t )  colnames ( tax_a )   -  renameSample ( metadata = metadata ,  localdf = tax_a )   \nThen transform a wide contingency table into a long one for the purposes of plotting.  1\n2\n3\n4\n5\n6 tax_metag  -  tax_g  % %  gather ( condition ,  count ,   3 : ncol ( tax_g ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count )) \ntax_metat  -  tax_t  % %  gather ( condition ,  count ,   3 : ncol ( tax_t ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count ))   \nWe can merge both metatranscriptomic and metagenomic relative occurrences of operational taxonomic units (OTU) per condition to visualise the community dynamic during treatment.   Question   What conclusion would you draw from this distribution?  The heatmap is of absolute Species counts per datatypes  1\n2\n3\n4 genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = phylum )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = count ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))", 
            "title": "1. Taxonomy: who is there and in what proportion?"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2A/#2-function-who-does-what-in-what-relative-proportion", 
            "text": "Recall the various functional annotations in the created list.  1 attributes ( results )    As an initial exploration, we will focus on a broad GO annotation (GO-slim)  ** (A) GO Biological processes**: relative occurrence during treatment   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 bp_slim  -  tbl_df ( results $ BP_GO_slim_abundances ) \nbp_slim  -  bp_slim  % %  mutate ( description = as.character ( description )) \nbp_slim_g  -  bp_slim  % % \nselect ( matches ( _G|description ))   % % \nmutate ( DataType = Metagenomics ) \nbp_slim_t  -  bp_slim  % % \nselect ( matches ( _T|description ))   % % \nmutate ( DataType = Metatranscriptomics ) \nbp_slim_a  -  bp_slim  % % \nselect ( matches ( _A|description ))   % % \nmutate ( DataType = Amplicon )    Give meaningful names to conditions, as for the taxonomic case above.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 colnames ( bp_slim_g )   -  renameSample ( metadata = metadata ,  localdf = bp_slim_g )  colnames ( bp_slim_t )   -  renameSample ( metadata = metadata ,  localdf = bp_slim_t )  colnames ( bp_slim_a )   -  renameSample ( metadata = metadata ,  localdf = bp_slim_a ) \nbp_slim_metagenomics  -  bp_slim_g  % % \ngather ( condition ,  count ,   3 : ncol ( bp_slim_g ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count )) \n\nbp_slim_metatransciptomics  -  bp_slim_t  % % \ngather ( condition ,  count ,   3 : ncol ( bp_slim_t ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count ))    Merge both metatranscriptomic and metagenomic relative occurences of biological process terms per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.  1 bp_genomic_transcriptomics  -   rbind ( bp_slim_metagenomics ,  bp_slim_metatransciptomics )   \nBar plot of relative abundance of Biological Processes   Question   What can you conclude from this plot?  Heat map of absolute biological process counts per datatype  1\n2\n3\n4 bp_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = count ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))    Heat map of biological process proportions per datatype  1\n2\n3\n4 bp_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = prop ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))    (B) GO Cellular Compartment : relative occurence during treatment  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 cc_slim  -  tbl_df ( results $ CC_GO_slim_abundances ) \ncc_slim  -  cc_slim  % %  mutate ( description = as.character ( description )) \ncc_slim_g  -  cc_slim  % % \nselect ( matches ( _G|description ))   % % \nmutate ( DataType = Metagenomics ) \ncc_slim_t  -  cc_slim  % % \nselect ( matches ( _T|description ))   % % \nmutate ( DataType = Metatranscriptomics ) \ncc_slim_a  -  cc_slim  % % \nselect ( matches ( _A|description ))   % % \nmutate ( DataType = Amplicon )   \nGive meaningful names to conditions as for the taxonomic case above  1\n2\n3 colnames ( cc_slim_g )   -  renameSample ( metadata = metadata ,  localdf = cc_slim_g )  colnames ( cc_slim_t )   -  renameSample ( metadata = metadata ,  localdf = cc_slim_t )  colnames ( cc_slim_a )   -  renameSample ( metadata = metadata ,  localdf = cc_slim_a )   \nCalculate the relative occurrence of cellular compartments per day of treatment for both metagenomic and transcriptomic data types. 1\n2\n3\n4\n5\n6\n7\n8\n9 cc_slim_metagenomics  -  cc_slim_g  % % \ngather ( condition ,  count ,   3 : ncol ( cc_slim_g ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count )) \n\ncc_slim_metatransciptomics  -  cc_slim_t  % % \ngather ( condition ,  count ,   3 : ncol ( cc_slim_t ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count ))   \nMerge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.  1 cc_genomic_transcriptomics  -   rbind ( cc_slim_metagenomics ,  cc_slim_metatransciptomics )    Bar plot of relative abundances of cellular compartment   Question   What can you conclude from this plot?  Heat map of absolute cellular compartment term counts per datatype  1\n2\n3\n4 cc_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = count ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))    Heat map of Cellular compartment term proportions per datatype  1\n2\n3\n4 cc_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = prop ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))    (C) GO Molecular function : relative occurence during treatment  Extract the data from the data structure  results  created earlier.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 mf_slim  -  tbl_df ( results $ MF_GO_slim_abundances ) \nmf_slim  -  mf_slim  % %  mutate ( description = as.character ( description )) \nmf_slim_g  -  mf_slim  % %  \n            select ( matches ( _G|description ))   % %  \n            mutate ( DataType = Metagenomics ) \nmf_slim_t  -  mf_slim  % %  \n            select ( matches ( _T|description ))   % %  \n            mutate ( DataType = Metatranscriptomics )  \nmf_slim_a  -  mf_slim  % %  \n            select ( matches ( _A|description ))    % %  \n            mutate ( DataType = Amplicon )    Give meaningful names to conditions as for the taxonomic case above   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 colnames ( mf_slim_g )   -  renameSample ( metadata = metadata ,  localdf = mf_slim_g )  colnames ( mf_slim_t )   -  renameSample ( metadata = metadata ,  localdf = mf_slim_t )  colnames ( mf_slim_a )   -  renameSample ( metadata = metadata ,  localdf = mf_slim_a ) \n\nmf_slim_metagenomics  -  mf_slim_g  % %  \n                        gather ( condition ,  count ,   3 : ncol ( mf_slim_g ) -1 )   % % \n                        group_by (  condition )   % %  \n                        mutate ( prop = count / sum ( count )) \n\nmf_slim_metatransciptomics  -  mf_slim_t  % %  \n                              gather ( condition ,  count ,   3 : ncol ( mf_slim_t ) -1 )   % % \n                              group_by (  condition )   % % \n                              mutate ( prop = count / sum ( count ))    Merge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundances of biological process terms that are dynamic during treatment.    1 mf_genomic_transcriptomics  -   rbind ( mf_slim_metagenomics ,  mf_slim_metatransciptomics )    Barplot of relative abundance Molecular function   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 mf_genomic_transcriptomics  % %  arrange ( desc ( prop ))   % % \nggplot (  aes ( x = condition ,  y = prop , fill = description ))   + \ngeom_bar ( stat = identity ,  position = fill ,  aes ( fill  =  description ))   + \nfacet_wrap ( ~ DataType ,  ncol = 1 )   + \nggtitle ( Molecular function relative abundance through  \\n treatment per datatype )   + \nylab ( proportion abundance )   + \ntheme_light ()   + \ntheme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))   + \ncoord_flip ()   + \ntheme ( legend.position = none ) # + guides(fill=guide_legend(ncol=2))   \nHeatmap of Absolute Molecular Function term counts through treatment  1\n2\n3\n4 mf_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description ))   +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = count ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))   \nHeat map of relative occurrence of molecular function term counts throughout treatment   Question   What can you conclude from this plot?", 
            "title": "2. Function: who does what, in what relative proportion?"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/", 
            "text": "3. InterproScan: relative occurence during treatment.\n\n\nExtract the data from the data structure \nresults\n created earlier and partition the extracted data per datatype.\n\n\n1\n2\n3\n4\n5\nipr \n-\n tbl_df\n(\nresults\n$\nIPR_abundances\n)\n\nipr\n-\n separateDataType\n(\nmydf\n=\nipr\n,\n metadata\n=\nmetadata\n)\n\nipr_g \n-\n ipr\n$\ngenome\nipr_t \n-\n ipr\n$\ntranscript\nipr_a \n-\n ipr\n$\namplicon\n\n\n\n\n\n\nCalculate the relative occurrence of IPR enrichment per day of treatment for both metagenomic and transcriptomic data types.\n\n\n1\n2\n3\n4\n5\n6\nipr_metagenomics \n-\n ipr_g \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nipr_g\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\nipr_metatransciptomics \n-\n ipr_t \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n3\n:\nncol\n(\nipr_t\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\nMerge both metatranscriptomic and metagenomic relative occurrences of IPR per condition to visualise the relative abundance of IPR terms that are dynamic during treatment.\n\n1\n2\n3\nipr_genomic_transcriptomics \n-\n \nrbind\n(\nipr_metagenomics\n,\n ipr_metatransciptomics\n)\n\nipr_genomic_transcriptomics \n-\n ipr_genomic_transcriptomics \n%\n%\n\nmutate\n(\ndescription\n=\nas.character\n(\ndescription\n),\n count\n=\nlog2\n(\ncount\n+1\n))\n\n\n\n\n\n\nBar plot of relative abundance of IPR\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nipr_genomic_transcriptomics \n%\n%\n arrange\n(\ndesc\n(\nprop\n))\n \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\n y\n=\nprop\n,\nfill\n=\ndescription\n))\n \n+\n\ngeom_bar\n(\nstat\n=\nidentity\n,\n position\n=\nfill\n,\n aes\n(\nfill \n=\n description\n))\n \n+\n\nfacet_wrap\n(\n~\nDataType\n,\n ncol\n=\n1\n)\n \n+\n\nggtitle\n(\nIPR relative abundance through treatment\\n per datatype\n)\n \n+\n\nylab\n(\nproportion abundance\n)\n \n+\n\ntheme_light\n()\n \n+\n\ntheme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n),\n\naxis.text.y\n=\nelement_blank\n())\n \n+\n\ncoord_flip\n()\n \n+\n\ntheme\n(\nlegend.position\n=\nnone\n)\n\n\n\n\n\n\n\nHeat map of absolute InterproScan hit counts throughout treatment\n\n\n1\n2\n3\n4\n5\nipr_genomic_transcriptomics \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\ny\n=\ndescription\n))\n+\n facet_grid\n(\n~\nDataType\n)\n \n+\n\ngeom_tile\n(\naes\n(\nfill\n=\ncount\n))\n \n+\n scale_fill_gradient\n(\nlow\n=\nwhite\n,\n high\n=\ndarkblue\n)\n \n+\n\nxlab\n(\n)\n \n+\n ylab\n(\n)\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n),\n\naxis.text.y\n=\nelement_blank\n())\n\n\n\n\n\n\n\n4. GO (all category): relative occurence during treatment.\n\n\nThe goal here is to create a composite plot of all GO category occurence during treatments. Give meaningful names to conditions as for the taxonomic case\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nGO \n-\n tbl_df\n(\nresults\n$\nGO_abundances\n)\n\nGO \n-\n GO \n%\n%\n mutate\n(\ndescription\n=\nas.character\n(\ndescription\n),\n \ncategory\n=\nas.character\n(\ncategory\n))\n\n\nGO_g \n-\n GO \n%\n%\n\nselect\n(\nmatches\n(\n_G|description|category\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetagenomics\n)\n\nGO_t \n-\n GO \n%\n%\n select\n(\nmatches\n(\n_T|description|category\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nMetatranscriptomics\n)\n\nGO_a \n-\n GO \n%\n%\n select\n(\nmatches\n(\n_A|description|category\n))\n \n%\n%\n\nmutate\n(\nDataType\n=\nAmplicon\n)\n\n\n\n\ncolnames\n(\nGO_g\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nGO_g\n)\n\n\ncolnames\n(\nGO_t\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nGO_t\n)\n\n\ncolnames\n(\nGO_a\n)\n \n-\n renameSample\n(\nmetadata\n=\nmetadata\n,\n localdf\n=\nGO_a\n)\n\n\n\n\n\n\n\nTransform the data from a wide format to a long format, making use of the tidyr package function gather, and compute the relative frequency of occurrence per days of treatment\n\n\n1\n2\n3\n4\n5\n6\nGO_metagenomics \n-\n GO_g \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n4\n:\nncol\n(\nGO_g\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n,\n \ncategory\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\nGO_metatransciptomics \n-\n GO_t \n%\n%\n gather\n(\ncondition\n,\n count\n,\n \n4\n:\nncol\n(\nGO_t\n)\n-1\n)\n \n%\n%\n\ngroup_by\n(\n condition\n,\n \ncategory\n)\n \n%\n%\n\nmutate\n(\nprop\n=\ncount\n/\nsum\n(\ncount\n))\n\n\n\n\n\n\n\nMerge both metatranscriptomic and metagenomic relative occurrences of GO per condition to visualise the relative abundance of all GO category terms that are dynamic during treatment.\n\n\n1\nGO_genomic_transcriptomics \n-\n \nrbind\n(\nGO_metagenomics\n,\n GO_metatransciptomics\n)\n\n\n\n\n\n\n\nBarplot of relative abundance of all GO terms\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nGO_genomic_transcriptomics \n%\n%\n arrange\n(\ndesc\n(\nprop\n))\n \n%\n%\n\nggplot\n(\n aes\n(\nx\n=\ncondition\n,\n y\n=\nprop\n,\nfill\n=\ndescription\n))\n \n+\n\ngeom_bar\n(\nstat\n=\nidentity\n,\n position\n=\nfill\n,\n aes\n(\nfill \n=\n description\n))\n \n+\n\nfacet_grid\n(\ncategory\n~\nDataType\n)\n \n+\n\nggtitle\n(\nGene Ontology relative abundance through days \\n of treatment per datatype\n)\n \n+\n\nylab\n(\nproportion abundance\n)\n \n+\n\ntheme_light\n()\n \n+\n theme\n(\naxis.text.x\n=\nelement_text\n(\nangle\n=\n45\n,\n hjust\n=\n1\n))\n \n+\n\ncoord_flip\n()\n \n+\n\ntheme\n(\nlegend.position\n=\nnone\n)\n \n#+ guides(fill=guide_legend(ncol=2))\n\n\n\n\n\n\n\n5.0 Correspondance Analysisis of the contingency tables.\n\n\nThe goal of this analysis is to elucidate the relationship between species and GO terms with respect to various treatment conditions (days).\nWe will make use of the variable created above.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nhead\n(\ngenomic_transcriptomics\n)\n\ntaxonomy_long \n-\n genomic_transcriptomics \n%\n%\n ungroup\n()\n \n%\n%\n\nmutate\n(\ncondition\n=\npaste\n(\nDataType\n,\n condition\n,\n sep\n=\n_\n))\n \n%\n%\n\nselect\n(\nphylum\n,\n condition\n,\n count\n)\n\n\nhead\n(\ntaxonomy_long\n)\n\ntaxonomy_wide \n-\n spread\n(\ntaxonomy_long\n,\n condition\n,\n count\n)\n\n\nrownames\n(\ntaxonomy_wide\n)\n \n-\n taxonomy_wide\n$\nPhylum\ntaxonomy_wide\ntaxonomy_wide.df \n-\n select\n(\ntaxonomy_wide\n,\n \n-\nphylum\n)\n \n%\n%\n\n\nas.data.frame\n()\n\n\n\n\n\n\n\nTranspose the matrix for the purpose of plotting\n\n\n1\ntaxonomy\n-\n \nt\n(\ntaxonomy_wide.df\n)\n\n\n\n\n\nDefine a main plot function to set up a plot for later use\n\n\n1\n2\n3\n4\nsetup.plot \n-\n ggplot\n()\n \n+\n coord_fixed\n()\n \n+\n\nlabs\n(\nx\n=\nComp1, Axis1\n,\n y\n=\nComp2, Axis2\n)\n \n+\n\ngeom_hline\n(\nyintercept\n=\n0\n,\n col\n=\ndarkgrey\n)\n \n+\n\ngeom_vline\n(\nxintercept\n=\n0\n,\n col\n=\ndarkgrey\n)\n\n\n\n\n\nMake the scree plot in a viewport\n\n\n1\n2\n3\n4\n5\n6\n7\nmyscree \n-\n \nfunction\n(\neigs\n,\n x\n=\n0.8\n,\n y\n=\n0.1\n,\n just\n=\nc\n(\nright\n,\nbottom\n)){\n\nvp \n-\n viewport\n(\nx\n=\nx\n,\n y\n=\ny\n,\n width\n=\n0.2\n,\n height\n=\n0.2\n,\n just\n=\njust\n)\n\nmm \n-\n \ndata.frame\n(\nx\n=\nfactor\n(\n1\n:\nlength\n(\neigs\n)),\n y\n=\neigs\n)\n\nsp \n-\n ggplot\n(\nmm\n,\n aes\n(\nx\n=\nx\n,\n y\n=\ny\n))\n \n+\n geom_bar\n(\nstat\n=\nidentity\n)\n \n+\n\nlabs\n(\nx \n=\n \nNULL\n,\n y \n=\n \nNULL\n)\n \n+\n theme_light\n()\n\n\nprint\n(\nsp\n,\n vp\n=\nvp\n)\n\n\n}\n\n\n\n\n\nExtract various datatype\n\n\n1\n2\n3\n4\ntaxonomy.dna \n-\n taxonomy_wide.df \n%\n%\n select\n(\nmatches\n(\nMetagenomics\n))\n\ntaxonomy.rna \n-\n taxonomy_wide.df \n%\n%\n select\n(\nmatches\n(\nMetatranscriptomics\n))\n\ntaxonomy.dna\n-\n \nt\n(\ntaxonomy.dna\n)\n\ntaxonomy.rna\n-\n \nt\n(\ntaxonomy.rna\n)\n\n\n\n\n\nRead in a file describing the data we loaded previously into R\n\n\n1\n2\n3\n4\ndata.dna \n-\n metadata\n[\nmetadata\n$\ndatatype\n==\nMetagenomic\n \n\n\n!\n(\nmetadata\n$\ndatatype\n==\nAmplicon_DNA\n|\n metadata\n$\ndatatype\n==\nAmplicon_RNA\n),]\n\ndata.rna \n-\n metadata\n[\nmetadata\n$\ndatatype\n==\nMetatranscriptomic\n \n\n\n!\n(\nmetadata\n$\ndatatype\n==\nAmplicon_DNA\n|\n metadata\n$\ndatatype\n==\nAmplicon_RNA\n),]\n\n\n\n\n\n\n\nPerform a Correspondance analysis on Metagenomic data\n\n\n1\ntaxonomy.coa \n-\n dudi.coa\n(\ntaxonomy.dna\n,\n scannf\n=\nF\n,\n nf\n=\n2\n)\n\n\n\n\n\n\n\nPlot the correspondence analysis with Ade4 native plotter\n\n\n1\nscatter\n(\ntaxonomy.coa\n)\n\n\n\n\n\n\n\nThe plot can be made more visually appealing and easier to interpret with ggplot2, making use of data computed for us by Ade4 (dudi.coa). These are in the object return by the dudi.coa function.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ntaxonomy.dna.plot\n-\n setup.plot \n+\n geom_point\n(\ndata\n=\ndata.frame\n(\ntaxonomy.coa\n$\nli\n,\n data.dna\n),\n\naes\n(\nx\n=\nAxis1\n,\n y\n=\nAxis2\n,\n col\n=\ndays\n,\n shape\n=\ndays\n,\n size\n=\n3\n))\n \n+\n\ngeom_text\n(\ndata\n=\ntaxonomy.coa\n$\nco\n,\n\naes\n(\nx\n=\nComp1\n,\n y\n=\nComp2\n,\n label\n=\nrownames\n(\ntaxonomy.coa\n$\nco\n)),\n\nposition \n=\n \njitter\n,\n alpha\n=\n0.2\n)\n \n+\n\nscale_size_area\n(\nbreaks\n=\nc\n(\n0\n,\n3\n,\n6\n,\n11\n,\n14\n,\n40\n))\n \n+\n\nscale_shape\n(\nsolid\n=\nF\n)\n \n+\n\nlabs\n(\ntitle\n=\nCorrespondence Analysis: Metagenomics\\nTaxonomy abundance\n)\n \n+\n theme_light\n()\n\ntaxonomy.dna.plot\nmyscree\n(\ntaxonomy.coa\n$\neig \n/\n \nsum\n(\ntaxonomy.coa\n$\neig\n))\n\n\n\n\n\n\n\n5.1 Perform a Correspondence analysis on the Metatranscriptomic data\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude despite the very sparse data?\n\n\nMerge taxonomic and GO_slim_abundances for correspondence analysis._\nOrganise the taxonomy data\n\n\n1\n2\n3\n4\n5\ntaxonomy \n-\n tbl_df\n(\nresults\n$\nphylum_taxonomy_abundances\n)\n \n%\n%\n select\n(\n-\nkingdom\n)\n\nphylum \n-\n taxonomy\n$\nphylum\ntaxonomy \n-\n taxonomy \n%\n%\n select\n(\n-\nphylum\n)\n \n%\n%\n \nt\n()\n\n\ncolnames\n(\ntaxonomy\n)\n \n-\n phylum\ntaxonomy \n-\n taxonomy\n[\n!\ngrepl\n(\n_A$\n,\nrownames\n(\ntaxonomy\n)),]\n\n\n\n\n\n\n\nOrganise the ontologies data\n\n\n1\nontology.meta \n-\n tbl_df\n(\nresults\n$\nGO_slim_abundances\n)\n \n%\n%\nselect\n(\nGO\n,\ndescription\n,\n \ncategory\n)\n\n\n\n\n\n\n\nOrganise the Biological processes data\n\n\n1\n2\n3\n4\ngo.bp \n-\n tbl_df\n(\nresults\n$\nBP_GO_slim_abundances\n)\n \n%\n%\nselect\n(\n-\nGO\n,\n \n-\ncategory\n)\n\ndescription.go \n-\n go.bp\n$\ndescription\ngo.bp \n-\n go.bp \n%\n%\n select\n(\n-\ndescription\n)\n \n%\n%\n \nt\n()\n\n\ncolnames\n(\ngo.bp\n)\n \n-\n description.go\n\n\n\n\n\n\nOrganise the Molecular function data\n\n1\n2\n3\n4\ngo.mf \n-\n tbl_df\n(\nresults\n$\nMF_GO_slim_abundances\n)\n \n%\n%\nselect\n(\n-\nGO\n,\n \n-\ncategory\n)\n\ndescription.go \n-\n go.mf\n$\ndescription\ngo.mf \n-\n go.mf \n%\n%\n select\n(\n-\ndescription\n)\n \n%\n%\n \nt\n()\n\n\ncolnames\n(\ngo.mf\n)\n \n-\n description.go\n\n\n\n\n\nOrganise the Cellular compartment data\n\n1\n2\n3\n4\ngo.cc \n-\n tbl_df\n(\nresults\n$\nCC_GO_slim_abundances\n)\n \n%\n%\nselect\n(\n-\nGO\n,\n \n-\ncategory\n)\n\ndescription.go \n-\n go.cc\n$\ndescription\ngo.cc \n-\n go.cc \n%\n%\n select\n(\n-\ndescription\n)\n \n%\n%\n \nt\n()\n\n\ncolnames\n(\ngo.cc\n)\n \n-\n description.go\n\n\n\n\n\nOrganise the IPR data\n\n\n1\n2\n3\n4\nipr \n-\n tbl_df\n(\nresults\n$\nIPR_abundances\n)\n \n%\n%\n select\n(\n-\nIPR\n)\n\ndescription.ipr \n-\n ipr\n$\ndescription\nipr \n-\n ipr \n%\n%\n select\n(\n-\ndescription\n)\n \n%\n%\n \nt\n()\n\n\ncolnames\n(\nipr\n)\n \n-\n description.ipr\n\n\n\n\n\n\nRead in the annotation material\n\n\n1\nmetainfo \n-\n metadata\n\n\n\n\n\n\nFilter out the amplicons data\n\n\n1\n2\nmetainfo \n-\n metainfo \n%\n%\n filter\n(\n!\ngrepl\n(\n_A$\n,\n id\n))\n\nmetainfo\n\n\n\n\n\n\nMerge the Taxonomy and Molecular Functions and perform Correspondence Analysis (CA) on the merge set\n\n\n1\n2\nmydf \n-\n \ncbind\n(\ntaxonomy\n,\n go.mf\n)\n\nmydf \n-\n mydf\n[\nrownames\n(\nmydf\n)\n \n%in%\n metainfo\n$\nid\n,]\n\n\n\n\n\n\n\nCall on dudi.coa function from Ade4 package to perform CA\n\n\n1\nmydf.coa \n-\n dudi.coa\n(\nmydf\n,\n scannf\n=\nF\n,\n nf\n=\n2\n)\n\n\n\n\n\n\n\nVisualize the CA analysis with ggplot2\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this analysis?\n\n\n5.2 Merge Taxonomy and Biological processess and perform Correspondence Analysis (CA) on the merge set\n\n\n1\n2\n3\nmydf \n-\n \ncbind\n(\ntaxonomy\n,\n go.bp\n)\n\nmydf \n-\n mydf\n[\nrownames\n(\nmydf\n)\n \n%in%\n metainfo\n$\nid\n,]\n\nmydf.coa \n-\n dudi.coa\n(\nmydf\n,\n scannf\n=\nF\n,\n nf\n=\n2\n)\n\n\n\n\n\nVisualize the CA analysis with ggplot2\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this analysis?\n\n\n5.3 \nMerge Taxonomy and Cellular Compartment and perform Correspondence Analysis (CA) on the merge set\n\n\n1\n2\n3\nmydf \n-\n \ncbind\n(\ntaxonomy\n,\n go.cc\n)\n\nmydf \n-\n mydf\n[\nrownames\n(\nmydf\n)\n \n%in%\n metainfo\n$\nid\n,]\n\nmydf.coa \n-\n dudi.coa\n(\nmydf\n,\n scannf\n=\nF\n,\n nf\n=\n2\n)\n\n\n\n\n\n\n\nVisualize the CA analysis with ggplot2\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this analysis?\n\n\n5.4 \nMerge Taxonomy, Cellular Compartments and Molecular functions then perform Correspondence Analysis (CA) on the merge set\n\n\nMerging Taxonomy and Cellular compartment and Molecular function\n\n\n1\n2\n3\nmydf \n-\n \ncbind\n(\ntaxonomy\n,\n go.cc\n,\n go.mf\n)\n\nmydf \n-\n mydf\n[\nrownames\n(\nmydf\n)\n \n%in%\n metainfo\n$\nid\n,]\n\nmydf.coa \n-\n dudi.coa\n(\nmydf\n,\n scannf\n=\nF\n,\n nf\n=\n2\n)\n\n\n\n\n\nVisualize the CA analysis with ggplot2\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this analysis?\n\n\n6.0 Clustering of the Principal Component\n\n\n1\n2\ncompositedata \n-\n \nas.data.frame\n(\nmydf\n)\n\n\nrownames\n(\ncompositedata\n)\n \n-\n \npaste\n(\nmetainfo\n$\ndays\n,\n metainfo\n$\ngenomictype\n,\n sep\n=\n_\n \n)\n\n\n\n\n\nRemove columns that only contain zero as they are not contributing to the creation of the component when using FactoMineR\n\n\n1\ncompositedata\n-\n compositedata\n[,\n \nwhich\n(\n!\napply\n(\ncompositedata\n,\n2\n,\nFUN\n=\nfunction\n(\nx\n){\nall\n(\nx\n==\n0\n)}))]\n\n\n\n\n\nPerform a correspondence analysis using FactoMineR. This automatically generates a perceptual map, showing the relationship between GO terms, species, and days of treatment and various datatypes.\n\n1\nca.analysis \n-\n CA\n(\ncompositedata\n)\n\n\n\n\n\nPerform a hierarchical clustering of the principal component created above to see the relationship between days of experiment and datatypes.\n\n1\nclustering\n-\n HCPC\n(\nca.analysis\n,\n nb.clust\n=\n-1\n,\n order\n=\nTRUE\n)\n\n\n\n\n\n\n1\nplot\n(\nclustering\n)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat do you conclude from this graph?\n\n\n6.1 To find out the relationship between annotations(BP,CC), species we transpose the above table prior to performing a CA.\n\n\n1\n2\nannoSpecies \n-\n \nas.data.frame\n(\nt\n(\ncompositedata\n))\n\n\nrownames\n(\nannoSpecies\n)\n \n-\n \nrownames\n(\nt\n(\ncompositedata\n))\n\n\n\n\n\n\n\nCA\n\n\n1\nanno.ca.analysis \n-\n CA\n(\nannoSpecies\n)\n\n\n\n\n\n\n\nPerform a hierarchical clustering of the following principal component created above to see the relationship between species and annotation.\n\n1\nanno.clustering\n-\n HCPC\n(\nanno.ca.analysis\n,\n nb.clust\n=\n-1\n,\n order\n=\nTRUE\n)\n\n\n\n\n\n\n1\nplot\n(\nanno.clustering\n)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat can you conclude from this clustering?\n\n\nAPPENDIX\n\n\nTo reproduce this hands on session, your R sessionInfo() must be identical to the following:\n\n\n1\nsessionInfo\n()", 
            "title": "Mining EMG with R part 2B"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#3-interproscan-relative-occurence-during-treatment", 
            "text": "Extract the data from the data structure  results  created earlier and partition the extracted data per datatype.  1\n2\n3\n4\n5 ipr  -  tbl_df ( results $ IPR_abundances ) \nipr -  separateDataType ( mydf = ipr ,  metadata = metadata ) \nipr_g  -  ipr $ genome\nipr_t  -  ipr $ transcript\nipr_a  -  ipr $ amplicon   Calculate the relative occurrence of IPR enrichment per day of treatment for both metagenomic and transcriptomic data types.  1\n2\n3\n4\n5\n6 ipr_metagenomics  -  ipr_g  % %  gather ( condition ,  count ,   3 : ncol ( ipr_g ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count )) \nipr_metatransciptomics  -  ipr_t  % %  gather ( condition ,  count ,   3 : ncol ( ipr_t ) -1 )   % % \ngroup_by (  condition )   % % \nmutate ( prop = count / sum ( count ))   \nMerge both metatranscriptomic and metagenomic relative occurrences of IPR per condition to visualise the relative abundance of IPR terms that are dynamic during treatment. 1\n2\n3 ipr_genomic_transcriptomics  -   rbind ( ipr_metagenomics ,  ipr_metatransciptomics ) \nipr_genomic_transcriptomics  -  ipr_genomic_transcriptomics  % % \nmutate ( description = as.character ( description ),  count = log2 ( count +1 ))    Bar plot of relative abundance of IPR   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ipr_genomic_transcriptomics  % %  arrange ( desc ( prop ))   % % \nggplot (  aes ( x = condition ,  y = prop , fill = description ))   + \ngeom_bar ( stat = identity ,  position = fill ,  aes ( fill  =  description ))   + \nfacet_wrap ( ~ DataType ,  ncol = 1 )   + \nggtitle ( IPR relative abundance through treatment\\n per datatype )   + \nylab ( proportion abundance )   + \ntheme_light ()   + \ntheme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ), \naxis.text.y = element_blank ())   + \ncoord_flip ()   + \ntheme ( legend.position = none )    Heat map of absolute InterproScan hit counts throughout treatment  1\n2\n3\n4\n5 ipr_genomic_transcriptomics  % % \nggplot (  aes ( x = condition , y = description )) +  facet_grid ( ~ DataType )   + \ngeom_tile ( aes ( fill = count ))   +  scale_fill_gradient ( low = white ,  high = darkblue )   + \nxlab ( )   +  ylab ( )   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ), \naxis.text.y = element_blank ())", 
            "title": "3. InterproScan: relative occurence during treatment."
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#4-go-all-category-relative-occurence-during-treatment", 
            "text": "The goal here is to create a composite plot of all GO category occurence during treatments. Give meaningful names to conditions as for the taxonomic case   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 GO  -  tbl_df ( results $ GO_abundances ) \nGO  -  GO  % %  mutate ( description = as.character ( description ),   category = as.character ( category )) \n\nGO_g  -  GO  % % \nselect ( matches ( _G|description|category ))   % % \nmutate ( DataType = Metagenomics ) \nGO_t  -  GO  % %  select ( matches ( _T|description|category ))   % % \nmutate ( DataType = Metatranscriptomics ) \nGO_a  -  GO  % %  select ( matches ( _A|description|category ))   % % \nmutate ( DataType = Amplicon )  colnames ( GO_g )   -  renameSample ( metadata = metadata ,  localdf = GO_g )  colnames ( GO_t )   -  renameSample ( metadata = metadata ,  localdf = GO_t )  colnames ( GO_a )   -  renameSample ( metadata = metadata ,  localdf = GO_a )    Transform the data from a wide format to a long format, making use of the tidyr package function gather, and compute the relative frequency of occurrence per days of treatment  1\n2\n3\n4\n5\n6 GO_metagenomics  -  GO_g  % %  gather ( condition ,  count ,   4 : ncol ( GO_g ) -1 )   % % \ngroup_by (  condition ,   category )   % % \nmutate ( prop = count / sum ( count )) \nGO_metatransciptomics  -  GO_t  % %  gather ( condition ,  count ,   4 : ncol ( GO_t ) -1 )   % % \ngroup_by (  condition ,   category )   % % \nmutate ( prop = count / sum ( count ))    Merge both metatranscriptomic and metagenomic relative occurrences of GO per condition to visualise the relative abundance of all GO category terms that are dynamic during treatment.  1 GO_genomic_transcriptomics  -   rbind ( GO_metagenomics ,  GO_metatransciptomics )    Barplot of relative abundance of all GO terms  1\n2\n3\n4\n5\n6\n7\n8\n9 GO_genomic_transcriptomics  % %  arrange ( desc ( prop ))   % % \nggplot (  aes ( x = condition ,  y = prop , fill = description ))   + \ngeom_bar ( stat = identity ,  position = fill ,  aes ( fill  =  description ))   + \nfacet_grid ( category ~ DataType )   + \nggtitle ( Gene Ontology relative abundance through days \\n of treatment per datatype )   + \nylab ( proportion abundance )   + \ntheme_light ()   +  theme ( axis.text.x = element_text ( angle = 45 ,  hjust = 1 ))   + \ncoord_flip ()   + \ntheme ( legend.position = none )   #+ guides(fill=guide_legend(ncol=2))", 
            "title": "4. GO (all category): relative occurence during treatment."
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#50-correspondance-analysisis-of-the-contingency-tables", 
            "text": "The goal of this analysis is to elucidate the relationship between species and GO terms with respect to various treatment conditions (days).\nWe will make use of the variable created above.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 head ( genomic_transcriptomics ) \ntaxonomy_long  -  genomic_transcriptomics  % %  ungroup ()   % % \nmutate ( condition = paste ( DataType ,  condition ,  sep = _ ))   % % \nselect ( phylum ,  condition ,  count )  head ( taxonomy_long ) \ntaxonomy_wide  -  spread ( taxonomy_long ,  condition ,  count )  rownames ( taxonomy_wide )   -  taxonomy_wide $ Phylum\ntaxonomy_wide\ntaxonomy_wide.df  -  select ( taxonomy_wide ,   - phylum )   % %  as.data.frame ()    Transpose the matrix for the purpose of plotting  1 taxonomy -   t ( taxonomy_wide.df )   \nDefine a main plot function to set up a plot for later use  1\n2\n3\n4 setup.plot  -  ggplot ()   +  coord_fixed ()   + \nlabs ( x = Comp1, Axis1 ,  y = Comp2, Axis2 )   + \ngeom_hline ( yintercept = 0 ,  col = darkgrey )   + \ngeom_vline ( xintercept = 0 ,  col = darkgrey )   \nMake the scree plot in a viewport  1\n2\n3\n4\n5\n6\n7 myscree  -   function ( eigs ,  x = 0.8 ,  y = 0.1 ,  just = c ( right , bottom )){ \nvp  -  viewport ( x = x ,  y = y ,  width = 0.2 ,  height = 0.2 ,  just = just ) \nmm  -   data.frame ( x = factor ( 1 : length ( eigs )),  y = eigs ) \nsp  -  ggplot ( mm ,  aes ( x = x ,  y = y ))   +  geom_bar ( stat = identity )   + \nlabs ( x  =   NULL ,  y  =   NULL )   +  theme_light ()  print ( sp ,  vp = vp )  }   \nExtract various datatype  1\n2\n3\n4 taxonomy.dna  -  taxonomy_wide.df  % %  select ( matches ( Metagenomics )) \ntaxonomy.rna  -  taxonomy_wide.df  % %  select ( matches ( Metatranscriptomics )) \ntaxonomy.dna -   t ( taxonomy.dna ) \ntaxonomy.rna -   t ( taxonomy.rna )   \nRead in a file describing the data we loaded previously into R  1\n2\n3\n4 data.dna  -  metadata [ metadata $ datatype == Metagenomic    ! ( metadata $ datatype == Amplicon_DNA |  metadata $ datatype == Amplicon_RNA ),] \ndata.rna  -  metadata [ metadata $ datatype == Metatranscriptomic    ! ( metadata $ datatype == Amplicon_DNA |  metadata $ datatype == Amplicon_RNA ),]    Perform a Correspondance analysis on Metagenomic data  1 taxonomy.coa  -  dudi.coa ( taxonomy.dna ,  scannf = F ,  nf = 2 )    Plot the correspondence analysis with Ade4 native plotter  1 scatter ( taxonomy.coa )    The plot can be made more visually appealing and easier to interpret with ggplot2, making use of data computed for us by Ade4 (dudi.coa). These are in the object return by the dudi.coa function.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 taxonomy.dna.plot -  setup.plot  +  geom_point ( data = data.frame ( taxonomy.coa $ li ,  data.dna ), \naes ( x = Axis1 ,  y = Axis2 ,  col = days ,  shape = days ,  size = 3 ))   + \ngeom_text ( data = taxonomy.coa $ co , \naes ( x = Comp1 ,  y = Comp2 ,  label = rownames ( taxonomy.coa $ co )), \nposition  =   jitter ,  alpha = 0.2 )   + \nscale_size_area ( breaks = c ( 0 , 3 , 6 , 11 , 14 , 40 ))   + \nscale_shape ( solid = F )   + \nlabs ( title = Correspondence Analysis: Metagenomics\\nTaxonomy abundance )   +  theme_light () \ntaxonomy.dna.plot\nmyscree ( taxonomy.coa $ eig  /   sum ( taxonomy.coa $ eig ))", 
            "title": "5.0 Correspondance Analysisis of the contingency tables."
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#51-perform-a-correspondence-analysis-on-the-metatranscriptomic-data", 
            "text": "Question   What can you conclude despite the very sparse data?  Merge taxonomic and GO_slim_abundances for correspondence analysis._\nOrganise the taxonomy data  1\n2\n3\n4\n5 taxonomy  -  tbl_df ( results $ phylum_taxonomy_abundances )   % %  select ( - kingdom ) \nphylum  -  taxonomy $ phylum\ntaxonomy  -  taxonomy  % %  select ( - phylum )   % %   t ()  colnames ( taxonomy )   -  phylum\ntaxonomy  -  taxonomy [ ! grepl ( _A$ , rownames ( taxonomy )),]    Organise the ontologies data  1 ontology.meta  -  tbl_df ( results $ GO_slim_abundances )   % % select ( GO , description ,   category )    Organise the Biological processes data  1\n2\n3\n4 go.bp  -  tbl_df ( results $ BP_GO_slim_abundances )   % % select ( - GO ,   - category ) \ndescription.go  -  go.bp $ description\ngo.bp  -  go.bp  % %  select ( - description )   % %   t ()  colnames ( go.bp )   -  description.go   Organise the Molecular function data 1\n2\n3\n4 go.mf  -  tbl_df ( results $ MF_GO_slim_abundances )   % % select ( - GO ,   - category ) \ndescription.go  -  go.mf $ description\ngo.mf  -  go.mf  % %  select ( - description )   % %   t ()  colnames ( go.mf )   -  description.go   Organise the Cellular compartment data 1\n2\n3\n4 go.cc  -  tbl_df ( results $ CC_GO_slim_abundances )   % % select ( - GO ,   - category ) \ndescription.go  -  go.cc $ description\ngo.cc  -  go.cc  % %  select ( - description )   % %   t ()  colnames ( go.cc )   -  description.go   Organise the IPR data  1\n2\n3\n4 ipr  -  tbl_df ( results $ IPR_abundances )   % %  select ( - IPR ) \ndescription.ipr  -  ipr $ description\nipr  -  ipr  % %  select ( - description )   % %   t ()  colnames ( ipr )   -  description.ipr   Read in the annotation material  1 metainfo  -  metadata   Filter out the amplicons data  1\n2 metainfo  -  metainfo  % %  filter ( ! grepl ( _A$ ,  id )) \nmetainfo   Merge the Taxonomy and Molecular Functions and perform Correspondence Analysis (CA) on the merge set  1\n2 mydf  -   cbind ( taxonomy ,  go.mf ) \nmydf  -  mydf [ rownames ( mydf )   %in%  metainfo $ id ,]    Call on dudi.coa function from Ade4 package to perform CA  1 mydf.coa  -  dudi.coa ( mydf ,  scannf = F ,  nf = 2 )    Visualize the CA analysis with ggplot2   Question   What can you conclude from this analysis?", 
            "title": "5.1 Perform a Correspondence analysis on the Metatranscriptomic data"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#52-merge-taxonomy-and-biological-processess-and-perform-correspondence-analysis-ca-on-the-merge-set", 
            "text": "1\n2\n3 mydf  -   cbind ( taxonomy ,  go.bp ) \nmydf  -  mydf [ rownames ( mydf )   %in%  metainfo $ id ,] \nmydf.coa  -  dudi.coa ( mydf ,  scannf = F ,  nf = 2 )   \nVisualize the CA analysis with ggplot2   Question   What can you conclude from this analysis?", 
            "title": "5.2 Merge Taxonomy and Biological processess and perform Correspondence Analysis (CA) on the merge set"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#53-merge-taxonomy-and-cellular-compartment-and-perform-correspondence-analysis-ca-on-the-merge-set", 
            "text": "1\n2\n3 mydf  -   cbind ( taxonomy ,  go.cc ) \nmydf  -  mydf [ rownames ( mydf )   %in%  metainfo $ id ,] \nmydf.coa  -  dudi.coa ( mydf ,  scannf = F ,  nf = 2 )    Visualize the CA analysis with ggplot2   Question   What can you conclude from this analysis?", 
            "title": "5.3 Merge Taxonomy and Cellular Compartment and perform Correspondence Analysis (CA) on the merge set"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#54-merge-taxonomy-cellular-compartments-and-molecular-functions-then-perform-correspondence-analysis-ca-on-the-merge-set", 
            "text": "Merging Taxonomy and Cellular compartment and Molecular function  1\n2\n3 mydf  -   cbind ( taxonomy ,  go.cc ,  go.mf ) \nmydf  -  mydf [ rownames ( mydf )   %in%  metainfo $ id ,] \nmydf.coa  -  dudi.coa ( mydf ,  scannf = F ,  nf = 2 )   \nVisualize the CA analysis with ggplot2   Question   What can you conclude from this analysis?", 
            "title": "5.4 Merge Taxonomy, Cellular Compartments and Molecular functions then perform Correspondence Analysis (CA) on the merge set"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#60-clustering-of-the-principal-component", 
            "text": "1\n2 compositedata  -   as.data.frame ( mydf )  rownames ( compositedata )   -   paste ( metainfo $ days ,  metainfo $ genomictype ,  sep = _   )   \nRemove columns that only contain zero as they are not contributing to the creation of the component when using FactoMineR  1 compositedata -  compositedata [,   which ( ! apply ( compositedata , 2 , FUN = function ( x ){ all ( x == 0 )}))]   \nPerform a correspondence analysis using FactoMineR. This automatically generates a perceptual map, showing the relationship between GO terms, species, and days of treatment and various datatypes. 1 ca.analysis  -  CA ( compositedata )   \nPerform a hierarchical clustering of the principal component created above to see the relationship between days of experiment and datatypes. 1 clustering -  HCPC ( ca.analysis ,  nb.clust = -1 ,  order = TRUE )    1 plot ( clustering )     Question   What do you conclude from this graph?", 
            "title": "6.0 Clustering of the Principal Component"
        }, 
        {
            "location": "/modules/metagenomics-module-emgr/emgr-part2B/#61-to-find-out-the-relationship-between-annotationsbpcc-species-we-transpose-the-above-table-prior-to-performing-a-ca", 
            "text": "1\n2 annoSpecies  -   as.data.frame ( t ( compositedata ))  rownames ( annoSpecies )   -   rownames ( t ( compositedata ))    CA  1 anno.ca.analysis  -  CA ( annoSpecies )    Perform a hierarchical clustering of the following principal component created above to see the relationship between species and annotation. 1 anno.clustering -  HCPC ( anno.ca.analysis ,  nb.clust = -1 ,  order = TRUE )    1 plot ( anno.clustering )     Question   What can you conclude from this clustering?  APPENDIX  To reproduce this hands on session, your R sessionInfo() must be identical to the following:  1 sessionInfo ()", 
            "title": "6.1 To find out the relationship between annotations(BP,CC), species we transpose the above table prior to performing a CA."
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/", 
            "text": "Whole Genome Analysis\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand the main approaches to perform metagenomics assembly\n\n\n\n\n\n\nBe able to perform assembly on your data and assess the quality of your assembly\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nMeta-Velvet: \n\n\nhttps://github.com/hacchy/MetaVelvet\n\n\nUseful Links\n\n\n\n\nMeta-Velvet: \n\n\nhttp://metavelvet.dna.bio.keio.ac.jp/\n\n\nIntroduction\n\n\n\n\nPerforming genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.\n\n\nThe k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor:\n\nhttp://dna.med.monash.edu.au/~torsten/velvet_advisor/\n\n\nBuilding a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.\n\n\nAssessing the quality of an assembly\n\n\nFor genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics\n\n\nPractical\n\n\n\n\nThe purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.\n\n\nInvestigation of the input sequence file\n\n\nOpen a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each\n\n\n1\n2\n3\n4\n5\n6\ncd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c \n A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta\n\n\n\n\n\n\nThe output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.\n\n\nPerforming the assembly using Velvet and Meta-Velvet\n\n\nVelvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.\n\n\nWe will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:\n\n\n1\n2\n# first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta\n\n\n\n\n\n\nThen run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:\n\n\n1\nvelvetg A7A-59 \u2013exp_cov auto\n\n\n\n\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:\n\n\n1\nmeta-velvetg A7A-59 | tee logfile\n\n\n\n\n\n\nAssessing the quality of the assembly\n\n\nThe main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c\n\u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:\n\n\n1\n2\n3\ncd A7A-59\ngrep -c \n meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa\n\n\n\n\n\n\nIt shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275\nth\n contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).\n\n\nChanging the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263\n\n\n\n\n\n\nThe number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850\n\n\n\n\n\n\nWithout extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.", 
            "title": "Metagenomics WGA"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#whole-genome-analysis", 
            "text": "", 
            "title": "Whole Genome Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Understand the main approaches to perform metagenomics assembly    Be able to perform assembly on your data and assess the quality of your assembly", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#tools-used", 
            "text": "Meta-Velvet:   https://github.com/hacchy/MetaVelvet", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#useful-links", 
            "text": "Meta-Velvet:   http://metavelvet.dna.bio.keio.ac.jp/", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#introduction", 
            "text": "Performing genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.  The k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor: http://dna.med.monash.edu.au/~torsten/velvet_advisor/  Building a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-an-assembly", 
            "text": "For genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics", 
            "title": "Assessing the quality of an assembly"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#practical", 
            "text": "The purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.", 
            "title": "Practical"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#investigation-of-the-input-sequence-file", 
            "text": "Open a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each  1\n2\n3\n4\n5\n6 cd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c   A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta   The output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.", 
            "title": "Investigation of the input sequence file"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#performing-the-assembly-using-velvet-and-meta-velvet", 
            "text": "Velvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.  We will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:  1\n2 # first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta   Then run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:  1 velvetg A7A-59 \u2013exp_cov auto   Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:  1 meta-velvetg A7A-59 | tee logfile", 
            "title": "Performing the assembly using Velvet and Meta-Velvet"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-the-assembly", 
            "text": "The main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:  Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c \u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:  1\n2\n3 cd A7A-59\ngrep -c   meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa   It shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275 th  contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).  Changing the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:  1\n2\n3\n4\n5\n6\n7\n8 sum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263   The number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):  1\n2\n3\n4\n5\n6\n7\n8 sum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850   Without extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.", 
            "title": "Assessing the quality of the assembly"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/", 
            "text": "EMG Functional Data Analysis\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand how EMG provides functional analysis of metagenomic data sets\n\n\n\n\n\n\nKnow where to find and how to interpret analysis results for samples on the EMG website\n\n\n\n\n\n\nKnow how to download the raw sample data and analysis results for use with 3\nrd\n party visualisation and statistical analysis packages\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nMegan6 :  \nhttp://ab.inf.uni-tuebingen.de/software/megan6/\n\n\nUseful Links\n\n\n\n\nEBI Metagenomics resource (EMG) : \n\n\nwww.ebi.ac.uk/metagenomics/\n\n\nIntroduction\n\n\n\n\nThe EBI Metagenomics resource (EMG) provides functional analysis of predicted coding sequences (pCDS) from metagenomic data sets using the InterPro database. InterPro is a sequence analysis resource that predicts protein family membership, along with the presence of important domains and sites. It does this by combining predictive models known as protein signatures from a number of different databases into a single searchable resource. InterPro curators manually integrate the different signatures, providing names and descriptive abstracts and, whenever possible, adding Gene Ontology (GO) terms. Links are also provided to pathway databases, such as KEGG, MetaCyc and Reactome, and to structural resources, such as SCOP, CATH and PDB.\n\n\nWhat are protein signatures?\n\n\nProtein signatures are obtained by modelling the conservation of amino acids at specific positions within a group of related proteins (i.e., a protein family), or within the domains/sites shared by a group of proteins. InterPro\u2019s different member databases use different computational methods to produce protein signatures, and they each have their own particular focus of interest: structural and/or functional domains, protein families, or protein features, such as active sites or binding sites (see Figure 1).\n\n\n\n\nFigure 1. InterPro member databases grouped by the methods, indicated in white coloured text, used to construct their signatures. Their focus of interest is shown in blue text.\n\n\nOnly a subset of the InterPro member databases are used by EMG: Gene3D, TIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were selected since, together, they provide both high coverage and offer detailed functional analysis, and have underlying algorithms that can cope with the vast amounts of fragmentary sequence data found in metagenomic datasets.\n\n\nAssigning functional information to metagenomic sequences\n\n\nWhilst InterPro matches to metagenomic sequence sets are informative in their own right, EMG offers an additional type of analysis in the form of Gene Ontology (GO) terms. The Gene Ontology is made up of 3 structured controlled vocabularies that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner. By using GO terms, scientists working on different species or using different databases can compare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into hierarchies, with less specific terms towards the top and more specific terms towards the bottom (see Figure 2).\n\n\n\n\nFigure 2. An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding is a type of cytoskeletal binding, which is a type of protein binding). Note that a GO term can have more than one parent term. The Gene Ontology also allows for different types of relationships between terms (such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only uses the straightforward \u2018is a\u2019 relationships.\n\n\nMore information about the GO project can be found\n\nhttp://www.geneontology.org/GO.doc.shtml\n\n\nAs part of the EMG analysis pipeline, GO terms for molecular function,biological process and cellular component are associated to pCDS in a sample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be accurately applied to all of the proteins matching that entry. Sequences searched against InterPro are then associated with GO terms by virtue of the entries they match - a protein that matches one InterPro entry with the GO term \u2018kinase activity\u2019 and another InterPro entry with the GO term \u2018zinc ion binding\u2019 will be annotated with both GO terms.\n\n\nFinding functional information about samples on the EMG website\n\n\nFunctional analysis of samples within projects on the EMG website\n\nwww.ebi.ac.uk/metagenomics/\n can be accessed by clicking on the Functional Analysis tab found toward the top of any sample page (see Figure 3 below).\n\n\n\n\nFigure 3. A Functional analysis tab can be found towards the top of each run page\n\n\nClicking on this tab brings up a page displaying sequence features (the number of reads with pCDS, the number of pCDS with InterPro matches, etc), InterPro match information and GO term annotation for the sample, as shown in Figure 4 and 5 below.\n\n\nInterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.\n\n\n\n\nFigure 4. Functional analysis of metagenomics data, as shown on the EMG website.\n\n\n\n\nFigure 5. The GO terms predicted for the sample are displayed. Different graphical representations are available, and can be selected by clicking on the \u2018Switch view\u2019 icons.\n\n\nThe Gene Ontology terms displayed graphically on the web site have been \u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO slims are cut-down versions of the Gene Ontology, containing a subset of the terms in the whole GO. They give a broad overview of the ontology content without the detail of the specific fine-grained terms.\n\n\nThe full data sets used to generate both the InterPro and GO overview charts, along with a host of additional data (processed reads, pCDS, reads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for further analysis by clicking the Download tab, found towards the top of the page (see Figure 6).\n\n\n\n\nFigure 6. Each sample has a download tab, where the full set of sequences, analyses, summaries and raw data can be downloaded.\n\n\nPractical\n\n\n\n\nBrowsing analysed data via the EMG website\n\n\nFor this session, we are going to look at a Metagenome of a microbial consortium obtained from the Tuna oil field in the Gippsland Basin, Australia project. In 2011, fluid samples were collected from the A7A oil well in the Tuna oil field (38\u00b010\n S, 148\u00b025\n E) in the Gippsland Basin. A map of the sampling site is shown on the project page: \nhttps://www.ebi.ac.uk/metagenomics/projects/ERP004636\n\n\nTo get the Tuna project page, follow the above link or open the Metagenomics Portal home page (\nhttps://www.ebi.ac.uk/metagenomics/\n).\n\n\n\n\nenter \u2018\nERP004636\n\u2019 in the search box on the top right hand side of the page, and follow the link to project. You should now have a Project overview page, which describes the project, submitter contact details, and links to the samples and runs that the project contains. \n\n\nClick on the Sample Name link (not the Run link) to arrive at the overview page, describing various contextual data, such as the geographic location from which the material was isolated, its collection date, and so on.\n\n\n\n\n\n\nQuestion 1:\n\n\nWhat is the latitude, longitude and depth at which the sample was collected?\n\n\n\n\n\n\nAnswer\n\n\n-38.1700 , 148.4200\n\n\n\n\n\n\nQuestion 2:\n\n\nWhat geographic location does this correspond to?\n\n\n\n\n\n\nAnswer\n\n\nAustralia\n\n\n\n\n\n\nQuestion 3:\n\n\nWhat environmental ontology (ENVO) identifer and name has the sample material been annotated with?\n\n\n\n\n\n\nAnswer\n\n\nocean water ENVO:00002151\n\n\n\n\nNow scroll down to the \u2018Associated runs\u2019 section of the page. Some samples can have a number of sequencing runs associated with them (for example, corresponding to 16S rRNA amplicon analyses and WGS analyses performed on the same sample). In this case, there is only 1 associated run: \nERR413102\n. Click on the Run ID to go to the Run page.\n\n\nThis page has a number of tabs towards the top (Overview, Quality control, Taxonomy analysis, Functional analysis, and Download). \n\n\nClick on the \u2018\nDownload\n\u2019 tab. Right click the file labelled \u2018\nPredicted CDS (FASTA)\n\u2019 link, and save this file to your desktop. Find the file (it should be in your Downloads folder), unzip it and examine it using \u2018less\u2019 by typing the following commands in a terminal window:\n\n\n1\n2\n3\ncd ~/Downloads\ngzip \u2013ERR4131021_MERGED_FASTQ_pCDS.faa.gz \nless ERR413102_MERGED_FASTQ_pCDS.faa\n\n\n\n\n\n\nHave a look at one or two of the many sequences it contains. \n\n\nYou can count the total number of sequences in the file by grepping the number of header lines that start with \u201c\n\u201d\n\n\n1\ngrep -c \n^\n ERR413102_MERGED_FASTQ_pCDS.faa\n\n\n\n\n\n\nIn a moment, we will look at the analysis results for this entire batch of sequences, displayed on the EMG website. First, we will attempt to analyse just one of the predicted coding sequences using InterPro (the analysis results on the EMG website summarise these kind of results for hundreds of thousands of sequences).\n\n\nIn a new tab or window, open your web browser and navigate to \nhttp://www.ebi.ac.uk/interpro/\n. Copy and paste the following sequence into the text box on the InterPro home page where it says \u2018Analyse your sequence\u2019:\n\n\n1\n2\nHWI-M02024:110:000000000-A8H0K:1:1101:23198:21331-1:N:0:TCAGAGAC_1_267\nHLLSYRYAYGKFSSTHEATIGGCFLTKDEELDDHIVKYEIWDTAGKNGTIHLPRCTTSKAYXIQVTWYRNAIAAVVVFDVTSRDSFEK\n\n\n\n\n\n\nPress Search and wait for your results. Your sequence will be run through the InterProScan analysis software, which attempts to match it against all of the different signatures in the InterProScan database.\n\n\n\n\nQuestion 4:\n\n\nWhich protein family does InterProScan predict your sequence belongs to, and what GO terms are predicted to describe its function?\n\n\n\n\n\n\nAnswer\n\n\nHere is an answer.\n\n\n\n\nClicking on the InterPro entry name or IPR accession number will take you to the InterPro entry page for your result, where more information\ncan be found.\n\n\nReturn to the overview page for \nERR413102\n.\n\n\nNow we are going to look at the functional analysis results for all of the pCDS in the sample. First, we will find the number of sequences that made it through to the functional analysis section of the pipeline.\n\n\nClick on the Quality control tab. This page displays a series of charts, showing how many sequences passed each quality control step, how many reads were left after clustering, and so on.\n\n\n\n\nQuestion 5:\n\n\nAfter all of the quality filtering steps are complete, how many reads were submitted for analysis by the pipeline?\n\n\n\n\n\n\nAnswer\n\n\n44,429,715 Reads.\n\n\n\n\nNext, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.\n\n\nClick on the Functional analysis tab and examine the InterPro match section. The top part of this page shows a sequence feature summary, showing the number of reads with predicted coding sequences (pCDS), the number of pCDS with InterPro matches, etc.\n\n\n\n\nQuestion 6:\n\n\nHow many predicted coding sequences (pCDS) are in the run?\n\n\n\n\n\n\nAnswer\n\n\n19,838,031 pCDS.\n\n\n\n\n\n\nQuestion 7:\n\n\nHow many pCDS have InterProScan hits?\n\n\n\n\n\n\nAnswer\n\n\n5,754,502 pCDS.\n\n\n\n\nScroll down the page to the InterPro match summary section\n\n\n\n\nQuestion 8:\n\n\nHow many different InterPro entries were matched by the pCDS?\n\n\n\n\n\n\nAnswer\n\n\n4,207 IPRO hits.\n\n\n\n\n\n\nQuestion 9:\n\n\nWhy is this figure different to the number of pCDS that have InterProScan hits?\n\n\n\n\n\n\nAnswer\n\n\nMultiple CDS match an interpro id.\n\n\n\n\nNext we will examine the GO terms predicted by InterPro for the pCDS in the sample.\n\n\nScroll down to the GO term annotation section of the page and examine the 3 bar charts, which correspond to the 3 different components of the Gene Ontology.Selecting the pie chart representation of GO terms makes it easier to visualise the data to find the answer.\n\n\n\n\nQuestion 10:\n\n\nWhat are the top 3 biological process terms predicted for the pCDS from the sample?\n\n\n\n\n\n\nAnswer\n\n\nbiosynthetic process, nitrogen compound metabolism, small molecule metabolic process\n\n\n\n\n\n\nQuestion 11:\n\n\nHow many of the WGS reads are predicted to encode 16S rRNAs?\n\n\n\n\n\n\nAnswer\n\n\n41,436\n\n\n\n\nNow we will look at the taxonomic analysis for this run.\n\n\nClick on the Taxonomy Analysis tab and examine the phylum composition graph and table.\n\n\n\n\nQuestion 12:\n\n\nWhat are the top 3 phyla in the run, according to 16S rRNA analysis?\n\n\n\n\n\n\nAnswer\n\n\nUnassigned 48%, protobacteria 20.81%, Firmicutes 14.721% and Thermotogae 8.19%\n\n\n\n\nSelect the Krona chart view of the data icon. This brings up an interactive chart that can be used to analyse data at different\ntaxonomic ranks.\n\n\n\n\nQuestion 13:\n\n\nWhat is the proportion of Thermoanaerobacteraceae in the population?\n\n\n\n\n\n\nAnswer\n\n\nTotal 161, 55% Clostridia; 41% Firmicutes; 6% of all\n\n\n\n\nNow we will compare these analyses with those for a WGS practical.\n\n\n\n\nQuestion 14:\n\n\nWhat are the differences in Taxonomy and Functional analysis?\n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\n\n\nQuestion 15:\n\n\nDo the number of pCDS predicted differ? \n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\n\n\nQuestion 16:\n\n\nDo the number of pCDSs with an InterPro match differ?\n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\n\n\nQuestion 17:\n\n\nDo InterPro entries matched differ?\n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\n\n\nQuestion 18:\n\n\nAre these figures broadly comparable to those for the previous analysis?\n\n\n\n\n\n\nAnswer\n\n\nTBD\n\n\n\n\nEMG comparison tools\n\n\nTo look at the differences in slimmed GO terms between the 2 runs. There are two ways to do this. First, you can simply scroll to the bottom of the page and examine the GO term annotation (note - selecting the bar chart representation of GO terms makes it easier to compare different data sets). Alternatively, you can use the comparison tool, which allows direct comparison of runs within a project. The tool can be accessed by clicking on the \u2018Comparison Tool\u2019 tab. At present, the tool only compares slimmed GO terms, but will be expanded to cover full GO terms, InterPro annotations, and taxonomic profiles as development of the site continues.\n\n\n\n\nBonus exercise\n\n\nBonus exercise.\n\n\n\n\n\n\nClick on the Comparison tool tab and choose the Ocean Sampling Day (OSD) 2014 project from the sample list and select the OSD80_2014-06- 21_0m_NPL022 - ERR770971 and OSD80_2014-06-21_2m_NPL022 - ERR770970.\n\n\n\n\n\n\nQuestion 19:\n\n\nAre there visible differences between the GO terms for these runs. Could there be any biological explanation for this?\n\n\n\n\n\n\nAnswer\n\n\nYes, shift in proportion of some functions. yes climate and population density. \n\n\n\n\nNavigate back and open the taxonomic analysis results tab for each run.\n\n\n\n\nQuestion 20:\n\n\nHow does the taxonomic composition differ between runs? Are any trends in the data consistent with your answer to question 19?\n\n\n\n\n\n\nAnswer\n\n\nUnassigned dropped from 48% to 13% and polaribacter is Total 931, 80% Flavobacteriaceae; 74% Bacteriodetes; 43% of all\n\n\n\n\nVisualising taxonomic data using MEGAN\n\n\nNext, we are going to look at the taxonomic predictions for all of the runs. To do this, we are going to load them into MEGAN. MEGAN is a tool suite that provides metagenomic data analysis and visualization. We are going to use only a small subset of its features, relating to taxonomic comparison. Detailed information on MEGAN and the analyses and visualisations it offers can be found here: \n\n1\nhttp://ab.inf.uni-tuebingen.de/data/software/megan5/download/manual.pdf\n\n\n\n\n\nMEGAN can be downloaded from \n\n1\nhttp://ab.inf.uni-tuebingen.de/software/megan6/\n\n\n\n\n(there are 2 editions Community and Ultimate. The Community edition is open source and free to download. We will use this version for the tutorial). However, it should already be installed on your Desktop.\n\n\nClick\n on the MEGAN icon on your desktop to load the s/w.\n\n\nWe now need to download the full taxonomic predictions for all of the runs in the Ocean Sampling Day project.\n\n\nNavigate to the Project: Ocean Sampling Day (OSD) page, using the breadcrumb URL link at the top of the page. Click on the Analysis summary tab, which will take you to a set of tab separated result matrix files, summarising the taxonomic and functional observations for all runs in the project.\n\n\nClick on the Taxonomic assignments (TSV) link, which will download the corresponding file to your computer. When prompted, choose to save the file in the Downloads folder.\n\n\nOpen the Terminal, navigate to the Downloads directory and take a look at the file you have just downloaded (hit q to exit less):\n\n\n1\n2\ncd\n ~/Downloads\nless -S ERP009703_taxonomy_abundances_v2.0.tsv\n\n\n\n\n\n\nYou will see it is a large matrix file, with abundance counts for each taxonomic lineage for each run in the project. From the MEGAN menu, choose \u2018File\u2019 and \u2018Import\u2019. Select \u2018CSV Import\u2019 and then find the file you have just downloaded and edited.\n\n\nFrom the pop up menu, go with the default options in the Format and Separator sections (which should have the \u2018Class, Count\u2019 option selected under format, and the separator set as \u2018Tab\u2019) and select \u2018Taxonomy\u2019 in classification section, then press \u2018Apply\u2019.\n\n\nThere are many different visualisations and comparisons that can be performed using MEGAN, so feel free to explore the data using the tool.\n\n\nClick on the \u2018Show chart\u2019 icon and choose \u2018Stacked Bar Chart\u2019. This should give you a bar chart, showing the abundance reads for taxonomic lineages across all of the sampling sites.\n\n\n\n\nQuestion 21:\n\n\nAre the number of classified 16S reads roughly equivalent across all of the different sampling sites?\n\n\n\n\n\n\nAnswer\n\n\nNo, range from ERR771006 ~10 to ~3,250 ERR771046\n\n\n\n\n\n\nQuestion 22:\n\n\nWhich run appears particularly enriched in Polaribacter?\n\n\n\n\n\n\nAnswer\n\n\nERR770970 ~920\n\n\n\n\nYou can change between abundance counts and relative abundance using the Options drop menu and choosing % Percentage Scale or Linear Scale.\n\n\nChange the chart view to \u2018Bubble Chart\u2019. This visualisation can be useful when comparing a large number of samples.\n\n\n\n\nQuestion 23:\n\n\nDo any samples contain taxa not found in other samples? \n\n\n\n\nTake a look at the Schlegelella genus.\n\n\n\n\nAnswer\n\n\nYes, schlegelella is only found in ERR770961 (64)\n\n\n\n\n\n\nQuestion 24:\n\n\nCan you discern any patterns in the geographical distribution of certain species (for example, the cluster of samples enriched for the lactobacillus genus compared to other samples)?\n\n\n\n\n\n\nAnswer\n\n\nlactobacillus ERR771039-58 Belgian\n\n\n\n\nWe are now going to take a look at which other datasets in EBI Metagenomics that lactobacilli are found in. Point your browser at \nhttps://www.ebi.ac.uk/metagenomics/search/\n\n\nThis interface allows you to search the project, sample and run related metadata and analysis results for all of the publicly available datasets in the EBI Metagenomics resource.\n\n\nClick on the \u2018Runs\u2019 tab. You should now see a number of run-related metadata search facets on the left hand side of the page, including \u2018Organism\u2019. Click on the \u2018More\n\u2019 option under Organism, scroll down to \u2018Lactobacillus\u2019 in the pop-up window and select the check box next to it. Now click \u2018Filter\u2019. The results page should now show all of the runs that have taxonomic matches to lactobacilli in their datasets. The \u2018Biome\u2019 facet on the left hand side of the page now shows the number of matching datasets in each biome category (to save space, the 10 biome categories with the most matching datasets are shown by default).\n\n\n\n\nQuestion 25:\n\n\nWhich biome category has the most datasets that contain lactobacilli?\n\n\n\n\n\n\nAnswer\n\n\nHost-associated (67,031) human digestive\n\n\n\n\n\n\nQuestion 26:\n\n\nHow well does this correlate with what\u2019s known about these bacteria?\n\n\n\n\n\n\nAnswer\n\n\nTBD", 
            "title": "Metagenomics Functional Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#emg-functional-data-analysis", 
            "text": "", 
            "title": "EMG Functional Data Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Understand how EMG provides functional analysis of metagenomic data sets    Know where to find and how to interpret analysis results for samples on the EMG website    Know how to download the raw sample data and analysis results for use with 3 rd  party visualisation and statistical analysis packages", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#tools-used", 
            "text": "Megan6 :   http://ab.inf.uni-tuebingen.de/software/megan6/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#useful-links", 
            "text": "EBI Metagenomics resource (EMG) :   www.ebi.ac.uk/metagenomics/", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#introduction", 
            "text": "The EBI Metagenomics resource (EMG) provides functional analysis of predicted coding sequences (pCDS) from metagenomic data sets using the InterPro database. InterPro is a sequence analysis resource that predicts protein family membership, along with the presence of important domains and sites. It does this by combining predictive models known as protein signatures from a number of different databases into a single searchable resource. InterPro curators manually integrate the different signatures, providing names and descriptive abstracts and, whenever possible, adding Gene Ontology (GO) terms. Links are also provided to pathway databases, such as KEGG, MetaCyc and Reactome, and to structural resources, such as SCOP, CATH and PDB.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#what-are-protein-signatures", 
            "text": "Protein signatures are obtained by modelling the conservation of amino acids at specific positions within a group of related proteins (i.e., a protein family), or within the domains/sites shared by a group of proteins. InterPro\u2019s different member databases use different computational methods to produce protein signatures, and they each have their own particular focus of interest: structural and/or functional domains, protein families, or protein features, such as active sites or binding sites (see Figure 1).   Figure 1. InterPro member databases grouped by the methods, indicated in white coloured text, used to construct their signatures. Their focus of interest is shown in blue text.  Only a subset of the InterPro member databases are used by EMG: Gene3D, TIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were selected since, together, they provide both high coverage and offer detailed functional analysis, and have underlying algorithms that can cope with the vast amounts of fragmentary sequence data found in metagenomic datasets.", 
            "title": "What are protein signatures?"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#assigning-functional-information-to-metagenomic-sequences", 
            "text": "Whilst InterPro matches to metagenomic sequence sets are informative in their own right, EMG offers an additional type of analysis in the form of Gene Ontology (GO) terms. The Gene Ontology is made up of 3 structured controlled vocabularies that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner. By using GO terms, scientists working on different species or using different databases can compare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into hierarchies, with less specific terms towards the top and more specific terms towards the bottom (see Figure 2).   Figure 2. An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding is a type of cytoskeletal binding, which is a type of protein binding). Note that a GO term can have more than one parent term. The Gene Ontology also allows for different types of relationships between terms (such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only uses the straightforward \u2018is a\u2019 relationships.  More information about the GO project can be found http://www.geneontology.org/GO.doc.shtml  As part of the EMG analysis pipeline, GO terms for molecular function,biological process and cellular component are associated to pCDS in a sample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be accurately applied to all of the proteins matching that entry. Sequences searched against InterPro are then associated with GO terms by virtue of the entries they match - a protein that matches one InterPro entry with the GO term \u2018kinase activity\u2019 and another InterPro entry with the GO term \u2018zinc ion binding\u2019 will be annotated with both GO terms.", 
            "title": "Assigning functional information to metagenomic sequences"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#finding-functional-information-about-samples-on-the-emg-website", 
            "text": "Functional analysis of samples within projects on the EMG website www.ebi.ac.uk/metagenomics/  can be accessed by clicking on the Functional Analysis tab found toward the top of any sample page (see Figure 3 below).   Figure 3. A Functional analysis tab can be found towards the top of each run page  Clicking on this tab brings up a page displaying sequence features (the number of reads with pCDS, the number of pCDS with InterPro matches, etc), InterPro match information and GO term annotation for the sample, as shown in Figure 4 and 5 below.  InterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.   Figure 4. Functional analysis of metagenomics data, as shown on the EMG website.   Figure 5. The GO terms predicted for the sample are displayed. Different graphical representations are available, and can be selected by clicking on the \u2018Switch view\u2019 icons.  The Gene Ontology terms displayed graphically on the web site have been \u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO slims are cut-down versions of the Gene Ontology, containing a subset of the terms in the whole GO. They give a broad overview of the ontology content without the detail of the specific fine-grained terms.  The full data sets used to generate both the InterPro and GO overview charts, along with a host of additional data (processed reads, pCDS, reads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for further analysis by clicking the Download tab, found towards the top of the page (see Figure 6).   Figure 6. Each sample has a download tab, where the full set of sequences, analyses, summaries and raw data can be downloaded.", 
            "title": "Finding functional information about samples on the EMG website"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#practical", 
            "text": "", 
            "title": "Practical"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#browsing-analysed-data-via-the-emg-website", 
            "text": "For this session, we are going to look at a Metagenome of a microbial consortium obtained from the Tuna oil field in the Gippsland Basin, Australia project. In 2011, fluid samples were collected from the A7A oil well in the Tuna oil field (38\u00b010  S, 148\u00b025  E) in the Gippsland Basin. A map of the sampling site is shown on the project page:  https://www.ebi.ac.uk/metagenomics/projects/ERP004636  To get the Tuna project page, follow the above link or open the Metagenomics Portal home page ( https://www.ebi.ac.uk/metagenomics/ ).   enter \u2018 ERP004636 \u2019 in the search box on the top right hand side of the page, and follow the link to project. You should now have a Project overview page, which describes the project, submitter contact details, and links to the samples and runs that the project contains.   Click on the Sample Name link (not the Run link) to arrive at the overview page, describing various contextual data, such as the geographic location from which the material was isolated, its collection date, and so on.    Question 1:  What is the latitude, longitude and depth at which the sample was collected?    Answer  -38.1700 , 148.4200    Question 2:  What geographic location does this correspond to?    Answer  Australia    Question 3:  What environmental ontology (ENVO) identifer and name has the sample material been annotated with?    Answer  ocean water ENVO:00002151   Now scroll down to the \u2018Associated runs\u2019 section of the page. Some samples can have a number of sequencing runs associated with them (for example, corresponding to 16S rRNA amplicon analyses and WGS analyses performed on the same sample). In this case, there is only 1 associated run:  ERR413102 . Click on the Run ID to go to the Run page.  This page has a number of tabs towards the top (Overview, Quality control, Taxonomy analysis, Functional analysis, and Download).   Click on the \u2018 Download \u2019 tab. Right click the file labelled \u2018 Predicted CDS (FASTA) \u2019 link, and save this file to your desktop. Find the file (it should be in your Downloads folder), unzip it and examine it using \u2018less\u2019 by typing the following commands in a terminal window:  1\n2\n3 cd ~/Downloads\ngzip \u2013ERR4131021_MERGED_FASTQ_pCDS.faa.gz \nless ERR413102_MERGED_FASTQ_pCDS.faa   Have a look at one or two of the many sequences it contains.   You can count the total number of sequences in the file by grepping the number of header lines that start with \u201c \u201d  1 grep -c  ^  ERR413102_MERGED_FASTQ_pCDS.faa   In a moment, we will look at the analysis results for this entire batch of sequences, displayed on the EMG website. First, we will attempt to analyse just one of the predicted coding sequences using InterPro (the analysis results on the EMG website summarise these kind of results for hundreds of thousands of sequences).  In a new tab or window, open your web browser and navigate to  http://www.ebi.ac.uk/interpro/ . Copy and paste the following sequence into the text box on the InterPro home page where it says \u2018Analyse your sequence\u2019:  1\n2 HWI-M02024:110:000000000-A8H0K:1:1101:23198:21331-1:N:0:TCAGAGAC_1_267\nHLLSYRYAYGKFSSTHEATIGGCFLTKDEELDDHIVKYEIWDTAGKNGTIHLPRCTTSKAYXIQVTWYRNAIAAVVVFDVTSRDSFEK   Press Search and wait for your results. Your sequence will be run through the InterProScan analysis software, which attempts to match it against all of the different signatures in the InterProScan database.   Question 4:  Which protein family does InterProScan predict your sequence belongs to, and what GO terms are predicted to describe its function?    Answer  Here is an answer.   Clicking on the InterPro entry name or IPR accession number will take you to the InterPro entry page for your result, where more information\ncan be found.  Return to the overview page for  ERR413102 .  Now we are going to look at the functional analysis results for all of the pCDS in the sample. First, we will find the number of sequences that made it through to the functional analysis section of the pipeline.  Click on the Quality control tab. This page displays a series of charts, showing how many sequences passed each quality control step, how many reads were left after clustering, and so on.   Question 5:  After all of the quality filtering steps are complete, how many reads were submitted for analysis by the pipeline?    Answer  44,429,715 Reads.   Next, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.  Click on the Functional analysis tab and examine the InterPro match section. The top part of this page shows a sequence feature summary, showing the number of reads with predicted coding sequences (pCDS), the number of pCDS with InterPro matches, etc.   Question 6:  How many predicted coding sequences (pCDS) are in the run?    Answer  19,838,031 pCDS.    Question 7:  How many pCDS have InterProScan hits?    Answer  5,754,502 pCDS.   Scroll down the page to the InterPro match summary section   Question 8:  How many different InterPro entries were matched by the pCDS?    Answer  4,207 IPRO hits.    Question 9:  Why is this figure different to the number of pCDS that have InterProScan hits?    Answer  Multiple CDS match an interpro id.   Next we will examine the GO terms predicted by InterPro for the pCDS in the sample.  Scroll down to the GO term annotation section of the page and examine the 3 bar charts, which correspond to the 3 different components of the Gene Ontology.Selecting the pie chart representation of GO terms makes it easier to visualise the data to find the answer.   Question 10:  What are the top 3 biological process terms predicted for the pCDS from the sample?    Answer  biosynthetic process, nitrogen compound metabolism, small molecule metabolic process    Question 11:  How many of the WGS reads are predicted to encode 16S rRNAs?    Answer  41,436   Now we will look at the taxonomic analysis for this run.  Click on the Taxonomy Analysis tab and examine the phylum composition graph and table.   Question 12:  What are the top 3 phyla in the run, according to 16S rRNA analysis?    Answer  Unassigned 48%, protobacteria 20.81%, Firmicutes 14.721% and Thermotogae 8.19%   Select the Krona chart view of the data icon. This brings up an interactive chart that can be used to analyse data at different\ntaxonomic ranks.   Question 13:  What is the proportion of Thermoanaerobacteraceae in the population?    Answer  Total 161, 55% Clostridia; 41% Firmicutes; 6% of all   Now we will compare these analyses with those for a WGS practical.   Question 14:  What are the differences in Taxonomy and Functional analysis?    Answer  TBD    Question 15:  Do the number of pCDS predicted differ?     Answer  TBD    Question 16:  Do the number of pCDSs with an InterPro match differ?    Answer  TBD    Question 17:  Do InterPro entries matched differ?    Answer  TBD    Question 18:  Are these figures broadly comparable to those for the previous analysis?    Answer  TBD", 
            "title": "Browsing analysed data via the EMG website"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#emg-comparison-tools", 
            "text": "To look at the differences in slimmed GO terms between the 2 runs. There are two ways to do this. First, you can simply scroll to the bottom of the page and examine the GO term annotation (note - selecting the bar chart representation of GO terms makes it easier to compare different data sets). Alternatively, you can use the comparison tool, which allows direct comparison of runs within a project. The tool can be accessed by clicking on the \u2018Comparison Tool\u2019 tab. At present, the tool only compares slimmed GO terms, but will be expanded to cover full GO terms, InterPro annotations, and taxonomic profiles as development of the site continues.   Bonus exercise  Bonus exercise.    Click on the Comparison tool tab and choose the Ocean Sampling Day (OSD) 2014 project from the sample list and select the OSD80_2014-06- 21_0m_NPL022 - ERR770971 and OSD80_2014-06-21_2m_NPL022 - ERR770970.    Question 19:  Are there visible differences between the GO terms for these runs. Could there be any biological explanation for this?    Answer  Yes, shift in proportion of some functions. yes climate and population density.    Navigate back and open the taxonomic analysis results tab for each run.   Question 20:  How does the taxonomic composition differ between runs? Are any trends in the data consistent with your answer to question 19?    Answer  Unassigned dropped from 48% to 13% and polaribacter is Total 931, 80% Flavobacteriaceae; 74% Bacteriodetes; 43% of all", 
            "title": "EMG comparison tools"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#visualising-taxonomic-data-using-megan", 
            "text": "Next, we are going to look at the taxonomic predictions for all of the runs. To do this, we are going to load them into MEGAN. MEGAN is a tool suite that provides metagenomic data analysis and visualization. We are going to use only a small subset of its features, relating to taxonomic comparison. Detailed information on MEGAN and the analyses and visualisations it offers can be found here:  1 http://ab.inf.uni-tuebingen.de/data/software/megan5/download/manual.pdf   MEGAN can be downloaded from  1 http://ab.inf.uni-tuebingen.de/software/megan6/  \n(there are 2 editions Community and Ultimate. The Community edition is open source and free to download. We will use this version for the tutorial). However, it should already be installed on your Desktop.  Click  on the MEGAN icon on your desktop to load the s/w.  We now need to download the full taxonomic predictions for all of the runs in the Ocean Sampling Day project.  Navigate to the Project: Ocean Sampling Day (OSD) page, using the breadcrumb URL link at the top of the page. Click on the Analysis summary tab, which will take you to a set of tab separated result matrix files, summarising the taxonomic and functional observations for all runs in the project.  Click on the Taxonomic assignments (TSV) link, which will download the corresponding file to your computer. When prompted, choose to save the file in the Downloads folder.  Open the Terminal, navigate to the Downloads directory and take a look at the file you have just downloaded (hit q to exit less):  1\n2 cd  ~/Downloads\nless -S ERP009703_taxonomy_abundances_v2.0.tsv   You will see it is a large matrix file, with abundance counts for each taxonomic lineage for each run in the project. From the MEGAN menu, choose \u2018File\u2019 and \u2018Import\u2019. Select \u2018CSV Import\u2019 and then find the file you have just downloaded and edited.  From the pop up menu, go with the default options in the Format and Separator sections (which should have the \u2018Class, Count\u2019 option selected under format, and the separator set as \u2018Tab\u2019) and select \u2018Taxonomy\u2019 in classification section, then press \u2018Apply\u2019.  There are many different visualisations and comparisons that can be performed using MEGAN, so feel free to explore the data using the tool.  Click on the \u2018Show chart\u2019 icon and choose \u2018Stacked Bar Chart\u2019. This should give you a bar chart, showing the abundance reads for taxonomic lineages across all of the sampling sites.   Question 21:  Are the number of classified 16S reads roughly equivalent across all of the different sampling sites?    Answer  No, range from ERR771006 ~10 to ~3,250 ERR771046    Question 22:  Which run appears particularly enriched in Polaribacter?    Answer  ERR770970 ~920   You can change between abundance counts and relative abundance using the Options drop menu and choosing % Percentage Scale or Linear Scale.  Change the chart view to \u2018Bubble Chart\u2019. This visualisation can be useful when comparing a large number of samples.   Question 23:  Do any samples contain taxa not found in other samples?    Take a look at the Schlegelella genus.   Answer  Yes, schlegelella is only found in ERR770961 (64)    Question 24:  Can you discern any patterns in the geographical distribution of certain species (for example, the cluster of samples enriched for the lactobacillus genus compared to other samples)?    Answer  lactobacillus ERR771039-58 Belgian   We are now going to take a look at which other datasets in EBI Metagenomics that lactobacilli are found in. Point your browser at  https://www.ebi.ac.uk/metagenomics/search/  This interface allows you to search the project, sample and run related metadata and analysis results for all of the publicly available datasets in the EBI Metagenomics resource.  Click on the \u2018Runs\u2019 tab. You should now see a number of run-related metadata search facets on the left hand side of the page, including \u2018Organism\u2019. Click on the \u2018More \u2019 option under Organism, scroll down to \u2018Lactobacillus\u2019 in the pop-up window and select the check box next to it. Now click \u2018Filter\u2019. The results page should now show all of the runs that have taxonomic matches to lactobacilli in their datasets. The \u2018Biome\u2019 facet on the left hand side of the page now shows the number of matching datasets in each biome category (to save space, the 10 biome categories with the most matching datasets are shown by default).   Question 25:  Which biome category has the most datasets that contain lactobacilli?    Answer  Host-associated (67,031) human digestive    Question 26:  How well does this correlate with what\u2019s known about these bacteria?    Answer  TBD", 
            "title": "Visualising taxonomic data using MEGAN"
        }
    ]
}